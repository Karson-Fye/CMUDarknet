{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CopulaGAN Synthetic Traffic Generation\n",
    "\n",
    "Here we load data from our seed [CIC-Darknet2020](https://www.unb.ca/cic/datasets/darknet2020.html) dataset to train two VAEs. We will then use the CopulaGAN model to generate synthetic traffic data based on Traffic Type and Application Type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys, contextlib, datetime, os, pathlib, platform, pprint, sys\n",
    "import gtda\n",
    "import ipywidgets as widgets\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import PIL.Image as Image\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "import sdv\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yellowbrick as yb\n",
    "\n",
    "fastai_gpu_conflict = True\n",
    "if(fastai_gpu_conflict):\n",
    "    torch.cuda.is_available = lambda : False\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import fastai\n",
    "from fastai.basics import Tensor\n",
    "from fastai.callback.all import CollectDataCallback, Callback, ProgressCallback\n",
    "from fastai.callback.all import LRFinder, EarlyStoppingCallback, SaveModelCallback, ShowGraphCallback, CSVLogger, slide, steep, minimum, valley\n",
    "from fastai.data.all import Transform, DisplayedTransform\n",
    "from fastai.metrics import MatthewsCorrCoef, F1Score, Recall, Precision, RocAuc, BalancedAccuracy\n",
    "from fastai.optimizer import ranger, Adam\n",
    "from fastai.tabular.data import TabularDataLoaders, TabularPandas, MultiCategoryBlock\n",
    "from fastai.tabular.all import delegates, tabular_config, TabularLearner, get_c, ifnone, is_listy, LinBnDrop, SigmoidRange \n",
    "from fastai.tabular.all import FillMissing, Categorify, Normalize, tabular_learner, accuracy, ClassificationInterpretation, range_of\n",
    "from fastai.tabular.all import get_emb_sz, Module, Learner, Embedding, CrossEntropyLossFlat, IndexSplitter, ColSplitter, RandomSplitter\n",
    "from fastai.test_utils import synth_learner, VerboseCallback\n",
    "\n",
    "from fastcore.all import Transform, store_attr, L\n",
    "from fastcore.all import L\n",
    "\n",
    "# from pytorch_tabnet.tab_network import TabNetNoEmbeddings\n",
    "# from fast_tabnet.core import TabNetModel as TabNet, tabnet_feature_importances, tabnet_explain\n",
    "\n",
    "from gtda.curves import Derivative, StandardFeatures\n",
    "from gtda.diagrams import PersistenceEntropy, Amplitude, NumberOfPoints, ComplexPolynomial, PersistenceLandscape, PersistenceImage, BettiCurve, Silhouette, HeatKernel\n",
    "from gtda.homology import VietorisRipsPersistence, EuclideanCechPersistence\n",
    "from gtda.plotting import plot_diagram, plot_point_cloud, plot_betti_curves, plot_betti_surfaces\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "from sdv.tabular import TVAE, CopulaGAN, CTGAN, GaussianCopula\n",
    "from sdv.evaluation import evaluate\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from yellowbrick.model_selection import CVScores, LearningCurve, ValidationCurve\n",
    "\n",
    "seed: int = 14\n",
    "\n",
    "# allow plots to be shown in the notebook\n",
    "init_notebook_mode(connected=True)\n",
    "# pio.renderers.default = \"iframe\"\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# set up pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# set up pretty printer for easier data evaluation\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\n",
    "\n",
    "\n",
    "# print library and python versions for reproducibility\n",
    "print(\n",
    "    f'''\n",
    "    Last Execution: {datetime.datetime.now()}\n",
    "    python:\\t{platform.python_version()}\n",
    "\n",
    "    \\tfastai:\\t\\t{fastai.__version__}\n",
    "    \\tgiotto-tda:\\t{gtda.__version__}\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\n",
    "    \\tnumpy:\\t\\t{np.__version__}\n",
    "    \\tpandas:\\t\\t{pd.__version__}\n",
    "    \\tplotly:\\t\\t{py.__version__}\n",
    "    \\tseaborn:\\t{sns.__version__}\n",
    "    \\tsdv:\\t\\t{sdv.__version__}\n",
    "    \\tsklearn:\\t{sklearn.__version__}\n",
    "    \\ttorch:\\t\\t{torch.__version__}\n",
    "    \\tyellowbrick:\\t{yb.__version__}\n",
    "\n",
    "    \\tdevice:\\t\\t{device}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create some helper functions to load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(directory: str):\n",
    "    '''\n",
    "        Closure that will return a function. \n",
    "        Function will return the filepath to the directory given to the closure\n",
    "    '''\n",
    "\n",
    "    def func(file: str) -> str:\n",
    "        return os.path.join(directory, file)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filePath):\n",
    "    '''\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\n",
    "        Function will only work when filepath is a .csv file\n",
    "    '''\n",
    "\n",
    "    p = pathlib.Path(filePath)\n",
    "    filePathClean: str = str(p.parts[-1])\n",
    "    pickleDump: str = f'./cache/{filePathClean}.pickle'\n",
    "\n",
    "    print(f'Loading Dataset: {filePath}')\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\n",
    "\n",
    "\n",
    "    # check if data already exists within cache\n",
    "    if os.path.exists(pickleDump):\n",
    "        df = pd.read_pickle(pickleDump)\n",
    " \n",
    "    # if not, load data and clean it before caching it\n",
    "    else:\n",
    "        df = pd.read_csv(filePath, low_memory=True)\n",
    "        df.to_pickle(pickleDump)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def features_with_bad_values(df: pd.DataFrame, datasetName: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will scan the dataframe for features with Inf, NaN, or Zero values.\n",
    "        Returns a new dataframe describing the distribution of these values in the original dataframe\n",
    "    '''\n",
    "\n",
    "    # Inf and NaN values can take different forms so we screen for every one of them\n",
    "    invalid_values: list = [ np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan', 0 ]\n",
    "    infs          : list = [ np.inf, 'Infinity', 'inf' ]\n",
    "    NaNs          : list = [ np.nan, 'NaN', 'nan' ]\n",
    "\n",
    "    # We will collect stats on the dataset, specifically how many instances of Infs, NaNs, and 0s are present.\n",
    "    # using a dictionary that will be converted into a (3, n) dataframe where n is the number of features in the dataset\n",
    "    stats: dict = {\n",
    "        'Dataset':[ datasetName, datasetName, datasetName ],\n",
    "        'Value'  :['Inf', 'NaN', 'Zero']\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "    for col in df.columns:\n",
    "        \n",
    "        i += 1\n",
    "        feature = np.zeros(3)\n",
    "        \n",
    "        for value in invalid_values:\n",
    "            if value in infs:\n",
    "                j = 0\n",
    "            elif value in NaNs:\n",
    "                j = 1\n",
    "            else:\n",
    "                j = 2\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                feature[j] += len(indexNames)\n",
    "                \n",
    "        stats[col] = feature\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will take a dataframe and remove the columns that match a value in prune \n",
    "        Inf and Nan values will also be removed once appropriate rows and columns \n",
    "        have been removed, \n",
    "        we will return the dataframe with the appropriate values\n",
    "    '''\n",
    "\n",
    "    # remove the features in the prune list    \n",
    "    for col in prune:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    \n",
    "    # drop missing values/NaN etc.\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Search through dataframe for any Infinite or NaN values in various forms that were not picked up previously\n",
    "    invalid_values: list = [\n",
    "        np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan'\n",
    "    ]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for value in invalid_values:\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                print(f'deleting {len(indexNames)} rows with Infinity in column {col}')\n",
    "                df.drop(indexNames, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "color_list: list = [\n",
    "    'aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance',\n",
    "    'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg',\n",
    "    'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl',\n",
    "    'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric',\n",
    "    'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys',\n",
    "    'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet',\n",
    "    'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges',\n",
    "    'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl',\n",
    "    'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn',\n",
    "    'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu',\n",
    "    'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar',\n",
    "    'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn',\n",
    "    'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid',\n",
    "    'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr',\n",
    "    'ylorrd'\n",
    "]\n",
    "\n",
    "\n",
    "def get_n_color_list(n: int, opacity=.8, luminosity=.5, saturation=.5) -> list:\n",
    "    '''\n",
    "        Function creates a list of n distinct colors, formatted for use in a plotly graph\n",
    "    '''\n",
    "\n",
    "    colors = []\n",
    "\n",
    "    for i in range(n):\n",
    "        color = colorsys.hls_to_rgb(\n",
    "                i / n,\n",
    "                luminosity,\n",
    "                saturation,\n",
    "        )\n",
    "        \n",
    "        colors.append(f'rgba({color[0] * 255}, {color[1] * 255}, {color[2] * 255}, {opacity})')\n",
    "\n",
    "    return colors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify what datasets we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to scale to processing numerous datasets, even though we currently are only looking at one now\n",
    "data_path_1: str = '../../../data/phase1/'   \n",
    "data_set_1: list = [\n",
    "    'Traffic_type_seed.csv',\n",
    "    'Application_type_seed.csv',\n",
    "]\n",
    "\n",
    "data_set: list   = data_set_1\n",
    "file_path_1      = get_file_path(data_path_1)\n",
    "file_set: list   = list(map(file_path_1, data_set_1))\n",
    "current_job: int = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes and Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn classes\n",
    "\n",
    "class SklearnWrapper(BaseEstimator):\n",
    "    '''\n",
    "        A wrapper for fastai learners for creating visualizations using yellowbrick\n",
    "        code sourced from: \n",
    "        forums.fast.ai/t/fastai-with-yellowbrics-how-to-get-roc-curves-more/79408\n",
    "    '''\n",
    "    _estimator_type = \"classifier\"\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.classes_ = list(self.model.dls.y.unique())\n",
    "        self._calls = [] \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._calls.append(('fit', X, y))\n",
    "        pass\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        self._calls.append(('score', X, y))\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "    \n",
    "    def get_new_preds(self, X):\n",
    "        self._calls.append(('get_new_preds', X))\n",
    "        new_to = self.model.dls.valid_ds.new(X)\n",
    "        new_to.conts = new_to.conts.astype(np.float32)\n",
    "        new_dl = self.model.dls.valid.new(new_to)\n",
    "        with self.model.no_bar():\n",
    "            preds,_,dec_preds = self.model.get_preds(dl=new_dl, with_decoded=True)\n",
    "        return (preds, dec_preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self._calls.append(('predict_proba', X))\n",
    "        return self.get_new_preds(X)[0].numpy()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self._calls.append(('predict', X))\n",
    "        return self.get_new_preds(X)[1].numpy()\n",
    "\n",
    "\n",
    "# fastai classes\n",
    "\n",
    "# transforms for the dataloaders pipeline\n",
    "\n",
    "class PCA_tabular(Transform):\n",
    "    '''\n",
    "        Class will implement a PCA feature extraction method for tabular data\n",
    "        On setup, we train a pca on the training data, then extract n_comps from the entire dataset \n",
    "            the components are then added to the dataframe as new columns\n",
    "    '''\n",
    "    def __init__(self, n_comps=3, add_col=True):\n",
    "        store_attr() # core function in the fastai library that stores all passed in parameters\n",
    "\n",
    "    def setups(self, to, **kwargs):\n",
    "\n",
    "        self.pca = PCA(n_components=self.n_comps)\n",
    "        self.pca.fit(to.train.conts)\n",
    "        pca = pd.DataFrame(self.pca.transform(to.conts))\n",
    "        pca.columns = [f'pca_{i+1}' for i in range(self.n_comps)]\n",
    "\n",
    "        for col in pca.columns:\n",
    "            to.items[col] = pca[col].values.astype('float32')\n",
    "        \n",
    "\n",
    "        if self.add_col:\n",
    "            for i in range(self.n_comps):\n",
    "                if f'pca_{i+1}' not in to.cont_names: to.cont_names.append(f'pca_{i+1}')\n",
    "        \n",
    "        return self(to)\n",
    "\n",
    "\n",
    "\n",
    "class Normal(DisplayedTransform):\n",
    "    '''\n",
    "        A data processing tool inherited from the fastai library. \n",
    "        This is a modified version of the normalize function that performs MinMax scaling and is able to be\n",
    "            used in our preprocessing pipeline. \n",
    "        The original normalizes the data to have a mean centered at 0 and a standard deviation of 1.\n",
    "    '''\n",
    "    def setups(self, to, **kwargs):\n",
    "        self.mins = getattr(to, 'train', to).conts.min()\n",
    "        self.maxs = getattr(to, 'train', to).conts.max()\n",
    "        \n",
    "        return self(to)\n",
    "        \n",
    "    def encodes(self, to, **kwargs):\n",
    "        to.conts = (to.conts-self.mins) / (self.maxs - self.mins)\n",
    "        return to\n",
    "\n",
    "    def decodes(self, to, **kwargs):\n",
    "        to.conts = (to.conts) * (to.maxs - to.mins) + to.mins\n",
    "        return to\n",
    "\n",
    "\n",
    "# callbacks for the fastai training schedule\n",
    "\n",
    "class DFLogger(Callback):\n",
    "    '''\n",
    "        Class defines a callback that is passed to the fastai learner that\n",
    "            will save the recorded metrics for each epoch to a dataframe\n",
    "    '''\n",
    "    order=60\n",
    "    def __init__(self, fname='history.csv', append=False):\n",
    "        self.fname,self.append = pathlib.Path(fname),append\n",
    "        self.df = pd.DataFrame()\n",
    "        self.flag = True\n",
    "\n",
    "    def to_csv(self, file: str or None = None):\n",
    "        if file is None:\n",
    "            self.df.to_csv(self.path/self.fname, index=False)\n",
    "        else:\n",
    "            self.df.to_csv(file, index=False)\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Prepare file with metric names.\"\n",
    "        if hasattr(self, \"gather_preds\"): return\n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.old_logger,self.learn.logger = self.logger,self.add_row\n",
    "\n",
    "    def add_row(self, log):\n",
    "        \"Write a line with `log` and call the old logger.\"\n",
    "        if(self.flag):\n",
    "            self.df = pd.DataFrame([log], columns=self.recorder.metric_names)\n",
    "            self.flag = False\n",
    "        else:\n",
    "            if (len(log) == len(self.df.columns)):\n",
    "                self.new_row = pd.DataFrame([log], columns=self.recorder.metric_names)\n",
    "                self.df = pd.concat([self.df, self.new_row], ignore_index=True)\n",
    "\n",
    "    def after_fit(self):\n",
    "        \"Close the file and clean up.\"\n",
    "        if hasattr(self, \"gather_preds\"): return\n",
    "        self.learn.logger = self.old_logger\n",
    "\n",
    "\n",
    "\n",
    "class LazyGraphCallback(Callback):\n",
    "    '''\n",
    "        Class defines a callback that is passed to the fastai learner that\n",
    "            saves the validation and training loss metrics to graph when\n",
    "            calling the .plot_graph() method\n",
    "        \n",
    "        This allows us to display the train/validation loss graph after the    \n",
    "            experiment is run, even if it is run in no_bar mode\n",
    "    '''\n",
    "    order: int = 65\n",
    "    run_valid: bool = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph_params: list = []\n",
    "        self.graphs: list = []\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not self.run: return\n",
    "        self.nb_batches: list = []\n",
    "\n",
    "    def after_train(self): self.nb_batches.append(self.train_iter)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "        rec = self.learn.recorder\n",
    "        iters = range_of(rec.losses)\n",
    "        val_losses = [v[1] for v in rec.values]\n",
    "        x_bounds = (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses))\n",
    "        y_bounds = (0, max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n",
    "        self.graph_params.append(([(iters, rec.losses), (self.nb_batches, val_losses)], x_bounds, y_bounds))\n",
    "\n",
    "\n",
    "    def plot_graph(self, ax: plt.Axes or None = None, title: str = 'Loss'):\n",
    "\n",
    "        params = self.graph_params[-1]\n",
    "        \n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Iterations')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.plot(params[0][0][0], params[0][0][1], label='Training')\n",
    "        ax.plot(params[0][1][0], params[0][1][1], label='Validation')\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "class ModelStatsCallback(Callback):\n",
    "    '''\n",
    "        Collect all batches, along with `pred` and `loss`, into `self.datum`.\n",
    "        This allows the data to be used after the model is used.\n",
    "    '''\n",
    "    order=60 # mysterious param we need to investigate\n",
    "    def __init__(self):\n",
    "        self.datum: list = []\n",
    "        self.records: list = []\n",
    "        self.c_idx: int  = -1\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.datum.append(L())\n",
    "        self.records.append({'loss': [], 'predictions': []})\n",
    "        self.c_idx += 1\n",
    "\n",
    "\n",
    "    def after_batch(self):\n",
    "        vals = self.learn.to_detach((self.xb,self.yb,self.pred,self.loss))\n",
    "        self.datum[self.c_idx].append(vals)\n",
    "        self.records[self.c_idx]['predictions'].append(vals[2])\n",
    "        self.records[self.c_idx]['loss'].append(vals[3])\n",
    "        self.records[self.c_idx]['iters'] = range_of(self.recorder.losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Manipulation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(job_id: int) -> dict:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe of the job_id passed in as well as that dataframe's\n",
    "        feature stats, data composition, and file name.\n",
    "\n",
    "        This dictionary is expected as the input for all of the other helper functions\n",
    "    '''\n",
    "\n",
    "    job_id = job_id - 1  # adjusts for indexing while enumerating jobs from 1\n",
    "    print(f'Dataset {job_id+1}/{len(data_set)}: We now look at {file_set[job_id]}\\n\\n')\n",
    "\n",
    "    # Load the dataset\n",
    "    df: pd.DataFrame = load_data(file_set[job_id])\n",
    " \n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        File:\\t\\t\\t\\t{file_set[job_id]}  \n",
    "        Job Number:\\t\\t\\t{job_id+1}\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             file_set[job_id],\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, file_set[job_id]), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection(df: pd.DataFrame) -> dict:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "    Dataset statistics:\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             '',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, ''), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection_with_label(df: pd.DataFrame, label: str) -> dict:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             f'{label}',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, f'{label}'),\n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def check_infs(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of Inf.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    return inf_df[inf_df[0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of NaN.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    return nan_df[nan_df[1] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "\n",
    "    return zero_df[zero_df[2] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold(data_summary: dict, threshold: int) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold_percentage(data_summary: dict, threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with all features with\n",
    "        a frequency of 0 values greater than the threshold\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    size: int = data_summary['Dataset'].shape[0]\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold*size]\n",
    "\n",
    "\n",
    "\n",
    "def remove_infs_and_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all inf and nan values removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, [])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rename_columns(data_summary: dict, columns: list, new_names: list) -> dict:\n",
    "    '''\n",
    "        Function will return the data_summary dict with the names of the columns in the dataframe changed\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    for x, i in enumerate(columns):\n",
    "        df.rename(columns={i: new_names[x]}, inplace=True)\n",
    "\n",
    "    data_summary['Dataset'] = df\n",
    "\n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def rename_values_in_column(data_summary: dict, replace: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe with the names of the columns changed\n",
    "\n",
    "        replace: [('column', {'old_name': 'new_name', ...}), ...]\n",
    "    '''\n",
    "    length: int = len(replace)\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    for i in range(length):\n",
    "        df[replace[i][0]].replace(replace[i][1], inplace=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rename_values_in_column_df(df: pd.DataFrame, replace: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe with the names of the columns changed\n",
    "\n",
    "        replace: [('column', {'old_name': 'new_name', ...}), ...]\n",
    "    '''\n",
    "    length: int = len(replace)\n",
    "\n",
    "    df1: pd.DataFrame = df.copy()\n",
    "    for i in range(length):\n",
    "        df1[replace[i][0]].replace(replace[i][1], inplace=True)\n",
    "\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "\n",
    "def prune_dataset(data_summary: dict, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all the columns in the prune list removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, prune)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prune_feature_by_values(df: pd.DataFrame, column: str, value: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function takes a dataframe, a column name, and a list of values and returns a dataframe\n",
    "        with all rows that do not have the values in the column removed\n",
    "\n",
    "        Deprecated, use reduce_feature_to_values:\n",
    "            ambiguous name implied it is removing values, not keeping them\n",
    "    '''\n",
    "    new_df = pd.DataFrame()\n",
    "    for v in value:\n",
    "        new_df = new_df.append(df[df[column] == v].copy())\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def reduce_feature_to_values(df: pd.DataFrame, column: str, value: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function takes a dataframe, a column name, and a list of values and returns a dataframe\n",
    "        with all rows that do not have the values in the column removed\n",
    "    '''\n",
    "    new_df = pd.DataFrame()\n",
    "    for v in value:\n",
    "        new_df = new_df.append(df[df[column] == v].copy())\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def test_infs(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no inf values.\n",
    "    '''\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    assert inf_df[inf_df[0] != 0].shape[0] == 2, 'Dataset has inf values'\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_nans(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no NaN values\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    assert nan_df[nan_df[1] != 0].shape[0] == 2, 'Dataset has NaN values'\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned(data_summary: dict, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    pruned: bool = True\n",
    "\n",
    "    for col in prune:\n",
    "        if col in data_summary['Dataset'].columns:\n",
    "            pruned = False\n",
    "\n",
    "    assert pruned, 'Dataset has columns present in prune list'\n",
    "\n",
    "    return pruned\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned_size(data_summary_original: dict, data_summary_pruned: dict, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    original_size: int = data_summary_original['Dataset'].shape[1]\n",
    "    pruned_size: int = data_summary_pruned['Dataset'].shape[1]\n",
    "    prune_list_size: int = len(prune)\n",
    "\n",
    "    assert original_size - prune_list_size == pruned_size, 'Dataset has columns present in prune list'\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytorch --> fastai models\n",
    "\n",
    "class ResidualBlock(Module):\n",
    "    '''\n",
    "        A simple residule block that creates a skip connection around any input module\n",
    "        Output size of the input module must match the module's input size\n",
    "    '''\n",
    "    def __init__(self, module, layer):\n",
    "        self.module = module\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fx = self.module(inputs)\n",
    "        if(inputs.shape != fx.shape):\n",
    "            print('mismatch at layer:', self.layer ,inputs.shape, fx.shape)\n",
    "            assert False\n",
    "        return fx + inputs\n",
    "\n",
    "\n",
    "class CardinalResidualBlock(Module):\n",
    "    '''\n",
    "        A residule block that creates a skip connection around a set of n branches\n",
    "            where the number of branches is determined by the number of input modules\n",
    "            in the branches list parameter.\n",
    "\n",
    "        The output of the branches is summed together along with the input\n",
    "        Output size of the input module must match the module's input size\n",
    "    '''\n",
    "    def __init__(self, branches: list, layer: int):\n",
    "        self.branches = branches\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fx = self.branches[0](inputs)\n",
    "        if(inputs.shape != fx.shape):\n",
    "            print('mismatch at layer:', self.layer ,inputs.shape, fx.shape)\n",
    "            assert False\n",
    "        if(len(self.branches) > 1):\n",
    "            for i in range(len(self.branches) - 1):\n",
    "                fx += self.branches[i + 1](inputs)\n",
    "\n",
    "        return fx + inputs\n",
    "\n",
    "# currently need to create a bottlenecking block that can be used to reduce the number of inputs\n",
    "#   being passed by the residual connection \n",
    "\n",
    "class BottleneckResidualBlock(Module):\n",
    "    '''\n",
    "        A residule block that creates a skip connection around a set of n branches\n",
    "            where the number of branches is determined by the number of input modules\n",
    "            in the branches list parameter.\n",
    "\n",
    "            the residual connection is put through a linear batchnormed layer if the\n",
    "            input size is different from the output size\n",
    "            Then, the output of the branches is summed together along with the possibly transformed input\n",
    "    '''\n",
    "    def __init__(self, branches: list, layer: int, in_size: int, out_size: int):\n",
    "        self.branches = branches\n",
    "        self.layer = layer\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        # self.linear = nn.Linear(in_size, out_size)\n",
    "        self.linear = LinBnDrop(in_size, out_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fx = self.branches[0](inputs)\n",
    "        for i in range(len(self.branches) - 1):\n",
    "            fx += self.branches[i + 1](inputs)\n",
    "\n",
    "        if(inputs.shape != fx.shape):\n",
    "            inputs = self.linear(inputs)\n",
    "        return fx + inputs\n",
    "\n",
    "\n",
    "class ResidualTabularModel(Module):\n",
    "    \"Residual model for tabular data.\"\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0.,\n",
    "                 y_range=None, use_bn=True, bn_final=False, bn_cont=True, act_cls=nn.ReLU(inplace=True),\n",
    "                 lin_first=True, cardinality: list or None = None):\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        if not is_listy(ps): ps = [ps]*len(layers)\n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(embed_p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_emb,self.n_cont = n_emb,n_cont\n",
    "        sizes = [n_emb + n_cont] + layers + [out_sz]\n",
    "\n",
    "        # print(f'sizes', sizes)\n",
    "        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n",
    "        \n",
    "        _layers: list = []\n",
    "        num_residuals = 0\n",
    "        residual_locations = []\n",
    "        enum_length = len(list(enumerate(zip(ps+[0.],actns))))\n",
    "        for i, (p, a) in enumerate(zip(ps+[0.],actns)):\n",
    "            if(i==0 or i == enum_length-1):\n",
    "                _layers.append(LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first))\n",
    "            else:\n",
    "                if(cardinality == None):\n",
    "                    modules = [ LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first), ]\n",
    "                else:\n",
    "                    modules = [ LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first) for _ in range(cardinality[i]) ]\n",
    "                num_residuals += 1 \n",
    "                residual_locations.append(i)\n",
    "                _layers.append( BottleneckResidualBlock(modules, i, sizes[i], sizes[i+1]) )\n",
    "\n",
    "        print(f'Layer sizes: {sizes}, length: {len(sizes)}')\n",
    "        print(f'Number of residual blocks: {num_residuals}')\n",
    "        print('Residual locations: ', residual_locations)\n",
    "\n",
    "        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n",
    "        self.layers = nn.Sequential(*_layers)\n",
    "\n",
    "    def forward(self, x_cat, x_cont=None):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "\n",
    "@delegates(Learner.__init__)\n",
    "def residual_tabular_learner(dls, layers=None, emb_szs=None, config=None, n_out=None, y_range=None, cardinality=None, ps=None, **kwargs):\n",
    "    \"Get a `Learner` using `dls`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
    "    if config is None: config = tabular_config()\n",
    "    if layers is None: layers = [200,100]\n",
    "    to = dls.train_ds\n",
    "    emb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n",
    "    model = ResidualTabularModel(emb_szs, len(dls.cont_names), n_out, layers, y_range=y_range, cardinality=cardinality, ps=ps, **config)\n",
    "    return TabularLearner(dls, model, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to run classification experiments or to transform and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_data = namedtuple(\n",
    "    'model_data', \n",
    "    ['name', 'model', 'classes', 'X_train', 'y_train', 'X_test', 'y_test', 'to', 'dls', 'model_type']\n",
    ")\n",
    "\n",
    "# todo: standardize new format of experiment runners\n",
    "\n",
    "def run_deep_nn_experiment(\n",
    "    df: pd.DataFrame, \n",
    "    file_name: str, \n",
    "    target_label: str, \n",
    "    shape: tuple, \n",
    "    split=0.2, \n",
    "    categorical: list = ['Protocol'],\n",
    "    procs = [FillMissing, Categorify, Normalize], \n",
    "    leave_out: list = [],\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    metrics: list or None = None,\n",
    "    callbacks: list = [ShowGraphCallback],\n",
    "    lr_choice: str = 'valley',\n",
    "    name: str or None = None,\n",
    "    fit_choice: str = 'one_cycle',\n",
    "    no_bar: bool = False,\n",
    ") -> Model_data:\n",
    "    '''\n",
    "        Function trains a deep neural network model on the given data. \n",
    "\n",
    "        Parameters:\n",
    "            df: pandas dataframe containing the data\n",
    "            file_name: name of the file the dataset came from\n",
    "            target_label: the label to predict\n",
    "            shape: the shape of the neural network, the i-th value in the tuple represents the number of nodes in the i+1 layer\n",
    "                    and the number of entries in the tuple represent the number of layers\n",
    "            name: name of the experiment, if none a default is given\n",
    "            split: the percentage of the data to use for testing\n",
    "            categorical: list of the categorical columns\n",
    "            procs: list of preprocessing functions to apply in the dataloaders pipeline\n",
    "                    additional options are: \n",
    "                        PCA_tabular (generate n principal components) \n",
    "                        Normal (features are scaled to the interval [0,1])\n",
    "            leave_out: list of columns to leave out of the experiment\n",
    "            epochs: number of epochs to train for\n",
    "            batch_size: number of samples processed in one forward and backward pass of the model\n",
    "            metrics: list of metrics to calculate and display during training\n",
    "            callbacks: list of callbacks to apply during training\n",
    "            lr_choice: where the learning rate sampling function should find the optimal learning rate\n",
    "                        choices are: 'valley', 'steep', 'slide', and 'minimum'\n",
    "            fit_choice: choice of function that controls the learning schedule choices are:\n",
    "                                'fit': a standard learning schedule \n",
    "                                'flat_cos': a learning schedule that starts high before annealing to a low value\n",
    "                                'one_cyle': a learning schedule that warms up for the first epochs, continues at a high\n",
    "                                                learning rate, and then cools down for the last epochs\n",
    "                                the default is 'one_cycle'\n",
    "         \n",
    "        \n",
    "        returns a model data named tuple\n",
    "            model_data: tuple = (file_name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "    '''\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if name is None:\n",
    "        width: int = shape[0]\n",
    "        for x in shape:\n",
    "            width = x if (x > width) else width\n",
    "        name = f'Deep_NN_{len(shape)}x{width}'\n",
    "\n",
    "    lr_choice = {'valley': 0, 'slide': 1, 'steep': 2, 'minimum': 3}[lr_choice]\n",
    "\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "        \n",
    "    if metrics is None:\n",
    "        metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "\n",
    "\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([target_label]) - set(untouched_features))\n",
    "\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "\n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=target_label                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "    # The dataframe is then converted into a fastai dataset\n",
    "    dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "    # extract the file_name from the path\n",
    "    p = pathlib.Path(file_name)\n",
    "    file_name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    learner = tabular_learner(\n",
    "        dls, \n",
    "        layers=list(shape), \n",
    "        metrics = metrics,\n",
    "        cbs=callbacks,\n",
    "    )\n",
    "\n",
    "    with learner.no_bar() if no_bar else contextlib.ExitStack() as gs:\n",
    "\n",
    "        lr = learner.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "            # fitting functions, they give different results, some networks perform better with different learning schedule during fitting\n",
    "        if(fit_choice == 'fit'):\n",
    "            learner.fit(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'flat_cos'):\n",
    "            learner.fit_flat_cos(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'one_cycle'):\n",
    "            learner.fit_one_cycle(epochs, lr_max=lr[lr_choice])\n",
    "        else:\n",
    "            assert False, f'{fit_choice} is not a valid fit_choice'\n",
    "\n",
    "        learner.recorder.plot_sched() \n",
    "        results = learner.validate()\n",
    "        interp = ClassificationInterpretation.from_learner(learner)\n",
    "        interp.plot_confusion_matrix()\n",
    "                \n",
    "\n",
    "    print(f'loss: {results[0]}, accuracy: {results[1]*100: .2f}%')\n",
    "    learner.save(f'{file_name}.model')\n",
    "\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "    wrapped_model = SklearnWrapper(learner)\n",
    "\n",
    "    classes = list(learner.dls.vocab)\n",
    "    if len(classes) == 2:\n",
    "        wrapped_model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        wrapped_model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "    \n",
    "    wrapped_model._target_labels = target_label\n",
    "    \n",
    "    model_data: Model_data = Model_data(file_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, name)\n",
    "\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_residual_deep_nn_experiment(\n",
    "    df: pd.DataFrame, \n",
    "    file_name: str, \n",
    "    target_label: str, \n",
    "    shape: tuple, \n",
    "    split=0.2, \n",
    "    categorical: list = ['Protocol'],\n",
    "    procs = [FillMissing, Categorify, Normalize], \n",
    "    leave_out: list = [],\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    metrics: list or None = None,\n",
    "    callbacks: list = [ShowGraphCallback],\n",
    "    lr_choice: str = 'valley',\n",
    "    name: str or None = None,\n",
    "    fit_choice: str = 'one_cycle',\n",
    "    cardinality: list or None = None,\n",
    "    no_bar: bool = False,\n",
    "    ps: list or None = None,\n",
    ") -> Model_data:\n",
    "    '''\n",
    "        Function trains a residual deep neural network model on the given data. \n",
    "            Based on ResNet from Deep Residual Learning for Image Recognition by He et al. (2016) \n",
    "            as well as the ResNext network proposed by Xie et al. (2017) but adapted to tabular data  \n",
    "        \n",
    "        (https://arxiv.org/abs/1512.03385)\n",
    "        (https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "        Parameters:\n",
    "            df: pandas dataframe containing the data\n",
    "            file_name: name of the file the dataset came from\n",
    "            target_label: the label to predict\n",
    "            shape: the shape of the neural network, the i-th value in the tuple represents the number of nodes in the i+1 layer\n",
    "                    and the number of entries in the tuple represent the number of layers\n",
    "            name: name of the experiment, if none a default is given\n",
    "            split: the percentage of the data to use for testing\n",
    "            categorical: list of the categorical columns\n",
    "            procs: list of preprocessing functions to apply in the dataloaders pipeline\n",
    "                    additional options are: \n",
    "                        PCA_tabular (generate n principal components) \n",
    "                        Normal (features are scaled to the interval [0,1])\n",
    "            leave_out: list of columns to leave out of the experiment\n",
    "            epochs: number of epochs to train for\n",
    "            batch_size: number of samples processed in one forward and backward pass of the model\n",
    "            metrics: list of metrics to calculate and display during training\n",
    "            callbacks: list of callbacks to apply during training\n",
    "            lr_choice: where the learning rate sampling function should find the optimal learning rate\n",
    "                        choices are: 'valley', 'steep', 'slide', and 'minimum'\n",
    "            fit_choice: choice of function that controls the learning schedule choices are:\n",
    "                    'fit': a standard learning schedule \n",
    "                    'flat_cos': a learning schedule that starts high before annealing to a low value\n",
    "                    'one_cyle': a learning schedule that warms up for the first epochs, continues at a high\n",
    "                                    learning rate, and then cools down for the last epochs\n",
    "                    the default is 'one_cycle'\n",
    "            cardinality: list of integers that represent the number of residual blocks in each layer, if none\n",
    "                            the default is one block per layer\n",
    "\n",
    "        \n",
    "        returns a model data named tuple\n",
    "            model_data: tuple = (file_name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "    '''\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if name is None:\n",
    "        width: int = shape[0]\n",
    "        for x in shape:\n",
    "            width = x if (x > width) else width\n",
    "        name = f'Residual_1D_Deep_NN_{len(shape)}x{width}'\n",
    "\n",
    "    lr_choice = {'valley': 0, 'slide': 1, 'steep': 2, 'minimum': 3}[lr_choice]\n",
    "\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "        \n",
    "    if metrics is None:\n",
    "        metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "\n",
    "\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([target_label]) - set(untouched_features))\n",
    "\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "\n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=target_label                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "    # The dataframe is then converted into a fastai dataset\n",
    "    dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "    # extract the file_name from the path\n",
    "    p = pathlib.Path(file_name)\n",
    "    file_name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    learner = residual_tabular_learner(\n",
    "        dls, \n",
    "        layers=list(shape), \n",
    "        metrics = metrics,\n",
    "        cbs=callbacks,\n",
    "        cardinality=cardinality\n",
    "    )\n",
    "\n",
    "\n",
    "    with learner.no_bar() if no_bar else contextlib.ExitStack() as gs:\n",
    "\n",
    "        lr = learner.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "            # fitting functions, they give different results, some networks perform better with different learning schedule during fitting\n",
    "        if(fit_choice == 'fit'):\n",
    "            learner.fit(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'flat_cos'):\n",
    "            learner.fit_flat_cos(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'one_cycle'):\n",
    "            learner.fit_one_cycle(epochs, lr_max=lr[lr_choice])\n",
    "        else:\n",
    "            assert False, f'{fit_choice} is not a valid fit_choice'\n",
    "\n",
    "        learner.recorder.plot_sched() \n",
    "        results = learner.validate()\n",
    "        interp = ClassificationInterpretation.from_learner(learner)\n",
    "        interp.plot_confusion_matrix()\n",
    "                \n",
    "\n",
    "    print(f'loss: {results[0]}, accuracy: {results[1]*100: .2f}%')\n",
    "    learner.save(f'{file_name}.model')\n",
    "\n",
    "\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "    wrapped_model = SklearnWrapper(learner)\n",
    "\n",
    "    classes = list(learner.dls.vocab)\n",
    "    if len(classes) == 2:\n",
    "        wrapped_model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        wrapped_model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "    \n",
    "    wrapped_model._target_labels = target_label\n",
    "    \n",
    "    model_data: Model_data = Model_data(file_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, name)\n",
    "\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "# def run_tabnet_experiment(\n",
    "#     df: pd.DataFrame, \n",
    "#     file_name: str, \n",
    "#     target_label: str, \n",
    "#     split=0.2, \n",
    "#     name: str or None = None,\n",
    "#     categorical: list = ['Protocol'],\n",
    "#     procs = [FillMissing, Categorify, Normalize], \n",
    "#     leave_out: list = [],\n",
    "#     epochs: int = 10,\n",
    "#     steps: int = 1,\n",
    "#     batch_size: int = 64,\n",
    "#     metrics: list or None = None,\n",
    "#     attention_size: int = 16,\n",
    "#     attention_width: int = 16,\n",
    "#     callbacks: list = [ShowGraphCallback],\n",
    "#     lr_choice: str = 'valley',\n",
    "#     fit_choice: str = 'flat_cos',\n",
    "# ) -> Model_data:\n",
    "#     '''\n",
    "#     Function trains a TabNet model on the dataframe and returns a model data named tuple\n",
    "#         Based on TabNet: Attentive Interpretable Tabular Learning by Sercan Arik and Tomas Pfister from Google Cloud AI (2016)\n",
    "#             where a DNN selects features from the input features based on an attention layer. Each step of the model selects \n",
    "#             different features and uses the input from the previous step to ultimately make predictions\n",
    "    \n",
    "#         Combines aspects of a transformer, decision trees, and deep neural networks to learn tabular data, and has achieved state\n",
    "#             of the art results on some datasets.\n",
    "\n",
    "#         Capable of self-supervised learning, however it is not implemented here yet.\n",
    "\n",
    "#     (https://arxiv.org/pdf/1908.07442.pdf)\n",
    "\n",
    "#     Parameters:\n",
    "#         df: pandas dataframe containing the data\n",
    "#         file_name: name of the file the dataset came from\n",
    "#         target_label: the label to predict\n",
    "#         name: name of the experiment, if none a default is given\n",
    "#         split: the percentage of the data to use for testing\n",
    "#         categorical: list of the categorical columns\n",
    "#         procs: list of preprocessing functions to apply in the dataloaders pipeline\n",
    "#                 additional options are: \n",
    "#                     PCA_tabular (generate n principal components) \n",
    "#                     Normal (features are scaled to the interval [0,1])\n",
    "#         leave_out: list of columns to leave out of the experiment\n",
    "#         epochs: number of epochs to train for  \n",
    "#         batch_size: number of samples processed in one forward and backward pass of the model\n",
    "#         metrics: list of metrics to calculate and display during training\n",
    "#         attention size: determines the number of rows and columns in the attention layers\n",
    "#         attention width: determines the width of the decision layer\n",
    "#         callbacks: list of callbacks to apply during training\n",
    "#         lr_choice: where the learning rate sampling function should find the optimal learning rate\n",
    "#                     choices are: 'valley', 'steep', 'slide', and 'minimum'\n",
    "#         fit_choice: choice of function that controls the learning schedule choices are:\n",
    "#                     'fit': a standard learning schedule \n",
    "#                     'flat_cos': a learning schedule that starts high before annealing to a low value\n",
    "#                     'one_cyle': a learning schedule that warms up for the first epochs, continues at a high\n",
    "#                                     learning rate, and then cools down for the last epochs\n",
    "#                     the default is 'flat_cos'\n",
    "\n",
    "    \n",
    "#     returns a model data named tuple\n",
    "#         model_data: tuple = (file_name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "#     '''\n",
    "\n",
    "#     if name is None:\n",
    "#         name = f\"TabNet_steps_{steps}_width_{attention_width}_attention_{attention_size}\"\n",
    "\n",
    "#     lr_choice = {'valley': 0, 'slide': 1, 'steep': 2, 'minimum': 3}[lr_choice]\n",
    "\n",
    "\n",
    "#     categorical_features: list = []\n",
    "#     untouched_features  : list = []\n",
    "\n",
    "#     for x in leave_out:\n",
    "#         if x in df.columns:\n",
    "#             untouched_features.append(x)\n",
    "\n",
    "#     for x in categorical:\n",
    "#         if x in df.columns:\n",
    "#             categorical_features.append(x)\n",
    "\n",
    "        \n",
    "#     if metrics is None:\n",
    "#         metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "\n",
    "\n",
    "#     continuous_features = list(set(df) - set(categorical_features) - set([target_label]) - set(untouched_features))\n",
    "\n",
    "#     splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "#     # The dataframe is loaded into a fastai datastructure now that \n",
    "#     # the feature engineering pipeline has been set up\n",
    "\n",
    "#     to = TabularPandas(\n",
    "#         df            , y_names=target_label                , \n",
    "#         splits=splits , cat_names=categorical_features ,\n",
    "#         procs=procs   , cont_names=continuous_features , \n",
    "#     )\n",
    "\n",
    "#     # The dataframe is then converted into a fastai dataset\n",
    "#     dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "#     # extract the file_name from the path\n",
    "#     p = pathlib.Path(file_name)\n",
    "#     file_name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "#     emb_szs = get_emb_sz(to)\n",
    "\n",
    "\n",
    "#     net = TabNet(emb_szs, len(to.cont_names), dls.c, n_d=attention_width, n_a=attention_size, n_steps=steps) \n",
    "#     tab_model = Learner(dls, net, loss_func=CrossEntropyLossFlat(), metrics=metrics, opt_func=ranger, cbs=callbacks)\n",
    "\n",
    "\n",
    "#     with learner.no_bar() if no_bar else contextlib.ExitStack() as gs:\n",
    "\n",
    "#         lr = tab_model.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "\n",
    "#         # fitting functions, they give different results, some networks perform better with different learning schedule during fitting\n",
    "#         if(fit_choice == 'fit'):\n",
    "#             tab_model.fit(epochs, lr[lr_choice])\n",
    "#         elif(fit_choice == 'flat_cos'):\n",
    "#             tab_model.fit_flat_cos(epochs, lr[lr_choice])\n",
    "#         elif(fit_choice == 'one_cycle'):\n",
    "#             tab_model.fit_one_cycle(epochs, lr_max=lr[lr_choice])\n",
    "#         else:\n",
    "#             assert False, f'{fit_choice} is not a valid fit_choice'\n",
    "\n",
    "\n",
    "#         tab_model.recorder.plot_sched() \n",
    "#         results = tab_model.validate()\n",
    "#         interp = ClassificationInterpretation.from_learner(tab_model)\n",
    "#         interp.plot_confusion_matrix()\n",
    "                \n",
    "\n",
    "#     tab_model.save(f'{file_name}.model')\n",
    "#     print(f'loss: {results[0]}, accuracy: {results[1]*100: .2f}%')\n",
    "\n",
    "\n",
    "#     X_train = to.train.xs.reset_index(drop=True)\n",
    "#     X_test = to.valid.xs.reset_index(drop=True)\n",
    "#     y_train = to.train.ys.values.ravel()\n",
    "#     y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "#     wrapped_model = SklearnWrapper(tab_model)\n",
    "\n",
    "#     classes = list(tab_model.dls.vocab)\n",
    "#     if len(classes) == 2:\n",
    "#         wrapped_model.target_type_ = 'binary'\n",
    "#     elif len(classes) > 2:  \n",
    "#         wrapped_model.target_type_ = 'multiclass'\n",
    "#     else:\n",
    "#         print('Must be more than one class to perform classification')\n",
    "#         raise ValueError('Wrong number of classes')\n",
    "    \n",
    "#     wrapped_model._target_labels = target_label\n",
    "    \n",
    "#     model_data: Model_data = Model_data(file_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, name)\n",
    "\n",
    "\n",
    "#     return model_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Todo: make into general function that takes sklearn model instead of just a knn\n",
    "\n",
    "def run_knn_experiment(df: pd.DataFrame, name: str, target_label: str, split=0.2, categorical : list = ['Protocol'], leave_out: list = [], model = None) -> Model_data:\n",
    "    '''\n",
    "        Run binary classification using K-Nearest Neighbors\n",
    "        returns the 10-tuple Model_data\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var: str = target_label\n",
    "    print(df.shape)\n",
    "\n",
    " \n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]) - set(untouched_features))\n",
    "\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\n",
    "    dls = to.dataloaders(bs=64)\n",
    "    temp_model = tabular_learner(dls)\n",
    "    classes : list = list(temp_model.dls.vocab)\n",
    "\n",
    "\n",
    "    # extract the name from the path\n",
    "    p = pathlib.Path(name)\n",
    "    name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "    # Now that we have the train and test datasets, we set up a gridsearch of the K-NN classifier\n",
    "    # using SciKitLearn and print the results \n",
    "    # params = {\"n_neighbors\": range(1, 50)}\n",
    "    # model = GridSearchCV(KNeighborsClassifier(), params)\n",
    "    \n",
    "    if( model == None ):\n",
    "        model = KNeighborsClassifier()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    report = classification_report(y_test, prediction)\n",
    "    print(report)\n",
    "    print(f'\\tAccuracy: {accuracy_score(y_test, prediction)}\\n')\n",
    "\n",
    "    # print(\"Best Parameters found by gridsearch:\")\n",
    "    # print(model.best_params_)\n",
    "\n",
    "\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    if len(classes) == 2:\n",
    "        model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "\n",
    "    model_data: Model_data = Model_data(name, model, classes, X_train, y_train, X_test, y_test, to, dls, f'K_Nearest_Neighbors')\n",
    "\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # for analysis and further experimentation\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_and_split_data(df: pd.DataFrame, target_label: str, split=0, name='', categorical : list = ['Protocol'], scale:bool = False, leave_out: list = []) -> Model_data:\n",
    "    '''\n",
    "        Transform and split the data into a train and test set\n",
    "        returns the 10-tuple with the following indicies:\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var             : str  = target_label\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]) - set(untouched_features))\n",
    "\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    if(scale): \n",
    "        procs.append(Normal)\n",
    "\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\n",
    "    dls = to.dataloaders(bs=64)\n",
    "    model = tabular_learner(dls)\n",
    "    classes : list = list(model.dls.vocab)\n",
    "\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    if len(classes) == 2:\n",
    "        model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        model.target_type_ = 'single'\n",
    "        \n",
    "    model_data: Model_data = Model_data(name, model, classes, X_train, y_train, X_test, y_test, to, dls, 'Transformed_and_Split_data')\n",
    "\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # for analysis and further experimentation\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validated Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_datum = namedtuple('Model_datum', ['models', 'results'])\n",
    "\n",
    "# Todo: create crossvalidated experiments for resnet, tabnet, and models with a sklearn api\n",
    "\n",
    "\n",
    "\n",
    "def run_cross_validated_deep_nn_experiment(\n",
    "    df: pd.DataFrame, \n",
    "    name: str, \n",
    "    target_label: str, \n",
    "    shape: tuple, \n",
    "    k_folds: int = 10,\n",
    "    categorical = ['Protocol'],\n",
    "    dropout_rate: float = 0.01,\n",
    "    callbacks: list = [ShowGraphCallback],\n",
    "    metrics: list or None= None,\n",
    "    procs: list = [FillMissing, Categorify, Normalize],\n",
    "    experiment_type: str or None = None,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    lr_choice: str = 'valley'\n",
    ") -> list:\n",
    "    '''\n",
    "        Function will fit a deep neural network to the given dataset using cross-validation\n",
    "    '''\n",
    "    metrics_ = metrics\n",
    "    lr_choice = {\n",
    "        'valley': 0,\n",
    "        'slide': 1,\n",
    "        'steep': 2,\n",
    "        'minimum': 3,\n",
    "    }[lr_choice]\n",
    "\n",
    "    if(experiment_type is None):\n",
    "        experiment_type = f'Deep_NN_{shape[0]}x{shape[1]}'\n",
    "\n",
    "    p = pathlib.Path(name)\n",
    "    name: str = str(p.parts[-1])\n",
    "    dep_var: str = target_label\n",
    "\n",
    "    print('Shape of input dataframe:', df.shape)\n",
    "    print(f\"Running {k_folds}-fold cross-validation\")\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]) - set(untouched_features))\n",
    "\n",
    "\n",
    "\n",
    "    ss = StratifiedShuffleSplit(n_splits=k_folds, random_state=seed, test_size=1/k_folds)\n",
    "\n",
    "\n",
    "    model_data_list  : list = [0]*k_folds\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "        fold_results: dict = {'loss': [], 'accuracy': [], 'BalancedAccuracy': [], 'roc_auc': [], 'MCC': [], 'f1': [], 'precision': [], 'recall': [], 'all': []}\n",
    "    else:\n",
    "        fold_results: dict = {'all': []}\n",
    "\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(ss.split(df.copy().drop(target_label, axis=1), df[target_label])):\n",
    "        \n",
    "        fold_name = f'{name}_fold_{i+1}'\n",
    "\n",
    "        print(i)\n",
    "        print(f'Train Index: {train_index.shape}')\n",
    "        print(f'Test Index: {test_index.shape}')\n",
    "\n",
    "        splits = IndexSplitter(test_index)(df)\n",
    "        to = TabularPandas(\n",
    "            df            , y_names=dep_var                , \n",
    "            splits=splits , cat_names=categorical_features ,\n",
    "            procs=procs   , cont_names=continuous_features , \n",
    "        )\n",
    "\n",
    "        dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "        model = tabular_learner(\n",
    "            dls, \n",
    "            layers=list(shape), \n",
    "            metrics=metrics, \n",
    "            cbs=callbacks,\n",
    "        )\n",
    "\n",
    "        model.model.emb_drop = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        lr = model.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "        model.fit_one_cycle(epochs, lr[lr_choice])\n",
    "        model.save(f'{name}.model')\n",
    "\n",
    "        model_results = model.validate()\n",
    "\n",
    "        if(metrics_ == None):\n",
    "            fold_results['loss'].append(model_results[0])\n",
    "            fold_results['accuracy'].append(model_results[1])\n",
    "            fold_results['BalancedAccuracy'].append(model_results[2])\n",
    "            fold_results['roc_auc'].append(model_results[3])\n",
    "            fold_results['MCC'].append(model_results[4])\n",
    "            fold_results['f1'].append(model_results[5])\n",
    "            fold_results['precision'].append(model_results[6])\n",
    "            fold_results['recall'].append(model_results[7])\n",
    "            fold_results['all'].append(model_results)\n",
    "            print(f'loss: {model_results[0]}, accuracy: {model_results[1]*100}%')\n",
    "        else:\n",
    "            fold_results['all'].append(model_results)\n",
    "            print(f'loss: {model_results[0]}')\n",
    "\n",
    "        X_train = to.train.xs.reset_index(drop=True)\n",
    "        X_test = to.valid.xs.reset_index(drop=True)\n",
    "        y_train = to.train.ys.values.ravel()\n",
    "        y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "        wrapped_model = SklearnWrapper(model)\n",
    "        wrapped_model._target_labels = dep_var\n",
    "        classes = list(model.dls.vocab)\n",
    "\n",
    "\n",
    "\n",
    "        # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "        if len(classes) == 2:\n",
    "            wrapped_model.target_type_ = 'binary'\n",
    "        elif len(classes) > 2:  \n",
    "            wrapped_model.target_type_ = 'multiclass'\n",
    "        else:\n",
    "            print('Must be more than one class to perform classification')\n",
    "            raise ValueError('Wrong number of classes')\n",
    "        \n",
    "\n",
    "\n",
    "        model_data: Model_data = Model_data(fold_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, experiment_type)\n",
    "        model_data_list[i] = model_data\n",
    "\n",
    "    model_datum: Model_datum = Model_datum(model_data_list, fold_results)\n",
    "\n",
    "    return model_datum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Preparation Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_classes_and_create_Xy_df(model_data: Model_data, target_label: str):\n",
    "    \"\"\"\n",
    "        Function takes a Model_data namedtuple and returns a dataframe with the X and decoded y data\n",
    "    \"\"\"\n",
    "    X = model_data.X_train\n",
    "    y = model_data.y_train\n",
    "    classes = model_data.classes\n",
    "\n",
    "    Xy_df = pd.DataFrame(X)\n",
    "    y_s: list = []\n",
    "    for x in y:\n",
    "        y_s.append(classes[x])\n",
    "    Xy_df[target_label] = y_s\n",
    "\n",
    "    return Xy_df\n",
    "\n",
    "\n",
    "def normalize_bilabel_dataset_between_0_1(df: pd.DataFrame, labels = ['Traffic Type', 'Application Type']) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function takes a dataframe and merges its labels before normalizing data. \n",
    "        The labels are then split back into their original form, but the merged label is kept for verification purposes.\n",
    "\n",
    "        returns a trilabel dataframe with the new label under the 'Merged Type' column\n",
    "    '''\n",
    "    inter_df = df.copy()\n",
    "\n",
    "    d1_y = inter_df[labels[0]]\n",
    "    d2_y = inter_df[labels[1]]\n",
    "\n",
    "    merged_y = pd.concat([d1_y, d2_y], axis=1)\n",
    "\n",
    "    merged: list = []\n",
    "    for x in zip(d1_y, d2_y):\n",
    "        merged.append(f'{x[0]}_{x[1]}')\n",
    "\n",
    "    inter_df['Merged Type'] = merged\n",
    "\n",
    "    normalized_model_data = transform_and_split_data(\n",
    "        inter_df,\n",
    "        'Merged Type',\n",
    "        split=0,\n",
    "        scale=True,\n",
    "        categorical=[],\n",
    "        leave_out=labels\n",
    "    )\n",
    "\n",
    "    merged_df = decode_classes_and_create_Xy_df(normalized_model_data, 'Merged Type')\n",
    "\n",
    "    merged_y = merged_df['Merged Type']\n",
    "    new_labels: list = []\n",
    "\n",
    "    for l in labels:\n",
    "        new_labels.append([])\n",
    "\n",
    "    for x in merged_y:\n",
    "        split_label = x.split('_')\n",
    "\n",
    "        for i in range(len(split_label)):\n",
    "            new_labels[i].append(split_label[i])\n",
    "\n",
    "    for i, x in enumerate(labels):\n",
    "        merged_df[x] = new_labels[i]\n",
    "\n",
    "    total_df = merged_df.copy()\n",
    "    total_df['Merged Type'] = merged_y\n",
    "\n",
    "\n",
    "    return total_df\n",
    "\n",
    "    \n",
    "\n",
    "def create_n_cloud_from_dataset(df: pd.DataFrame, n_points: int = 100, target_label: str = 'label', allow_partial: bool = False, segmented=True):\n",
    "    '''\n",
    "        Function takes a dataframe and splits it into point clouds with n_points each. \n",
    "            The generated clouds will all have the indicated number of points, discarding the remainder if allow_partial is False.\n",
    "            The generated clouds will be partitioned by the classes contained within the target_label column.\n",
    "            Each cloud will be labeled with the class it was generated from.\n",
    "\n",
    "            If the segmented parameter is true, the clouds will be split into df.shape[0]/n_points number of clouds.\n",
    "                This is because we segment the dataframe to produce the clouds.\n",
    "            if the segmented parameter is false, the clouds will be split into df.shape[0] - n_points number of clouds.\n",
    "                This is because we will be using a sliding interval to construct the clouds.\n",
    "\n",
    "        returns a numpy array of point clouds as well as a numpy array with class labels and class mappings.\n",
    "    '''\n",
    "\n",
    "    data = transform_and_split_data(df, target_label=target_label, split=0)\n",
    "    X = data.X_train.copy()\n",
    "    X[target_label] = data.y_train.copy()\n",
    "\n",
    "    classes = data.classes\n",
    "\n",
    "    # create n clouds by class\n",
    "    clouds          : list = []\n",
    "    clouds_y        : list = []\n",
    "    clouds_y_decoded: list = []\n",
    "\n",
    "    for i, x in enumerate(classes):\n",
    "        temp_df = X[X[target_label] == i]\n",
    "        if(segmented):\n",
    "            iterations: int = int(temp_df.shape[0]/n_points)\n",
    "        else:\n",
    "            iterations: int = temp_df.shape[0] - n_points\n",
    "\n",
    "\n",
    "        for j in range(int(iterations)):\n",
    "            if(segmented):\n",
    "                cloud = temp_df[j*n_points:(j+1)*n_points].drop(target_label, axis=1).values\n",
    "            else:\n",
    "                cloud = temp_df[j:j+n_points].drop(target_label, axis=1).values\n",
    "            \n",
    "            keep: bool = allow_partial and not segmented\n",
    "            if(len(cloud) == n_points or keep):\n",
    "                clouds.append(cloud)\n",
    "                clouds_y.append(i)\n",
    "                clouds_y_decoded.append(x)\n",
    "\n",
    "    clouds = np.array(clouds)\n",
    "    clouds_y = np.array(clouds_y)\n",
    "    clouds_y_decoded = np.array(clouds_y_decoded)\n",
    "\n",
    "    return clouds, clouds_y, classes, clouds_y_decoded\n",
    "\n",
    "\n",
    "def calculate_correlations(model_data: Model_data, target_label: str):\n",
    "    '''\n",
    "        Function merges together the encoded and standardized model data and labels to calculate pearson correlation\n",
    "    '''\n",
    "\n",
    "    encoded_data = model_data.X_train.copy()\n",
    "    encoded_data[target_label] = model_data.y_train\n",
    "\n",
    "    return encoded_data.corr()\n",
    "\n",
    "\n",
    "def extract_correlations(correlations: pd.DataFrame, feature: str, leave_out: list = ['Traffic Type', 'Application Type']) -> list:\n",
    "    '''\n",
    "        Function takes a correlation dataframe and extracts a list of features correlated with the input feature. \n",
    "            Anything in leave_out is not included in the list.\n",
    "    '''\n",
    "\n",
    "    correlation_order: list = list(correlations.sort_values(by=feature, ascending=False).index)\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in correlation_order:\n",
    "            correlation_order.remove(x)\n",
    "\n",
    "    return correlation_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical functions on the interval [0,1] with unary function proceeded by binary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unary functions\n",
    "\n",
    "def clamp_to_0_1(x):\n",
    "    return max(min(x, 1), 0)\n",
    "\n",
    "def p_2_pot(x):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using a second degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    return np.power(2, 2*(x-.5))\n",
    "\n",
    "def p_4_pot(x):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using a fourth degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    return np.power(4, 2*(x-.5))\n",
    "\n",
    "def p_6_pot(x):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using a sixth degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    return np.power(6, 2*(x-.5))\n",
    "\n",
    "def gaussian_spike(x):\n",
    "    return np.exp(-np.power(x, 2))\n",
    "\n",
    "def double_gaussian_spike(x):\n",
    "    ''' \n",
    "        A double gaussian spike shape in the interval [0,1]\n",
    "            gives larger changes in gradient to spread around input \n",
    "            in the interval [0,1]\n",
    "    '''\n",
    "\n",
    "    first_spike  = (-0.5) * gaussian_spike((53*x) - 2.2)\n",
    "    second_spike = gaussian_spike((2.7*x) - 2.2)\n",
    "\n",
    "    return first_spike + second_spike\n",
    "\n",
    "def sharp_zero_spike(x):\n",
    "    return 1 - np.exp(-np.power(np.power((200*x), 2)+0.0001, -1))\n",
    "\n",
    "def curve_over_x_1(x):\n",
    "    '''\n",
    "        A curve on the interval [0,1] that maps 0 to 0 and 1 to 1 but stays above\n",
    "            the line f(x) = x\n",
    "    '''\n",
    "\n",
    "    return x*np.exp(1-x)\n",
    "\n",
    "def curve_over_x_2(x):\n",
    "    return -(x - 1)*(x - 1) + 1\n",
    "\n",
    "\n",
    "# binary functions / operations\n",
    "# _ precedes names to prevent name conflicts\n",
    "\n",
    "def _dist_l1(x, y):\n",
    "    return np.abs(x - y)\n",
    "\n",
    "def _dist_l2(x, y):\n",
    "    return np.sqrt(np.power(x, 2) + np.power(y, 2))\n",
    "\n",
    "def _avg(x, y):\n",
    "    return (x + y) / 2\n",
    "\n",
    "def _mult(x, y):\n",
    "    return x * y\n",
    "\n",
    "def _add(x, y):\n",
    "    return x + y\n",
    "\n",
    "def _rbf_l1(x, y):\n",
    "    '''radial basis function'''\n",
    "    return np.exp(-_dist_l1(x, y)**2)\n",
    "\n",
    "def _rbf_l2(x, y):\n",
    "    '''radial basis function'''\n",
    "    return np.exp(-_dist_l2(x, y)**2)\n",
    "\n",
    "\n",
    "# closures allowing customizable functions\n",
    "\n",
    "def mult_by_n(n):\n",
    "    def f(x):\n",
    "        return n * x\n",
    "    return f\n",
    "\n",
    "\n",
    "def p_n_pot(n):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using an nth-degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    def f(x):\n",
    "        return np.power(n, 2*(x-.5))\n",
    "    return f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to automate the Identity-oriented Tabular Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_single_channel_image_matrix(\n",
    "    df_sample: pd.DataFrame, \n",
    "    row_ordering: list, \n",
    "    column_ordering: list, \n",
    "    padding: tuple, \n",
    "    identity: float = 1, \n",
    "    operation: callable = lambda x, y: x*y,\n",
    "    activation: callable = lambda x: x,\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Function will create a len(row_ordering) x len(column_ordering) matrix\n",
    "        representing the input sample.\n",
    "    \n",
    "    This uses the Identity-oriented Tabular Image Transformation algorithm to produce an image from tabular data:\n",
    "            From the sample, two vectors reordered by the specified row and column ordering are padded with identity values\n",
    "                before the operation is then applied to the generalised outer product of the vectors.\n",
    "\n",
    "            Since we have an identity (or constant value if an identity does not exist), the top right corner with a size dictated \n",
    "                by the padding parameter will have a monochromatic square the allows classifiers to understand the orientation \n",
    "                of the image.\n",
    "\n",
    "            The padding[0]xlen(row_ordering) and len(column_ordering)xpadding[1] submatrices will contain feature bands that \n",
    "                are meant to communicate the true values of the input features to the classifier\n",
    "\n",
    "            The remaining interior submatrix will contain the nested feature geometry imposed by the operation on the \n",
    "                outer product of the feature vectors ordered by the input row_ordering and column_ordering vectors.\n",
    "\n",
    "            An activation function is called on the output of the operation in order to tune the output before being clamped \n",
    "                to the interval [0,1]. \n",
    "\n",
    "            The produced image is called a Symmetric Identity-oriented Tabular Image if and only if these conditions are met\n",
    "                1) the row_ordering and column_ordering vectors are the same or are in reversed order\n",
    "                2) padding[0] = padding[1]\n",
    "                3) the operation (binary function) being performed is commutative, i.e. f(a, b) = f(b, a)\n",
    "\n",
    "            The produced image is called an Asymmetric Identity-oriented Tabular Image if it does not fulfill one of the above conditions.\n",
    "    '''\n",
    "\n",
    "    row_vector    = [identity] * padding[0] + list(df_sample.reindex(columns=row_ordering   ).values[0])\n",
    "    column_vector = [identity] * padding[1] + list(df_sample.reindex(columns=column_ordering).values[0])\n",
    "\n",
    "    product: callable = lambda x, y: clamp_to_0_1(activation(operation(x, y)))\n",
    "    composed_outer = np.frompyfunc(product, 2, 1)\n",
    "\n",
    "    pic = composed_outer.outer(row_vector, column_vector).astype(np.float64)\n",
    "\n",
    "    return pic\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using Linear Feature Reduction through Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_data = namedtuple(\n",
    "    'pca_data',\n",
    "    ['title', 'X_train', 'y_train', 'Xy', 'components', 'n_components', 'classes', 'target_label']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_1D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes the 10-tuple model_data from the transform and split function and uses Principal component analysis\n",
    "            to break up the feature dimensions into a single dimension for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=1)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\"]\n",
    "\n",
    "    _X              = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X           = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns   = _X.columns\n",
    "    new_X[\"filler\"] = 0\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=cluster[\"component_1\"],\n",
    "                y=cluster[\"filler\"],\n",
    "                mode='markers',\n",
    "                marker = dict(color = label_colors[i]),\n",
    "                name=classes[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5,zeroline= False\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= '',\n",
    "            ticklen= 5,\n",
    "            zeroline= False\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into two dimension  for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=2)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\"]\n",
    "\n",
    "\n",
    "    _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns = _X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_3D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into three dimensions for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=3)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\"]\n",
    "\n",
    "\n",
    "    _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns = _X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                z      = cluster[\"component_3\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_4D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into four dimensions for cluster analysis.\n",
    "            Displays a 2 two-dimensional graphs of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=4)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\", \"component_4\"]\n",
    "\n",
    "\n",
    "    _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns = _X.columns\n",
    "\n",
    "\n",
    "    traces1: list = []\n",
    "    traces2: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces1.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "        traces2.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_3\"]    ,\n",
    "                y      = cluster[\"component_4\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout1 = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout2 = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 3',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 4',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces1, 'layout': layout1})\n",
    "    iplot({'data': traces2, 'layout': layout2})\n",
    "\n",
    "\n",
    "def pca_to_model_data(pca_data: PCA_data) -> Model_data:\n",
    "    '''\n",
    "        Takes a PCA_data namedtuple and returns a model_data namedtuple\n",
    "    '''\n",
    "\n",
    "    model_data: Model_data = Model_data(\n",
    "        pca_data.title,\n",
    "        None,\n",
    "        pca_data.classes,\n",
    "        pca_data.X_train,\n",
    "        pca_data.y_train,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        'pca'\n",
    "    )\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "def run_pca_on_dataset(model_data: tuple, target_label: str = 'label',  title='PCA data', components: int = 6) -> PCA_data:\n",
    "\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    # num_labels   = len(X.groupby(target_label).size())\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=components)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X))\n",
    "    component.columns = ['PCA_' + str(i+1) for i in range(components)]\n",
    "\n",
    "    for col in component.columns:\n",
    "        X[col] = component[col]\n",
    "\n",
    "\n",
    "    Xy = decode_classes_and_create_Xy_df(Model_data(None, None, classes, component.copy(), model_data.y_train, None, None, None, None, 'pca'), target_label)\n",
    "    component[target_label] = model_data.y_train\n",
    "\n",
    "    pca_data: PCA_data = PCA_data(title, X, model_data.y_train, Xy, component, components, classes, target_label)\n",
    "\n",
    "\n",
    "\n",
    "    return pca_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using Non-Linear Feature Reduction through T-Stochastic Neighbor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clusters_by_label_1D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into a single dimension for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X           = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns   = X.columns\n",
    "    new_X[\"filler\"] = 0\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=1, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\"]\n",
    "    new_X             = pd.concat([X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=cluster[\"component_1\"],\n",
    "                y=cluster[\"filler\"],\n",
    "                mode='markers',\n",
    "                marker = dict(color = label_colors[i]),\n",
    "                name=classes[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5,zeroline= False\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= '',\n",
    "            ticklen= 5,\n",
    "            zeroline= False\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into two dimension  for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=2, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(columns=['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\"]\n",
    "    new_X             = pd.concat([new_X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_3D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into three dimensions for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=3, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\"]\n",
    "    new_X             = pd.concat([new_X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                z      = cluster[\"component_3\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title    = 'Component 1',\n",
    "            ticklen  = 5            ,\n",
    "            zeroline = False        ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title    = 'Component 2',\n",
    "            ticklen  = 5            ,\n",
    "            zeroline = False        ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using tools built using Topological Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topology_data = namedtuple(\n",
    "    'topology_data',\n",
    "    ['title', 'clouds', 'clouds_y', 'persistence', 'fig', 'features', 'classes', 'target_label', 'clouds_y_decoded', 'Xy']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_single_batch_dataset_topology(model_data: tuple, samples: int or None = 1000, dimensions: tuple = (0,1), title=''):\n",
    "    \"\"\"\n",
    "    Function calculates the topology of the dataset and returns it\n",
    "        requires data to be transformed by fastai first to properly encode and normalize the\n",
    "        data.\n",
    "\n",
    "    Returns a namedtuple with the following fields:\n",
    "        title: Graph title\n",
    "        persistence: Vietoris-Rips Peristence\n",
    "        fig: plotly figure\n",
    "        features: Persistence Entropy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if(model_data.name != '' and title == ''):\n",
    "        title = model_data.name\n",
    "\n",
    "\n",
    "    if(samples is None):\n",
    "        X = model_data.X_train.values\n",
    "    else:\n",
    "        X = model_data.X_train.sample(samples).values\n",
    "        \n",
    "    X = np.array([X])\n",
    "\n",
    "    # calculate the topology\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=dimensions)\n",
    "    topology = VR.fit_transform(X)\n",
    "\n",
    "    # calculate the persistence diagram\n",
    "    fig = plot_diagram(topology[0])\n",
    "\n",
    "    # calculate the entropy\n",
    "    PE = PersistenceEntropy()\n",
    "    entropy = PE.fit_transform(topology)\n",
    "\n",
    "    # package in topological_data named tuple. We do this so data can be accessed using [i] or . operators\n",
    "    topological_data: Topology_data = Topology_data(title, X, None, topology, fig, entropy, model_data.classes, '', None, None)\n",
    "\n",
    "    return topological_data\n",
    "\n",
    "\n",
    "def run_tda_on_dataset(df: pd.DataFrame, target_label: str = 'label',  title='Clusters by Label', n_points: int = 100, dimension: tuple = (0,1), allow_partial: bool = False, segmented: bool = True, jobs: int = None) -> Topology_data:\n",
    "    '''\n",
    "        Function takes a dataset and breaks it into n_point clouds, where n_point is the number of points in each cloud.\n",
    "            Then, the giotto-tda Topological Data Analysis toolkit is used to generate a \n",
    "            persistence module for each cloud. This module describes the topological structure of the cloud.\n",
    "            The topological diagrams have their entropy calculated to break up the feature dimensions into\n",
    "            the indicated dimensions corresponding to topological features like connectivity(0d), loopiness(1d), caviatation(2d), etc\n",
    "            for cluster analysis.\n",
    "            Allow_partial is a boolean that determines whether or not to allow clouds to be less \n",
    "            than n_points if there arent n_points remaining.\n",
    "\n",
    "        Returns the topological data for the dataset\n",
    "\n",
    "        Topology_data = namedtuple(\n",
    "           'topology_data',\n",
    "            ['title', 'clouds', 'clouds_y', 'persistence', 'fig', 'features', 'classes', 'target_label', 'clouds_y_decoded']\n",
    "        )\n",
    "    '''\n",
    "\n",
    "\n",
    "    clouds, clouds_y, classes, clouds_y_decoded = create_n_cloud_from_dataset(df, n_points=n_points, target_label=target_label, allow_partial=allow_partial, segmented=segmented)\n",
    "\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=dimension, n_jobs=jobs)\n",
    "    PE = PersistenceEntropy()\n",
    "\n",
    "    topologies = VR.fit_transform(clouds)\n",
    "\n",
    "    entropies = pd.DataFrame(PE.fit_transform(topologies))\n",
    "\n",
    "    col: list = []\n",
    "    for n in dimension:\n",
    "        col.append(f'{n}d entropy')\n",
    "    entropies.columns = col\n",
    "\n",
    "    Xy = entropies.copy()\n",
    "    Xy[target_label] = clouds_y_decoded\n",
    "\n",
    "\n",
    "    entropies[target_label] = clouds_y\n",
    "\n",
    "    topology_data: Topology_data = Topology_data(title, clouds, clouds_y, topologies, None, entropies, classes, target_label, clouds_y_decoded, Xy)\n",
    "\n",
    "    return topology_data\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_tda(df: pd.DataFrame, target_label: str = 'label',  title='Clusters by Label', n_points: int = 100, allow_partial: bool = False) -> Topology_data:\n",
    "    '''\n",
    "        Function takes a dataset and breaks it into n_point clouds, where n_point is the number of points in each cloud.\n",
    "            Then, the giotto-tda Topological Data Analysis toolkit is used to generate a \n",
    "            persistence module for each cloud. This module describes the topological structure of the cloud.\n",
    "            The topological diagrams have their entropy calculated to break up the feature dimensions into\n",
    "            two dimensions corresponding to connectivity and loopinessfor cluster analysis.\n",
    "            Allow_partial is a boolean that determines whether or not to allow clouds to be less \n",
    "            than n_points if there arent n_points remaining.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        Returns the topological data for the dataset\n",
    "    '''\n",
    "\n",
    "\n",
    "    clouds, clouds_y, classes, clouds_y_decoded = create_n_cloud_from_dataset(df, n_points=n_points, target_label=target_label, allow_partial=allow_partial)\n",
    "\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=(0,1))\n",
    "    PE = PersistenceEntropy()\n",
    "\n",
    "    topologies      = VR.fit_transform(clouds)\n",
    "\n",
    "    X               = pd.DataFrame(PE.fit_transform(topologies))\n",
    "    X.columns       = [\"Connectivity\", \"Loopiness\"]\n",
    "    X[target_label] = clouds_y\n",
    "    num_labels      = len(X.groupby(target_label).size())\n",
    "    label_colors    = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = X[X[target_label] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"Connectivity\"]   ,\n",
    "                y      = cluster[\"Loopiness\"]      ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Connectivity',\n",
    "            ticklen= 5           ,\n",
    "            zeroline= False      ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Loopiness',\n",
    "            ticklen= 5        ,\n",
    "            zeroline= False   ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = {'data': traces, 'layout': layout}\n",
    "\n",
    "    iplot(fig)\n",
    "\n",
    "    topology_data: Topology_data = Topology_data(title, clouds, clouds_y, topologies, fig, X, classes, target_label, clouds_y_decoded, None)\n",
    "\n",
    "    return topology_data\n",
    "\n",
    "\n",
    "def extract_topological_correlations(topology_data: Topology_data, original_dataset: pd.DataFrame, label: str):\n",
    "    '''\n",
    "        Takes a Topology_data namedtuple and returns a dictionary of lists of features ordered topological correlations \n",
    "            for each value in the topology_data.entropy dataframe with the values of the points in the point clouds\n",
    "            being used to calculate the persistence entropy\n",
    "    '''        \n",
    "\n",
    "    cols = list(original_dataset.columns)\n",
    "    if(label in cols):\n",
    "        cols.remove(label)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    new_cols = list(topology_data.features.columns)\n",
    "    for i, x in enumerate(topology_data.clouds):\n",
    "        new_df = pd.DataFrame(x)\n",
    "        for col in new_cols:\n",
    "            new_df[col] = topology_data.features[col][i]\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "    df.columns = cols + new_cols\n",
    "\n",
    "    correlation = df.corr()\n",
    "\n",
    "    correlations: dict = {}\n",
    "    for col in new_cols:\n",
    "        if col != label:\n",
    "            correlations[col] = extract_correlations(correlation, col)\n",
    "\n",
    "    for cor in correlations.keys():\n",
    "        for col in new_cols:\n",
    "            if col in correlations[cor]:\n",
    "                correlations[cor].remove(col)\n",
    "\n",
    "\n",
    "    correlations['Graph'] = plt.figure(figsize=(20,15))\n",
    "    sns.heatmap(correlation, vmax=1, square=True, cmap='YlGnBu')\n",
    "    plt.title(\"Topographic Correlation Heatmap for n_clouds Generated by Traffic Type\", fontsize=16)\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "\n",
    "def top_to_model_data(topology_data: Topology_data) -> Model_data:\n",
    "    '''\n",
    "        Takes a Topology_data namedtuple and returns a model_data namedtuple\n",
    "    '''\n",
    "\n",
    "    X_train = topology_data.features.copy()\n",
    "\n",
    "    if topology_data.target_label in X_train.columns:\n",
    "        X_train.drop(topology_data.target_label, axis=1, inplace=True)\n",
    "\n",
    "    model_data: Model_data = Model_data(\n",
    "        topology_data.title,\n",
    "        None,\n",
    "        topology_data.classes,\n",
    "        X_train,   \n",
    "        topology_data.clouds_y,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        'topology'\n",
    "    )\n",
    "\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools to visualize the results of classification experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a confusion matrix\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ConfusionMatrix(model_data[1], classes=model_data[2], title=model_data[0], ax=ax, cmap=cmap, is_fitted=True)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    \n",
    "    return visualizer\n",
    "\n",
    "\n",
    "def visualize_roc(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a \n",
    "        Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ROCAUC(model_data[1], classes=model_data[2], title=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    \n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def visualize_pr_curve(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a \n",
    "        Precision-Recall Curve\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.PrecisionRecallCurve(model_data[1], title=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def visualize_report(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a report\n",
    "        detailing the Precision, Recall, f1, and Support scores for all \n",
    "        classification outcomes\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ClassificationReport(model_data[1], classes=model_data[2], title=model_data[0], support=True, ax=ax, cmap=cmap)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def visualize_class_balance(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a histogram\n",
    "        detailing the balance between classification outcomes\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.target.ClassBalance(labels=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.fit(model_data[4], model_data[6])\n",
    "    \n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def confusion_matrix_from_dataset(model_data: tuple, df: pd.DataFrame, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses the model to classify\n",
    "            the data passed in as the dataframe. Produces a confusion matrix\n",
    "            (currently only confirmed to work with fastai models)\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    dl = model_data[1].model.dls.test_dl(df, bs=64)\n",
    "    preds, v, dec_preds = model_data[1].model.get_preds(dl=dl, with_decoded=True)\n",
    "\n",
    "    visualizer = yb.classifier.ConfusionMatrix(model_data[1], classes=model_data[2], title=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.score(dl.xs, dl.y)\n",
    "    acc = accuracy_score(dl.y, dec_preds)\n",
    "    print(f'Accuracy: {acc}')\n",
    "\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "def show_stacked_graphs(figures: list, stack_shape: tuple = (1,1), title: str = '', individual_titles: list or str = None) -> widgets.VBox: \n",
    "    \"\"\"\n",
    "    Function takes a list of figures and stacks them in a grid.\n",
    "        stack_shape is a tuple of the number of rows and columns in the grid and\n",
    "        the product of the indices must equal the number of figures.\n",
    "        individual_titles allows one to specify titles for each figure, or a single title for all figures.\n",
    "\n",
    "    Returns a plotly figure with the input figures stacked as indicated\n",
    "    \"\"\"\n",
    "\n",
    "    if(individual_titles is not None and type(individual_titles) is not str):\n",
    "        assert len(figures) == len(individual_titles)     , \"Number of titles must match number of figures\"\n",
    "    assert len(figures) == stack_shape[0] * stack_shape[1], \"Number of figures must match stack shape\"\n",
    "\n",
    "    vertical_stack  : list = []\n",
    "    horizantal_stack: list = []\n",
    "\n",
    "    if(title != ''):\n",
    "        vertical_stack.append(widgets.Label(title))\n",
    "    \n",
    "    for i in range(stack_shape[1]):\n",
    "        for j in range(stack_shape[0]):\n",
    "                \n",
    "            temp_fig = go.FigureWidget(figures[i * stack_shape[0] + j].data)\n",
    "            if(type(individual_titles) == str):\n",
    "                temp_fig.layout.title = individual_titles\n",
    "            elif(type(individual_titles) == list):\n",
    "                temp_fig.layout.title = individual_titles[i * stack_shape[0] + j]\n",
    "                \n",
    "            horizantal_stack.append(temp_fig)\n",
    "        vertical_stack.append(widgets.HBox(horizantal_stack))\n",
    "        horizantal_stack = []\n",
    "\n",
    "    fig = widgets.VBox(vertical_stack)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D(model_data: Model_data, x_col: str, y_col: str, target_label: str = 'label', samples=25000, title='Clusters by Label', ) -> dict:\n",
    "    '''\n",
    "        Takes a model_data tuple and displays the indicated columns of the X_train data as a scatter plot.\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X[target_label]   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby(target_label).size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "\n",
    "    # _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X[target_label] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[x_col]            ,\n",
    "                y      = cluster[y_col]            ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis = dict(\n",
    "            title    = x_col,\n",
    "            ticklen  = 5    ,\n",
    "            zeroline = False,\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            title    = y_col,\n",
    "            ticklen  = 5    ,\n",
    "            zeroline = False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig: dict = {'data': traces, 'layout': layout}\n",
    "    fig = go.Figure(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def faceted_clusters_by_label_2D(model_data: Model_data, x_col: str, y_col: str, target_label: str = 'label', samples=25000, title='Clusters by Label', ) -> dict:\n",
    "    '''\n",
    "        Takes a model_data tuple and returns a list of figures that each contain a highlighted value in the label with the other values\n",
    "            serving as a grey background in a scatter plot that demonstrates the distribution\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X[target_label]   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby(target_label).size()) + 1\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.7)\n",
    "\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "    figs: list = []\n",
    "    for i, x in enumerate(classes):\n",
    "\n",
    "        cluster_1 = new_X[new_X[target_label] == i]\n",
    "        cluster_2 = new_X[new_X[target_label] != i]\n",
    "\n",
    "        traces: list = []\n",
    "\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster_2[x_col]                     ,\n",
    "                y      = cluster_2[y_col]                     ,\n",
    "                marker = {'color': 'rgba(170, 170, 170, 0.5)'},\n",
    "                mode   = 'markers'                            ,\n",
    "                name   = 'Other'                              ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster_1[x_col]          ,\n",
    "                y      = cluster_1[y_col]          ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = x                         ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        layout = dict(\n",
    "            title = title,\n",
    "            xaxis = dict(\n",
    "                title    = x_col,\n",
    "                ticklen  = 5    ,\n",
    "                zeroline = False,\n",
    "            ),\n",
    "            yaxis = dict(\n",
    "                title    = y_col,\n",
    "                ticklen  = 5    ,\n",
    "                zeroline = False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig = go.Figure({'data': traces, 'layout': layout})\n",
    "        # a, b = fig.data\n",
    "        # fig.data = b, a\n",
    "        figs.append(fig)\n",
    "\n",
    "    return figs\n",
    "\n",
    "\n",
    "def visualize_side_by_side(\n",
    "    model_datum: list,\n",
    "    results: dict,\n",
    "    title: str = \"Confusion Matrices\",\n",
    "    model_descriptions: list or None = None,\n",
    "    plotting_function: callable = visualize_confusion_matrix,\n",
    "    shape: tuple = (2,5),\n",
    "    size: tuple = (20,10),\n",
    "    x_label: str = 'Predicted',\n",
    "    y_label: str = 'True',\n",
    ") -> tuple:\n",
    "    '''\n",
    "        Function will take the plotting function and execute it on each Model_data tuple passed in through the model_datum list\n",
    "            The plots will be oriented in a subplot grid with the number of rows and columns specified by the shape tuple\n",
    "            average accuracy will be calculated and displayed in the subtitle of the figure\n",
    "            \n",
    "    '''\n",
    "\n",
    "    print('Ignore yellowbrick warnings, this is a side-effect of using the sklearn wrapper on the fastai model')\n",
    "    rows = shape[0]\n",
    "    cols = shape[1]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=size)\n",
    "    fig.suptitle(title)\n",
    "    fig.text(0.5, .001, f'    Average Accuracy: {np.average(results[\"accuracy\"]) * 100: .4f}', ha='center')\n",
    "\n",
    "    viz: list = [0] * len(model_datum)\n",
    "    for i in range(rows*cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        if i < len(model_datum):\n",
    "            if(rows == 1):\n",
    "                current_ax = ax[col]\n",
    "            else:\n",
    "                current_ax = ax[row][col]\n",
    "            viz[i] = plotting_function(model_datum[i], ax=current_ax)\n",
    "            viz[i].finalize()\n",
    "            if model_descriptions is not None:\n",
    "                current_ax.set_title(model_descriptions[i])\n",
    "\n",
    "        if(row == rows-1):\n",
    "            current_ax.set_xlabel(x_label)\n",
    "        else:\n",
    "            current_ax.set_xlabel('')\n",
    "            current_ax.xaxis.set_ticklabels([])\n",
    "\n",
    "        if(col == 0):\n",
    "            current_ax.set_ylabel(y_label)\n",
    "        else:\n",
    "            current_ax.set_ylabel('')\n",
    "            current_ax.yaxis.set_ticklabels([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def show_feature_importance_TA(model: Learner, df: pd.DataFrame, ax = None, figsize = (15,5), title='Feature Importances') -> tuple:\n",
    "    '''\n",
    "        Function shows the feature importance calculated by the attention layer of the TabNet model.\n",
    "\n",
    "    '''\n",
    "\n",
    "    dl = model.dls.test_dl(df, bs=64)\n",
    "    feature_importances = tabnet_feature_importances(model.model, dl)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.bar(dl.x_names, feature_importances, color='g')\n",
    "    ax.set_xticks(range(len(feature_importances)))\n",
    "    ax.set_xticklabels(dl.x_names, rotation=90)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def plot_point_clouds(clouds: list, shape: tuple = (1, 4), titles: list or None = None, homology_dimensions: list or None = None, color='magma'):\n",
    "\n",
    "\n",
    "    specs = np.zeros(shape).astype(object)\n",
    "    cols = shape[1]\n",
    "\n",
    "    for i in range(specs.shape[0]):\n",
    "        for j in range(specs.shape[1]):\n",
    "            specs[i, j] = {'type': 'scene'}\n",
    "\n",
    "\n",
    "    fig = make_subplots(rows=shape[0], cols=shape[1], specs=specs.tolist(), column_titles=titles)\n",
    "\n",
    "    for i, cloud in enumerate(clouds):\n",
    "        row = (i // cols) + 1\n",
    "        col = (i % cols) + 1\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            # x=cloud[0, :],\n",
    "            # y=cloud[1, :],\n",
    "            # z=cloud[2, :],\n",
    "            x=cloud[:, 0],\n",
    "            y=cloud[:, 1],\n",
    "            z=cloud[:, 2],\n",
    "            showlegend=False,\n",
    "            mode=\"markers\",\n",
    "            marker={\"size\": 4,\n",
    "                    \"color\": list(range(cloud.shape[0])),\n",
    "                    \"colorscale\": color,\n",
    "                    \"opacity\": 0.8}\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_persistence_diagrams(diagrams: list, shape: tuple = (1, 4), titles: list or None = None, homology_dimensions: list or None = None, colors: list or None = None):\n",
    "    '''\n",
    "        Function plots a set of persistence diagrams\n",
    "    '''\n",
    "\n",
    "\n",
    "    cols = shape[1]\n",
    "    fig = make_subplots(rows=shape[0], cols=cols, column_titles=titles)\n",
    "\n",
    "    if homology_dimensions is None:\n",
    "        homology_dimensions = np.unique(diagrams[0][:, 2])\n",
    "\n",
    "    if colors is None:\n",
    "        colors = get_n_color_list(len(homology_dimensions), opacity=.8)\n",
    "\n",
    "    for i, diagram in enumerate(diagrams):\n",
    "        row = (i // cols) + 1\n",
    "        col = (i % cols) + 1\n",
    "\n",
    "\n",
    "        diagram = diagram[diagram[:, 0] != diagram[:, 1]]\n",
    "        diagram_no_dims = diagram[:, :2]\n",
    "        posinfinite_mask = np.isposinf(diagram_no_dims)\n",
    "        neginfinite_mask = np.isneginf(diagram_no_dims)\n",
    "        max_val = np.max(np.where(posinfinite_mask, -np.inf, diagram_no_dims))\n",
    "        min_val = np.min(np.where(neginfinite_mask, np.inf, diagram_no_dims))\n",
    "        parameter_range = max_val - min_val\n",
    "        extra_space_factor = 0.02\n",
    "        has_posinfinite_death = np.any(posinfinite_mask[:, 1])\n",
    "\n",
    "        if has_posinfinite_death:\n",
    "            posinfinity_val = max_val + 0.1 * parameter_range\n",
    "            extra_space_factor += 0.1\n",
    "\n",
    "        extra_space = extra_space_factor * parameter_range\n",
    "        min_val_display = min_val - extra_space\n",
    "        max_val_display = max_val + extra_space\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = [min_val_display, max_val_display],\n",
    "                y = [min_val_display, max_val_display],\n",
    "                mode       = \"lines\",\n",
    "                line       = {\"dash\": \"dash\", \"width\": 1, \"color\": \"black\"},\n",
    "                showlegend = False,\n",
    "                hoverinfo  = \"none\"\n",
    "            ), \n",
    "            row = row, \n",
    "            col = col\n",
    "        )\n",
    "\n",
    "        for k, dim in enumerate(homology_dimensions):\n",
    "            name = f\"{int(dim)}d Persistence\" if dim != np.inf else \"Persistence\"\n",
    "            subdiagram = diagram[diagram[:, 2] == dim]\n",
    "            unique, inverse, counts = np.unique(subdiagram, axis=0, return_inverse=True, return_counts=True)\n",
    "            \n",
    "            hovertext = [\n",
    "                f\"{tuple(unique[unique_row_index][:2])}\" + (\n",
    "                    f\", multiplicity: {counts[unique_row_index]}\" if counts[unique_row_index] > 1 else \"\"\n",
    "                ) for unique_row_index in inverse\n",
    "            ]\n",
    "\n",
    "            y = subdiagram[:, 1]\n",
    "            if has_posinfinite_death:\n",
    "                y[np.isposinf(y)] = posinfinity_val\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = subdiagram[:, 0], \n",
    "                    y = y, \n",
    "                    hoverinfo  = \"text\", hovertext=hovertext, name=name,\n",
    "                    marker     = {'color': colors[k]},\n",
    "                    mode       = \"markers\", \n",
    "                    showlegend = (i==0),\n",
    "                ), \n",
    "                row=row, \n",
    "                col=col\n",
    "            )\n",
    "\n",
    "\n",
    "        if(row == (len(diagrams)//cols)):\n",
    "            fig.update_xaxes(title_text=\"Feature Birth\", row=row, col=col)\n",
    "        \n",
    "        if(col == 1):\n",
    "            fig.update_yaxes(title_text=\"Feature Death\", row=row, col=col)\n",
    "\n",
    "\n",
    "        if has_posinfinite_death:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [min_val_display, max_val_display],\n",
    "                    y = [posinfinity_val, posinfinity_val],\n",
    "                    line = {\"dash\": \"dash\", \"width\": 0.5, \"color\": \"black\"},\n",
    "                    hoverinfo = \"none\",\n",
    "                    showlegend = True,\n",
    "                    name = u\"\\u221E\",\n",
    "                    mode = \"lines\",\n",
    "                ), \n",
    "                row=row, \n",
    "                col=col\n",
    "            )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_point_clouds_and_persistence_diagram(point_clouds: list, persistence_diagrams: list, rows: int, columns: int, homology_dimensions: list or None = None, color='magma', column_titles: list or None = None):\n",
    "\n",
    "\n",
    "\n",
    "    cols = columns\n",
    "\n",
    "    ri1 = {'type': 'scene'}\n",
    "    ri2 = {'type': 'xy'}\n",
    "    specs = np.zeros((2,4)).astype(object)\n",
    "\n",
    "    for i in range(specs.shape[0]):\n",
    "        for j in range(specs.shape[1]):\n",
    "            if(i == 0):\n",
    "                specs[i, j] = ri1\n",
    "            else:\n",
    "                specs[i, j] = ri2\n",
    "\n",
    "\n",
    "    if homology_dimensions is None:\n",
    "        homology_dimensions = np.unique(persistence_diagrams[0][:, 2])\n",
    "\n",
    "    label_colors = get_n_color_list(len(homology_dimensions), opacity=.8)\n",
    "\n",
    "\n",
    "    fig = make_subplots(rows=rows, cols=cols, specs=specs.tolist(), column_titles=column_titles)\n",
    "\n",
    "\n",
    "\n",
    "    for i, diagram in enumerate(persistence_diagrams):\n",
    "        row = (i // cols) + 1\n",
    "        col = (i % cols) + 1\n",
    "\n",
    "        if(i >= rows*cols):\n",
    "            break\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=point_clouds[i][0, :],\n",
    "            y=point_clouds[i][1, :],\n",
    "            z=point_clouds[i][2, :],\n",
    "            # x=point_clouds[i][:, 0],\n",
    "            # y=point_clouds[i][:, 1],\n",
    "            # z=point_clouds[i][:, 2],\n",
    "            showlegend=False,\n",
    "            mode=\"markers\",\n",
    "            marker={\"size\": 4,\n",
    "                    \"color\": list(range(point_clouds[j].shape[0])),\n",
    "                    \"colorscale\": color,\n",
    "                    \"opacity\": 0.8}\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "\n",
    "        row += 1\n",
    "\n",
    "        diagram = diagram[diagram[:, 0] != diagram[:, 1]]\n",
    "        diagram_num_dims = diagram[:, :2]\n",
    "        posinfinite_mask = np.isposinf(diagram_num_dims)\n",
    "        neginfinite_mask = np.isneginf(diagram_num_dims)\n",
    "        max_val = np.max(np.where(posinfinite_mask, -np.inf, diagram_num_dims))\n",
    "        min_val = np.min(np.where(neginfinite_mask, np.inf, diagram_num_dims))\n",
    "        parameter_range = max_val - min_val\n",
    "        extra_space_factor = 0.02\n",
    "        has_posinfinite_death = np.any(posinfinite_mask[:, 1])\n",
    "\n",
    "        if has_posinfinite_death:\n",
    "            posinfinity_val     = max_val + 0.1 * parameter_range\n",
    "            extra_space_factor += 0.1\n",
    "\n",
    "\n",
    "        extra_space     = extra_space_factor * parameter_range\n",
    "        min_val_display = min_val - extra_space\n",
    "        max_val_display = max_val + extra_space\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = [min_val_display, max_val_display],\n",
    "                y = [min_val_display, max_val_display],\n",
    "                mode = \"lines\",\n",
    "                line = {\"dash\": \"dash\", \"width\": 1, \"color\": \"black\"},\n",
    "                showlegend = False,\n",
    "                hoverinfo = \"none\"\n",
    "            ), \n",
    "            row = row, \n",
    "            col = col\n",
    "        )\n",
    "\n",
    "        for k, dim in enumerate(homology_dimensions):\n",
    "\n",
    "            name = f\"{int(dim)}d Persistence\" if dim != np.inf else \"Persistence\"\n",
    "            subdiagram = diagram[diagram[:, 2] == dim]\n",
    "            unique, inverse, counts = np.unique(subdiagram, axis=0, return_inverse=True, return_counts=True)\n",
    "            \n",
    "            hovertext = [\n",
    "                f\"{tuple(unique[unique_row_index][:2])}\" + (\n",
    "                    f\", multiplicity: {counts[unique_row_index]}\"\n",
    "                    if counts[unique_row_index] > 1 else \"\"\n",
    "                ) for unique_row_index in inverse\n",
    "            ]\n",
    "\n",
    "            y = subdiagram[:, 1]\n",
    "            if has_posinfinite_death:\n",
    "                y[np.isposinf(y)] = posinfinity_val\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = subdiagram[:, 0], \n",
    "                    y = y, \n",
    "                    name       = name     , \n",
    "                    mode       = \"markers\", \n",
    "                    hoverinfo  = \"text\"   , \n",
    "                    hovertext  = hovertext, \n",
    "                    showlegend = (i==0)   ,\n",
    "                    marker     = {'color': label_colors[k]},\n",
    "                ), \n",
    "                row = row, \n",
    "                col = col\n",
    "            )\n",
    "\n",
    "\n",
    "        if(row == 2):\n",
    "            fig.update_xaxes(title_text=\"Feature Birth\", row=row, col=col)\n",
    "        \n",
    "        if(col == 1):\n",
    "            fig.update_yaxes(title_text=\"Feature Death\", row=row, col=col)\n",
    "\n",
    "        # Add a horizontal dashed line for points with infinite death\n",
    "        if has_posinfinite_death:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[min_val_display, max_val_display],\n",
    "                y=[posinfinity_val, posinfinity_val],\n",
    "                mode=\"lines\",\n",
    "                line={\"dash\": \"dash\", \"width\": 0.5, \"color\": \"black\"},\n",
    "                showlegend=True,\n",
    "                name=u\"\\u221E\",\n",
    "                hoverinfo=\"none\"\n",
    "            ), row=row, col=col)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We will be looking at {len(file_set)} files:')\n",
    "pretty(file_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "We load the data and separate the dataset by label, giving us a traffic dataset and an application dataset. We will show correlations in both datasets, extract the correlations to labels or features, and use these to construct images from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1: dict = examine_dataset(1)\n",
    "dataset_2: dict = examine_dataset(2)\n",
    "# traffic_dataset_1    : dict = package_data_for_inspection_with_label(prune_dataset(dataset_1, ['Application Type']), 'Traffic_Dataset_1')\n",
    "# application_dataset_1: dict = package_data_for_inspection_with_label(prune_dataset(dataset_1, ['Traffic Type']), 'Application_Dataset_1')\n",
    "\n",
    "\n",
    "# dataset_2            : dict = package_data_for_inspection_with_label(reduce_feature_to_values(dataset_1['Dataset'], 'Traffic Type', ['Tor', 'VPN', 'Non-Tor'] ), 'Dataset_2')\n",
    "# traffic_dataset_2    : dict = package_data_for_inspection_with_label(prune_dataset(dataset_2, ['Application Type']), 'Traffic_Dataset_2')\n",
    "# application_dataset_2: dict = package_data_for_inspection_with_label(prune_dataset(dataset_2, ['Traffic Type']), 'Application_Dataset_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_model = CopulaGAN()\n",
    "traffic_model = CopulaGAN()\n",
    "application_model.fit(dataset_2['Dataset'])\n",
    "traffic_model.fit(dataset_1['Dataset'])\n",
    "application_model.save('./models/app_copulaGAN_model.pkl')\n",
    "traffic_model.save('./models/traff_copulaGAN_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traff_df =      traffic_model.sample(30000, conditions={'Traffic Type': 'Tor'})\n",
    "traff_df.append(traffic_model.sample(30000, conditions={'Traffic Type': 'VPN'}))\n",
    "traff_df.append(traffic_model.sample(30000, conditions={'Traffic Type': 'Regular'}))\n",
    "\n",
    "\n",
    "app_df =      application_model.sample(30000, conditions={'Application Type': 'audio-streaming'})\n",
    "app_df.append(application_model.sample(30000, conditions={'Application Type': 'browsing'}))\n",
    "app_df.append(application_model.sample(30000, conditions={'Application Type': 'chat'}))\n",
    "app_df.append(application_model.sample(30000, conditions={'Application Type': 'email'}))\n",
    "app_df.append(application_model.sample(30000, conditions={'Application Type': 'file-transfer'}))\n",
    "app_df.append(application_model.sample(30000, conditions={'Application Type': 'p2p'}))\n",
    "app_df.append(application_model.sample(30000, conditions={'Application Type': 'video-streaming'}))\n",
    "app_df.append(application_model.sample(30000, conditions={'Application Type': 'voip'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traff_df.to_csv('../../../data/phase2/copulagan_traffic_dataset.csv')\n",
    "app_df.to_csv('../../../data/phase2/copulagan_application_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Last Execution: {datetime.datetime.now()}')\n",
    "assert False, 'Nothing after this point is included in the study'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
