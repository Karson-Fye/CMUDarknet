{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we load data from the [CIC-Darknet2020](https://www.unb.ca/cic/datasets/darknet2020.html) dataset to explore methods of converting tabular data into image data using the Identity-oriented Tabular Image Transformation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Last Execution: 2022-05-13 17:23:59.279007\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tfastai:\t\t2.4.1\n",
      "    \tgiotto-tda:\t0.5.1\n",
      "    \tmatplotlib:\t3.5.1\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.3.4\n",
      "    \tplotly:\t\t5.6.0\n",
      "    \tseaborn:\t0.11.2\n",
      "    \tsdv:\t\t0.13.1\n",
      "    \tsklearn:\t1.0.2\n",
      "    \ttorch:\t\t1.9.0\n",
      "    \tyellowbrick:\t1.3.post1\n",
      "\n",
      "    \tdevice:\t\tcpu\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import colorsys, contextlib, datetime, os, pathlib, platform, pprint, sys\n",
    "import gtda\n",
    "import ipywidgets as widgets\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import PIL.Image as Image\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import pomegranate as pg\n",
    "import seaborn as sns\n",
    "import sdv\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yellowbrick as yb\n",
    "\n",
    "fastai_gpu_conflict = True\n",
    "if(fastai_gpu_conflict):\n",
    "    torch.cuda.is_available = lambda : False\n",
    "\n",
    "\n",
    "from collections import namedtuple, ChainMap\n",
    "\n",
    "import fastai\n",
    "from fastai.basics import Tensor\n",
    "from fastai.callback.all import CollectDataCallback, Callback, ProgressCallback\n",
    "from fastai.callback.all import LRFinder, EarlyStoppingCallback, SaveModelCallback, ShowGraphCallback, CSVLogger, slide, steep, minimum, valley\n",
    "from fastai.data.all import Transform, DisplayedTransform\n",
    "from fastai.layers import Cat, Identity\n",
    "from fastai.metrics import MatthewsCorrCoef, F1Score, Recall, Precision, RocAuc, BalancedAccuracy\n",
    "from fastai.optimizer import ranger, Adam\n",
    "from fastai.tabular.data import TabularDataLoaders, TabularPandas, MultiCategoryBlock\n",
    "from fastai.tabular.all import delegates, tabular_config, TabularLearner, get_c, ifnone, is_listy, LinBnDrop, SigmoidRange \n",
    "from fastai.tabular.all import FillMissing, Categorify, Normalize, tabular_learner, accuracy, ClassificationInterpretation, range_of\n",
    "from fastai.tabular.all import get_emb_sz, Module, Learner, Embedding, CrossEntropyLossFlat, IndexSplitter, ColSplitter, RandomSplitter\n",
    "from fastai.test_utils import synth_learner, VerboseCallback\n",
    "from fastai.torch_core import Enum\n",
    "\n",
    "from fastcore.all import Transform, store_attr, L\n",
    "\n",
    "from pytorch_tabnet.tab_network import TabNetNoEmbeddings\n",
    "from fast_tabnet.core import TabNetModel as TabNet, tabnet_feature_importances, tabnet_explain\n",
    "\n",
    "from gtda.curves import Derivative, StandardFeatures\n",
    "from gtda.diagrams import PersistenceEntropy, Amplitude, NumberOfPoints, ComplexPolynomial, PersistenceLandscape, PersistenceImage, BettiCurve, Silhouette, HeatKernel\n",
    "from gtda.homology import VietorisRipsPersistence, EuclideanCechPersistence\n",
    "from gtda.plotting import plot_diagram, plot_point_cloud, plot_betti_curves, plot_betti_surfaces\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sdv.metrics.tabular import MulticlassDecisionTreeClassifier, CSTest, KSTest, ContinuousKLDivergence, BNLikelihood, GMLogLikelihood, LogisticDetection, MulticlassMLPClassifier, BNLogLikelihood\n",
    "from sdv.tabular import TVAE, CopulaGAN, CTGAN, GaussianCopula\n",
    "from sdv.evaluation import evaluate\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from yellowbrick.model_selection import CVScores, LearningCurve, ValidationCurve\n",
    "\n",
    "seed: int = 14\n",
    "\n",
    "# allow plots to be shown in the notebook\n",
    "init_notebook_mode(connected=True)\n",
    "# pio.renderers.default = \"iframe\"\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# set up pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# set up pretty printer for easier data evaluation\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\n",
    "\n",
    "\n",
    "# print library and python versions for reproducibility\n",
    "print(\n",
    "    f'''\n",
    "    Last Execution: {datetime.datetime.now()}\n",
    "    python:\\t{platform.python_version()}\n",
    "\n",
    "    \\tfastai:\\t\\t{fastai.__version__}\n",
    "    \\tgiotto-tda:\\t{gtda.__version__}\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\n",
    "    \\tnumpy:\\t\\t{np.__version__}\n",
    "    \\tpandas:\\t\\t{pd.__version__}\n",
    "    \\tplotly:\\t\\t{py.__version__}\n",
    "    \\tseaborn:\\t{sns.__version__}\n",
    "    \\tsdv:\\t\\t{sdv.__version__}\n",
    "    \\tsklearn:\\t{sklearn.__version__}\n",
    "    \\ttorch:\\t\\t{torch.__version__}\n",
    "    \\tyellowbrick:\\t{yb.__version__}\n",
    "\n",
    "    \\tdevice:\\t\\t{device}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create some helper functions to load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(directory: str) -> callable:\n",
    "    '''\n",
    "        Closure that will return a function. \n",
    "        Function will return the filepath to the directory given to the closure\n",
    "    '''\n",
    "\n",
    "    def func(file: str) -> str:\n",
    "        return os.path.join(directory, file)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filePath: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\n",
    "        Function will only work when filepath is a .csv file\n",
    "    '''\n",
    "\n",
    "    p = pathlib.Path(filePath)\n",
    "\n",
    "    filePathClean: str = str(p.parts[-1])\n",
    "    pickleDump   : str = f'./cache/{filePathClean}.pickle'\n",
    "\n",
    "    print(f'Loading Dataset: {filePath}')\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\n",
    "\n",
    "\n",
    "    # check if data already exists within cache\n",
    "    if os.path.exists(pickleDump):\n",
    "        df = pd.read_pickle(pickleDump)\n",
    " \n",
    "    # if not, load data and clean it before caching it\n",
    "    else:\n",
    "        df: pd.DataFrame = pd.read_csv(filePath, low_memory=True)\n",
    "        df.to_pickle(pickleDump)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def features_with_bad_values(df: pd.DataFrame, datasetName: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will scan the dataframe for features with Inf, NaN, or Zero values.\n",
    "        Returns a new dataframe describing the distribution of these values in the original dataframe\n",
    "    '''\n",
    "\n",
    "    # Inf and NaN values can take different forms so we screen for every one of them\n",
    "    invalid_values: list = [ np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan', 0 ]\n",
    "    infs          : list = [ np.inf, 'Infinity', 'inf' ]\n",
    "    NaNs          : list = [ np.nan, 'NaN', 'nan' ]\n",
    "\n",
    "    # We will collect stats on the dataset, specifically how many instances of Infs, NaNs, and 0s are present.\n",
    "    # using a dictionary that will be converted into a (3, n) dataframe where n is the number of features in the dataset\n",
    "    stats: dict = {\n",
    "        'Dataset':[ datasetName, datasetName, datasetName ],\n",
    "        'Value'  :['Inf', 'NaN', 'Zero']\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "    for col in df.columns:\n",
    "        \n",
    "        i += 1\n",
    "        feature = np.zeros(3)\n",
    "        \n",
    "        for value in invalid_values:\n",
    "            if value in infs:\n",
    "                j = 0\n",
    "            elif value in NaNs:\n",
    "                j = 1\n",
    "            else:\n",
    "                j = 2\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                feature[j] += len(indexNames)\n",
    "                \n",
    "        stats[col] = feature\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will take a dataframe and remove the columns that match a value in prune \n",
    "        Inf and Nan values will also be removed once appropriate rows and columns \n",
    "        have been removed, \n",
    "        we will return the dataframe with the appropriate values\n",
    "    '''\n",
    "\n",
    "    # remove the features in the prune list    \n",
    "    for col in prune:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    \n",
    "    # drop missing values/NaN etc.\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Search through dataframe for any Infinite or NaN values in various forms that were not picked up previously\n",
    "    invalid_values: list = [\n",
    "        np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan'\n",
    "    ]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for value in invalid_values:\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                print(f'deleting {len(indexNames)} rows with Infinity in column {col}')\n",
    "                df.drop(indexNames, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def examine_dataset(job_id: int, files: str or None = None, datasets: str or None = None) -> ChainMap:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe of the job_id passed in as well as that dataframe's\n",
    "        feature stats, data composition, and file name.\n",
    "\n",
    "        This dictionary is expected as the input for all of the other helper functions\n",
    "    '''\n",
    "    if files is None and 'file_set' in tuple(globals()):\n",
    "        files = file_set\n",
    "\n",
    "    if datasets is None and 'data_set' in tuple(globals()):\n",
    "        datasets = data_set\n",
    "\n",
    "\n",
    "    job_id = job_id - 1  # adjusts for indexing while enumerating jobs from 1\n",
    "    print(f'Dataset {job_id+1}/{len(datasets)}: We now look at {files[job_id]}\\n\\n')\n",
    "\n",
    "    # Load the dataset\n",
    "    df: pd.DataFrame = load_data(files[job_id])\n",
    " \n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        File:\\t\\t\\t\\t{files[job_id]}  \n",
    "        Job Number:\\t\\t\\t{job_id+1}\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats in a chainmap. This is a dictionary that can be\n",
    "    #     updated by other functions, grouping together dictionaries and only showing the keys\n",
    "    #     that appear first in the chainmap. This is useful for transforming the dataset and \n",
    "    #     performing experiments, so we can keep track of the history and do not need to reload\n",
    "    #     the dataset.\n",
    "    data_summary: ChainMap =  ChainMap({\n",
    "        'File':             files[job_id],\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, files[job_id]), \n",
    "    })\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection(df: pd.DataFrame) -> ChainMap:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "    Packaging Data:\n",
    "\n",
    "    Dataset statistics:\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: ChainMap =  ChainMap({\n",
    "        'File':             '',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, ''), \n",
    "    })\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection_with_label(df: pd.DataFrame, label: str) -> ChainMap:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "    Packaging Data {label}:\n",
    "\n",
    "    Dataset statistics:\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: ChainMap =  ChainMap({\n",
    "        'File':             f'{label}',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, f'{label}'),\n",
    "    })\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def check_infs(data_summary: ChainMap) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of Inf.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    return inf_df[inf_df[0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_nans(data_summary: ChainMap) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of NaN.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    return nan_df[nan_df[1] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros(data_summary: ChainMap) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "\n",
    "    return zero_df[zero_df[2] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold(data_summary: ChainMap, threshold: int) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold_percentage(data_summary: ChainMap, threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with all features with\n",
    "        a frequency of 0 values greater than the threshold\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    size: int = data_summary['Dataset'].shape[0]\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold*size]\n",
    "\n",
    "\n",
    "\n",
    "def remove_infs_and_nans(data_summary: ChainMap) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all inf and nan values removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, [])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rename_columns(data_summary: ChainMap, columns: list, new_names: list) -> ChainMap:\n",
    "    '''\n",
    "        Function will return the data_summary dict with the names of the columns in the dataframe changed\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    for x, i in enumerate(columns):\n",
    "        df.rename(columns={i: new_names[x]}, inplace=True)\n",
    "\n",
    "    out: dict = {'Dataset': df}\n",
    "\n",
    "    return data_summary.new_child(out)\n",
    "\n",
    "\n",
    "\n",
    "def rename_values_in_column(data_summary: ChainMap, replace: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe with the names of the columns changed\n",
    "\n",
    "        replace: [('column', {'old_name': 'new_name', ...}), ...]\n",
    "    '''\n",
    "    length: int = len(replace)\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    for i in range(length):\n",
    "        df[replace[i][0]].replace(replace[i][1], inplace=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rename_values_in_column_df(df: pd.DataFrame, replace: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe with the names of the columns changed\n",
    "\n",
    "        replace: [('column', {'old_name': 'new_name', ...}), ...]\n",
    "    '''\n",
    "    length: int = len(replace)\n",
    "\n",
    "    df1: pd.DataFrame = df.copy()\n",
    "    for i in range(length):\n",
    "        df1[replace[i][0]].replace(replace[i][1], inplace=True)\n",
    "\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "\n",
    "def prune_dataset(data_summary: ChainMap, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all the columns in the prune list removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, prune)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prune_feature_by_values(df: pd.DataFrame, column: str, value: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function takes a dataframe, a column name, and a list of values and returns a dataframe\n",
    "        with all rows that do not have the values in the column removed\n",
    "\n",
    "        Deprecated, use reduce_feature_to_values:\n",
    "            ambiguous name implied it is removing values, not keeping them\n",
    "    '''\n",
    "    new_df = pd.DataFrame()\n",
    "    for v in value:\n",
    "        new_df = new_df.append(df[df[column] == v].copy())\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def reduce_feature_to_values(df: pd.DataFrame, column: str, value: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function takes a dataframe, a column name, and a list of values and returns a dataframe\n",
    "        with all rows that do not have the values in the column removed\n",
    "    '''\n",
    "    new_df = pd.DataFrame()\n",
    "    for v in value:\n",
    "        new_df = new_df.append(df[df[column] == v].copy())\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def test_infs(data_summary: ChainMap) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no inf values.\n",
    "    '''\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    assert inf_df[inf_df[0] != 0].shape[0] == 2, 'Dataset has inf values'\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_nans(data_summary: ChainMap) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no NaN values\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    assert nan_df[nan_df[1] != 0].shape[0] == 2, 'Dataset has NaN values'\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned(data_summary: ChainMap, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    pruned: bool = True\n",
    "\n",
    "    for col in prune:\n",
    "        if col in data_summary['Dataset'].columns:\n",
    "            pruned = False\n",
    "\n",
    "    assert pruned, 'Dataset has columns present in prune list'\n",
    "\n",
    "    return pruned\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned_size(data_summary_original: ChainMap, data_summary_pruned: ChainMap, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    original_size: int = data_summary_original['Dataset'].shape[1]\n",
    "    pruned_size: int = data_summary_pruned['Dataset'].shape[1]\n",
    "    prune_list_size: int = len(prune)\n",
    "\n",
    "    assert original_size - prune_list_size == pruned_size, 'Dataset has columns present in prune list'\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify what datasets we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to scale to processing numerous datasets, even though we currently are only looking at one now\n",
    "data_path_1: str = './phase2/'   \n",
    "data_set_1: list = [\n",
    "    'Darknet_experiments_base.csv',\n",
    "    'copulagan_application_dataset_50k.csv',\n",
    "    'copulagan_traffic_dataset_100k.csv',\n",
    "    'ctgan_application_dataset_50k.csv',\n",
    "    'ctgan_traffic_dataset_100k.csv',\n",
    "    'vae_application_dataset.csv',\n",
    "    'vae_traffic_dataset.csv',\n",
    "    'smote_fake_application.csv',\n",
    "    'smote_fake_traffic.csv',\n",
    "]\n",
    "\n",
    "data_set: list   = data_set_1\n",
    "file_path_1      = get_file_path(data_path_1)\n",
    "file_set: list   = list(map(file_path_1, data_set_1))\n",
    "current_job: int = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes and Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn classes\n",
    "\n",
    "class SklearnWrapper(BaseEstimator):\n",
    "    '''\n",
    "        A wrapper for fastai learners for creating visualizations using yellowbrick\n",
    "        code sourced from: \n",
    "        forums.fast.ai/t/fastai-with-yellowbrics-how-to-get-roc-curves-more/79408\n",
    "    '''\n",
    "    _estimator_type = \"classifier\"\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.classes_ = list(self.model.dls.y.unique())\n",
    "        self._calls = [] \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._calls.append(('fit', X, y))\n",
    "        pass\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        self._calls.append(('score', X, y))\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "    \n",
    "    def get_new_preds(self, X):\n",
    "        self._calls.append(('get_new_preds', X))\n",
    "        new_to = self.model.dls.valid_ds.new(X)\n",
    "        new_to.conts = new_to.conts.astype(np.float32)\n",
    "        new_dl = self.model.dls.valid.new(new_to)\n",
    "        with self.model.no_bar():\n",
    "            preds,_,dec_preds = self.model.get_preds(dl=new_dl, with_decoded=True)\n",
    "        return (preds, dec_preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        self._calls.append(('predict_proba', X))\n",
    "        return self.get_new_preds(X)[0].numpy()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self._calls.append(('predict', X))\n",
    "        return self.get_new_preds(X)[1].numpy()\n",
    "\n",
    "\n",
    "# fastai classes\n",
    "\n",
    "# transforms for the dataloaders pipeline\n",
    "\n",
    "class PCA_tabular(Transform):\n",
    "    '''\n",
    "        Class will implement a PCA feature extraction method for tabular data\n",
    "        On setup, we train a pca on the training data, then extract n_comps from the entire dataset \n",
    "            the components are then added to the dataframe as new columns\n",
    "    '''\n",
    "    def __init__(self, n_comps=3, add_col=True):\n",
    "        store_attr() # core function in the fastai library that stores all passed in parameters\n",
    "\n",
    "    def setups(self, to, **kwargs):\n",
    "\n",
    "        self.pca = PCA(n_components=self.n_comps)\n",
    "        self.pca.fit(to.train.conts)\n",
    "        pca = pd.DataFrame(self.pca.transform(to.conts))\n",
    "        pca.columns = [f'pca_{i+1}' for i in range(self.n_comps)]\n",
    "\n",
    "        for col in pca.columns:\n",
    "            to.items[col] = pca[col].values.astype('float32')\n",
    "        \n",
    "\n",
    "        if self.add_col:\n",
    "            for i in range(self.n_comps):\n",
    "                if f'pca_{i+1}' not in to.cont_names: to.cont_names.append(f'pca_{i+1}')\n",
    "        \n",
    "        return self(to)\n",
    "\n",
    "\n",
    "\n",
    "class Normal(DisplayedTransform):\n",
    "    '''\n",
    "        A data processing tool inherited from the fastai library. \n",
    "        This is a modified version of the normalize function that performs MinMax scaling and is able to be\n",
    "            used in our preprocessing pipeline. \n",
    "        The original normalizes the data to have a mean centered at 0 and a standard deviation of 1.\n",
    "    '''\n",
    "    def setups(self, to, **kwargs):\n",
    "        self.mins = getattr(to, 'train', to).conts.min()\n",
    "        self.maxs = getattr(to, 'train', to).conts.max()\n",
    "        \n",
    "        return self(to)\n",
    "        \n",
    "    def encodes(self, to, **kwargs):\n",
    "        to.conts = (to.conts-self.mins) / (self.maxs - self.mins)\n",
    "        return to\n",
    "\n",
    "    def decodes(self, to, **kwargs):\n",
    "        to.conts = (to.conts) * (to.maxs - to.mins) + to.mins\n",
    "        return to\n",
    "\n",
    "\n",
    "# callbacks for the fastai training schedule\n",
    "\n",
    "class DFLogger(Callback):\n",
    "    '''\n",
    "        Class defines a callback that is passed to the fastai learner that\n",
    "            will save the recorded metrics for each epoch to a dataframe. \n",
    "            Also saves results from predictions with negative epochs\n",
    "    '''\n",
    "    order=60\n",
    "    def __init__(self, fname='history.csv', append=False):\n",
    "        self.fname,self.append = pathlib.Path(fname),append\n",
    "        self.df = pd.DataFrame()\n",
    "        self.np = np.array([])\n",
    "        self.flag = True\n",
    "        self.pred_count = 0\n",
    "\n",
    "    def to_csv(self, file: str or None = None):\n",
    "        if file is None:\n",
    "            self.df.to_csv(self.path/self.fname, index=False)\n",
    "        else:\n",
    "            self.df.to_csv(file, index=False)\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Prepare file with metric names.\"\n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.old_logger,self.learn.logger = self.logger,self.add_row\n",
    "\n",
    "    def add_row(self, log):\n",
    "        \"Write a line with `log` and call the old logger.\"\n",
    "        if(self.flag):\n",
    "            if not hasattr(self, \"gather_preds\"):\n",
    "                self.df = pd.DataFrame([log], columns=self.recorder.metric_names)\n",
    "            else:\n",
    "                self.pred_count += 1\n",
    "                print('predicting')\n",
    "                self.df = pd.DataFrame([[-self.pred_count] + log], columns=self.recorder.metric_names)\n",
    "            self.np = np.append(self.np, [log])\n",
    "            self.flag = False\n",
    "        else:\n",
    "            if (len(log) == len(self.df.columns)):\n",
    "                self.new_row = pd.DataFrame([log], columns=self.recorder.metric_names)\n",
    "                self.df = pd.concat([self.df, self.new_row], ignore_index=True)\n",
    "                self.np = np.append(self.np, log, axis=0)\n",
    "            elif (len(log) < len(self.df.columns)):\n",
    "                self.pred_count += 1\n",
    "                self.new_row = pd.DataFrame([[-self.pred_count] + log], columns=self.recorder.metric_names)\n",
    "                self.df = pd.concat([self.df, self.new_row], ignore_index=True)\n",
    "                self.np = np.append(self.np, [-self.pred_count] + log, axis=0)\n",
    "\n",
    "    def after_fit(self):\n",
    "        \"Close the file and clean up.\"\n",
    "        self.learn.logger = self.old_logger\n",
    "\n",
    "\n",
    "\n",
    "class LazyGraphCallback(Callback):\n",
    "    '''\n",
    "        Class defines a callback that is passed to the fastai learner that\n",
    "            saves the validation and training loss metrics to graph when\n",
    "            calling the .plot_graph() method\n",
    "        \n",
    "        This allows us to display the train/validation loss graph after the    \n",
    "            experiment is run, even if it is run in no_bar mode\n",
    "    '''\n",
    "    order: int = 65\n",
    "    run_valid: bool = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph_params: list = []\n",
    "        self.graphs: list = []\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not self.run: return\n",
    "        self.nb_batches: list = []\n",
    "\n",
    "    def after_train(self): self.nb_batches.append(self.train_iter)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "        rec = self.learn.recorder\n",
    "        iters = range_of(rec.losses)\n",
    "        val_losses = [v[1] for v in rec.values]\n",
    "        x_bounds = (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses))\n",
    "        y_bounds = (0, max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n",
    "        self.graph_params.append(([(iters, rec.losses), (self.nb_batches, val_losses)], x_bounds, y_bounds))\n",
    "\n",
    "\n",
    "    def plot_graph(self, ax: plt.Axes or None = None, title: str = 'Loss'):\n",
    "\n",
    "        params = self.graph_params[-1]\n",
    "        \n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Iterations')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.plot(params[0][0][0], params[0][0][1], label='Training')\n",
    "        ax.plot(params[0][1][0], params[0][1][1], label='Validation')\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "class ModelStatsCallback(Callback):\n",
    "    '''\n",
    "        Collect all batches, along with `pred` and `loss`, into `self.datum`.\n",
    "        This allows the data to be used after the model is used.\n",
    "    '''\n",
    "    order=60 # mysterious param we need to investigate\n",
    "    def __init__(self):\n",
    "        self.datum: list = []\n",
    "        self.records: list = []\n",
    "        self.c_idx: int  = -1\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.datum.append(L())\n",
    "        self.records.append({'loss': [], 'predictions': []})\n",
    "        self.c_idx += 1\n",
    "\n",
    "\n",
    "    def after_batch(self):\n",
    "        vals = self.learn.to_detach((self.xb,self.yb,self.pred,self.loss))\n",
    "        self.datum[self.c_idx].append(vals)\n",
    "        self.records[self.c_idx]['predictions'].append(vals[2])\n",
    "        self.records[self.c_idx]['loss'].append(vals[3])\n",
    "        self.records[self.c_idx]['iters'] = range_of(self.recorder.losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytorch --> fastai models\n",
    "\n",
    "class ResidualBlock(Module):\n",
    "    '''\n",
    "        A simple residule block that creates a skip connection around any input module\n",
    "        Output size of the input module must match the module's input size\n",
    "    '''\n",
    "    def __init__(self, module, layer):\n",
    "        self.module = module\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fx = self.module(inputs)\n",
    "        if(inputs.shape != fx.shape):\n",
    "            print('mismatch at layer:', self.layer ,inputs.shape, fx.shape)\n",
    "            assert False\n",
    "        return fx + inputs\n",
    "\n",
    "\n",
    "class CardinalResidualBlock(Module):\n",
    "    '''\n",
    "        A residule block that creates a skip connection around a set of n branches\n",
    "            where the number of branches is determined by the number of input modules\n",
    "            in the branches list parameter.\n",
    "\n",
    "        The output of the branches is summed together along with the input\n",
    "        Output size of the input module must match the module's input size\n",
    "    '''\n",
    "    def __init__(self, branches: list, layer: int):\n",
    "        self.branches = branches\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fx = self.branches[0](inputs)\n",
    "        if(inputs.shape != fx.shape):\n",
    "            print('mismatch at layer:', self.layer ,inputs.shape, fx.shape)\n",
    "            assert False\n",
    "        if(len(self.branches) > 1):\n",
    "            for i in range(len(self.branches) - 1):\n",
    "                fx += self.branches[i + 1](inputs)\n",
    "\n",
    "        return fx + inputs\n",
    "\n",
    "# currently need to create a bottlenecking block that can be used to reduce the number of inputs\n",
    "#   being passed by the residual connection \n",
    "\n",
    "class BottleneckResidualBlock(Module):\n",
    "    '''\n",
    "        A residule block that creates a skip connection around a set of n branches\n",
    "            where the number of branches is determined by the number of input modules\n",
    "            in the branches list parameter.\n",
    "\n",
    "            the residual connection is put through a linear batchnormed layer if the\n",
    "            input size is different from the output size\n",
    "            Then, the output of the branches is summed together along with the possibly transformed input\n",
    "    '''\n",
    "    def __init__(self, branches: list, layer: int, in_size: int, out_size: int):\n",
    "        self.branches = branches\n",
    "        self.layer = layer\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        # self.linear = nn.Linear(in_size, out_size)\n",
    "        self.linear = LinBnDrop(in_size, out_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        fx = self.branches[0](inputs)\n",
    "        for i in range(len(self.branches) - 1):\n",
    "            fx += self.branches[i + 1](inputs)\n",
    "\n",
    "        if(inputs.shape != fx.shape):\n",
    "            inputs = self.linear(inputs)\n",
    "        return fx + inputs\n",
    "\n",
    "\n",
    "class ResidualTabularModel(Module):\n",
    "    \"Residual model for tabular data.\"\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0.,\n",
    "                 y_range=None, use_bn=True, bn_final=False, bn_cont=True, act_cls=nn.ReLU(inplace=True),\n",
    "                 lin_first=True, cardinality: list or None = None):\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        if not is_listy(ps): ps = [ps]*len(layers)\n",
    "        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(embed_p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_emb,self.n_cont = n_emb,n_cont\n",
    "        sizes = [n_emb + n_cont] + layers + [out_sz]\n",
    "\n",
    "        # print(f'sizes', sizes)\n",
    "        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n",
    "        \n",
    "        _layers: list = []\n",
    "        num_residuals = 0\n",
    "        residual_locations = []\n",
    "        enum_length = len(list(enumerate(zip(ps+[0.],actns))))\n",
    "        for i, (p, a) in enumerate(zip(ps+[0.],actns)):\n",
    "            if(i==0 or i == enum_length-1):\n",
    "                _layers.append(LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first))\n",
    "            else:\n",
    "                if(cardinality == None):\n",
    "                    modules = [ LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first), ]\n",
    "                else:\n",
    "                    modules = [ LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first) for _ in range(cardinality[i]) ]\n",
    "                num_residuals += 1 \n",
    "                residual_locations.append(i)\n",
    "                _layers.append( BottleneckResidualBlock(modules, i, sizes[i], sizes[i+1]) )\n",
    "\n",
    "        print(f'Layer sizes: {sizes}, length: {len(sizes)}')\n",
    "        print(f'Number of residual blocks: {num_residuals}')\n",
    "        print('Residual locations: ', residual_locations)\n",
    "\n",
    "        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n",
    "        self.layers = nn.Sequential(*_layers)\n",
    "\n",
    "    def forward(self, x_cat, x_cont=None):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "\n",
    "@delegates(Learner.__init__)\n",
    "def residual_tabular_learner(dls, layers=None, emb_szs=None, config=None, n_out=None, y_range=None, cardinality=None, ps=None, **kwargs):\n",
    "    \"Get a `Learner` using `dls`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
    "    if config is None: config = tabular_config()\n",
    "    if layers is None: layers = [200,100]\n",
    "    to = dls.train_ds\n",
    "    emb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n",
    "    model = ResidualTabularModel(emb_szs, len(dls.cont_names), n_out, layers, y_range=y_range, cardinality=cardinality, ps=ps, **config)\n",
    "    return TabularLearner(dls, model, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to run classification experiments or to transform and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_data = namedtuple(\n",
    "    'model_data', \n",
    "    ['name', 'model', 'classes', 'X_train', 'y_train', 'X_test', 'y_test', 'to', 'dls', 'model_type']\n",
    ")\n",
    "\n",
    "# todo: standardize new format of experiment runners\n",
    "\n",
    "def run_deep_nn_experiment(\n",
    "    df: pd.DataFrame, \n",
    "    file_name: str, \n",
    "    target_label: str, \n",
    "    shape: tuple, \n",
    "    split=0.2, \n",
    "    categorical: list = ['Protocol'],\n",
    "    procs = [FillMissing, Categorify, Normalize], \n",
    "    leave_out: list = [],\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    metrics: list or None = None,\n",
    "    callbacks: list = [ShowGraphCallback],\n",
    "    lr_choice: str = 'valley',\n",
    "    name: str or None = None,\n",
    "    fit_choice: str = 'one_cycle',\n",
    "    no_bar: bool = False,\n",
    ") -> Model_data:\n",
    "    '''\n",
    "        Function trains a deep neural network model on the given data. \n",
    "\n",
    "        Parameters:\n",
    "            df: pandas dataframe containing the data\n",
    "            file_name: name of the file the dataset came from\n",
    "            target_label: the label to predict\n",
    "            shape: the shape of the neural network, the i-th value in the tuple represents the number of nodes in the i+1 layer\n",
    "                    and the number of entries in the tuple represent the number of layers\n",
    "            name: name of the experiment, if none a default is given\n",
    "            split: the percentage of the data to use for testing\n",
    "            categorical: list of the categorical columns\n",
    "            procs: list of preprocessing functions to apply in the dataloaders pipeline\n",
    "                    additional options are: \n",
    "                        PCA_tabular (generate n principal components) \n",
    "                        Normal (features are scaled to the interval [0,1])\n",
    "            leave_out: list of columns to leave out of the experiment\n",
    "            epochs: number of epochs to train for\n",
    "            batch_size: number of samples processed in one forward and backward pass of the model\n",
    "            metrics: list of metrics to calculate and display during training\n",
    "            callbacks: list of callbacks to apply during training\n",
    "            lr_choice: where the learning rate sampling function should find the optimal learning rate\n",
    "                        choices are: 'valley', 'steep', 'slide', and 'minimum'\n",
    "            fit_choice: choice of function that controls the learning schedule choices are:\n",
    "                                'fit': a standard learning schedule \n",
    "                                'flat_cos': a learning schedule that starts high before annealing to a low value\n",
    "                                'one_cyle': a learning schedule that warms up for the first epochs, continues at a high\n",
    "                                                learning rate, and then cools down for the last epochs\n",
    "                                the default is 'one_cycle'\n",
    "         \n",
    "        \n",
    "        returns a model data named tuple\n",
    "            model_data: tuple = (file_name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "    '''\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if name is None:\n",
    "        width: int = shape[0]\n",
    "        for x in shape:\n",
    "            width = x if (x > width) else width\n",
    "        name = f'Deep_NN_{len(shape)}x{width}'\n",
    "\n",
    "    lr_choice = {'valley': 0, 'slide': 1, 'steep': 2, 'minimum': 3}[lr_choice]\n",
    "\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "        \n",
    "    if metrics is None:\n",
    "        metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "\n",
    "\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([target_label]) - set(untouched_features))\n",
    "\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "\n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=target_label                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "    # The dataframe is then converted into a fastai dataset\n",
    "    dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "    # extract the file_name from the path\n",
    "    p = pathlib.Path(file_name)\n",
    "    file_name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    learner = tabular_learner(\n",
    "        dls, \n",
    "        layers=list(shape), \n",
    "        metrics = metrics,\n",
    "        cbs=callbacks,\n",
    "    )\n",
    "\n",
    "    with learner.no_bar() if no_bar else contextlib.ExitStack() as gs:\n",
    "\n",
    "        lr = learner.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "            # fitting functions, they give different results, some networks perform better with different learning schedule during fitting\n",
    "        if(fit_choice == 'fit'):\n",
    "            learner.fit(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'flat_cos'):\n",
    "            learner.fit_flat_cos(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'one_cycle'):\n",
    "            learner.fit_one_cycle(epochs, lr_max=lr[lr_choice])\n",
    "        else:\n",
    "            assert False, f'{fit_choice} is not a valid fit_choice'\n",
    "\n",
    "        learner.recorder.plot_sched() \n",
    "        results = learner.validate()\n",
    "        interp = ClassificationInterpretation.from_learner(learner)\n",
    "        interp.plot_confusion_matrix()\n",
    "                \n",
    "\n",
    "    print(f'loss: {results[0]}, accuracy: {results[1]*100: .2f}%')\n",
    "    learner.save(f'{file_name}.model')\n",
    "\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "    wrapped_model = SklearnWrapper(learner)\n",
    "\n",
    "    classes = list(learner.dls.vocab)\n",
    "    if len(classes) == 2:\n",
    "        wrapped_model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        wrapped_model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "    \n",
    "    wrapped_model._target_labels = target_label\n",
    "    \n",
    "    model_data: Model_data = Model_data(file_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, name)\n",
    "\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_residual_deep_nn_experiment(\n",
    "    df: pd.DataFrame, \n",
    "    file_name: str, \n",
    "    target_label: str, \n",
    "    shape: tuple, \n",
    "    split=0.2, \n",
    "    categorical: list = ['Protocol'],\n",
    "    procs = [FillMissing, Categorify, Normalize], \n",
    "    leave_out: list = [],\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    metrics: list or None = None,\n",
    "    callbacks: list = [ShowGraphCallback],\n",
    "    lr_choice: str = 'valley',\n",
    "    name: str or None = None,\n",
    "    fit_choice: str = 'one_cycle',\n",
    "    cardinality: list or None = None,\n",
    "    no_bar: bool = False,\n",
    "    ps: list or None = None,\n",
    ") -> Model_data:\n",
    "    '''\n",
    "        Function trains a residual deep neural network model on the given data. \n",
    "            Based on ResNet from Deep Residual Learning for Image Recognition by He et al. (2016) \n",
    "            as well as the ResNext network proposed by Xie et al. (2017) but adapted to tabular data  \n",
    "        \n",
    "        (https://arxiv.org/abs/1512.03385)\n",
    "        (https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "        Parameters:\n",
    "            df: pandas dataframe containing the data\n",
    "            file_name: name of the file the dataset came from\n",
    "            target_label: the label to predict\n",
    "            shape: the shape of the neural network, the i-th value in the tuple represents the number of nodes in the i+1 layer\n",
    "                    and the number of entries in the tuple represent the number of layers\n",
    "            name: name of the experiment, if none a default is given\n",
    "            split: the percentage of the data to use for testing\n",
    "            categorical: list of the categorical columns\n",
    "            procs: list of preprocessing functions to apply in the dataloaders pipeline\n",
    "                    additional options are: \n",
    "                        PCA_tabular (generate n principal components) \n",
    "                        Normal (features are scaled to the interval [0,1])\n",
    "            leave_out: list of columns to leave out of the experiment\n",
    "            epochs: number of epochs to train for\n",
    "            batch_size: number of samples processed in one forward and backward pass of the model\n",
    "            metrics: list of metrics to calculate and display during training\n",
    "            callbacks: list of callbacks to apply during training\n",
    "            lr_choice: where the learning rate sampling function should find the optimal learning rate\n",
    "                        choices are: 'valley', 'steep', 'slide', and 'minimum'\n",
    "            fit_choice: choice of function that controls the learning schedule choices are:\n",
    "                    'fit': a standard learning schedule \n",
    "                    'flat_cos': a learning schedule that starts high before annealing to a low value\n",
    "                    'one_cyle': a learning schedule that warms up for the first epochs, continues at a high\n",
    "                                    learning rate, and then cools down for the last epochs\n",
    "                    the default is 'one_cycle'\n",
    "            cardinality: list of integers that represent the number of residual blocks in each layer, if none\n",
    "                            the default is one block per layer\n",
    "\n",
    "        \n",
    "        returns a model data named tuple\n",
    "            model_data: tuple = (file_name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "    '''\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if name is None:\n",
    "        width: int = shape[0]\n",
    "        for x in shape:\n",
    "            width = x if (x > width) else width\n",
    "        name = f'Residual_1D_Deep_NN_{len(shape)}x{width}'\n",
    "\n",
    "    lr_choice = {'valley': 0, 'slide': 1, 'steep': 2, 'minimum': 3}[lr_choice]\n",
    "\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "        \n",
    "    if metrics is None:\n",
    "        metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "\n",
    "\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([target_label]) - set(untouched_features))\n",
    "\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "\n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=target_label                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "    # The dataframe is then converted into a fastai dataset\n",
    "    dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "    # extract the file_name from the path\n",
    "    p = pathlib.Path(file_name)\n",
    "    file_name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    learner = residual_tabular_learner(\n",
    "        dls, \n",
    "        layers=list(shape), \n",
    "        metrics = metrics,\n",
    "        cbs=callbacks,\n",
    "        cardinality=cardinality\n",
    "    )\n",
    "\n",
    "\n",
    "    with learner.no_bar() if no_bar else contextlib.ExitStack() as gs:\n",
    "\n",
    "        lr = learner.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "            # fitting functions, they give different results, some networks perform better with different learning schedule during fitting\n",
    "        if(fit_choice == 'fit'):\n",
    "            learner.fit(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'flat_cos'):\n",
    "            learner.fit_flat_cos(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'one_cycle'):\n",
    "            learner.fit_one_cycle(epochs, lr_max=lr[lr_choice])\n",
    "        else:\n",
    "            assert False, f'{fit_choice} is not a valid fit_choice'\n",
    "\n",
    "        learner.recorder.plot_sched() \n",
    "        results = learner.validate()\n",
    "        interp = ClassificationInterpretation.from_learner(learner)\n",
    "        interp.plot_confusion_matrix()\n",
    "                \n",
    "\n",
    "    print(f'loss: {results[0]}, accuracy: {results[1]*100: .2f}%')\n",
    "    learner.save(f'{file_name}.model')\n",
    "\n",
    "\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "    wrapped_model = SklearnWrapper(learner)\n",
    "\n",
    "    classes = list(learner.dls.vocab)\n",
    "    if len(classes) == 2:\n",
    "        wrapped_model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        wrapped_model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "    \n",
    "    wrapped_model._target_labels = target_label\n",
    "    \n",
    "    model_data: Model_data = Model_data(file_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, name)\n",
    "\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "def run_tabnet_experiment(\n",
    "    df: pd.DataFrame, \n",
    "    file_name: str, \n",
    "    target_label: str, \n",
    "    split=0.2, \n",
    "    name: str or None = None,\n",
    "    categorical: list = ['Protocol'],\n",
    "    procs = [FillMissing, Categorify, Normalize], \n",
    "    leave_out: list = [],\n",
    "    epochs: int = 10,\n",
    "    steps: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    metrics: list or None = None,\n",
    "    attention_size: int = 16,\n",
    "    attention_width: int = 16,\n",
    "    callbacks: list = [ShowGraphCallback],\n",
    "    lr_choice: str = 'valley',\n",
    "    fit_choice: str = 'flat_cos',\n",
    ") -> Model_data:\n",
    "    '''\n",
    "    Function trains a TabNet model on the dataframe and returns a model data named tuple\n",
    "        Based on TabNet: Attentive Interpretable Tabular Learning by Sercan Arik and Tomas Pfister from Google Cloud AI (2016)\n",
    "            where a DNN selects features from the input features based on an attention layer. Each step of the model selects \n",
    "            different features and uses the input from the previous step to ultimately make predictions\n",
    "    \n",
    "        Combines aspects of a transformer, decision trees, and deep neural networks to learn tabular data, and has achieved state\n",
    "            of the art results on some datasets.\n",
    "\n",
    "        Capable of self-supervised learning, however it is not implemented here yet.\n",
    "\n",
    "    (https://arxiv.org/pdf/1908.07442.pdf)\n",
    "\n",
    "    Parameters:\n",
    "        df: pandas dataframe containing the data\n",
    "        file_name: name of the file the dataset came from\n",
    "        target_label: the label to predict\n",
    "        name: name of the experiment, if none a default is given\n",
    "        split: the percentage of the data to use for testing\n",
    "        categorical: list of the categorical columns\n",
    "        procs: list of preprocessing functions to apply in the dataloaders pipeline\n",
    "                additional options are: \n",
    "                    PCA_tabular (generate n principal components) \n",
    "                    Normal (features are scaled to the interval [0,1])\n",
    "        leave_out: list of columns to leave out of the experiment\n",
    "        epochs: number of epochs to train for  \n",
    "        batch_size: number of samples processed in one forward and backward pass of the model\n",
    "        metrics: list of metrics to calculate and display during training\n",
    "        attention size: determines the number of rows and columns in the attention layers\n",
    "        attention width: determines the width of the decision layer\n",
    "        callbacks: list of callbacks to apply during training\n",
    "        lr_choice: where the learning rate sampling function should find the optimal learning rate\n",
    "                    choices are: 'valley', 'steep', 'slide', and 'minimum'\n",
    "        fit_choice: choice of function that controls the learning schedule choices are:\n",
    "                    'fit': a standard learning schedule \n",
    "                    'flat_cos': a learning schedule that starts high before annealing to a low value\n",
    "                    'one_cyle': a learning schedule that warms up for the first epochs, continues at a high\n",
    "                                    learning rate, and then cools down for the last epochs\n",
    "                    the default is 'flat_cos'\n",
    "\n",
    "    \n",
    "    returns a model data named tuple\n",
    "        model_data: tuple = (file_name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "    '''\n",
    "\n",
    "    if name is None:\n",
    "        name = f\"TabNet_steps_{steps}_width_{attention_width}_attention_{attention_size}\"\n",
    "\n",
    "    lr_choice = {'valley': 0, 'slide': 1, 'steep': 2, 'minimum': 3}[lr_choice]\n",
    "\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "        \n",
    "    if metrics is None:\n",
    "        metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "\n",
    "\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([target_label]) - set(untouched_features))\n",
    "\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=target_label                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "    # The dataframe is then converted into a fastai dataset\n",
    "    dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "    # extract the file_name from the path\n",
    "    p = pathlib.Path(file_name)\n",
    "    file_name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    emb_szs = get_emb_sz(to)\n",
    "\n",
    "\n",
    "    net = TabNet(emb_szs, len(to.cont_names), dls.c, n_d=attention_width, n_a=attention_size, n_steps=steps) \n",
    "    tab_model = Learner(dls, net, loss_func=CrossEntropyLossFlat(), metrics=metrics, opt_func=ranger, cbs=callbacks)\n",
    "\n",
    "\n",
    "    with learner.no_bar() if no_bar else contextlib.ExitStack() as gs:\n",
    "\n",
    "        lr = tab_model.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "\n",
    "        # fitting functions, they give different results, some networks perform better with different learning schedule during fitting\n",
    "        if(fit_choice == 'fit'):\n",
    "            tab_model.fit(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'flat_cos'):\n",
    "            tab_model.fit_flat_cos(epochs, lr[lr_choice])\n",
    "        elif(fit_choice == 'one_cycle'):\n",
    "            tab_model.fit_one_cycle(epochs, lr_max=lr[lr_choice])\n",
    "        else:\n",
    "            assert False, f'{fit_choice} is not a valid fit_choice'\n",
    "\n",
    "\n",
    "        tab_model.recorder.plot_sched() \n",
    "        results = tab_model.validate()\n",
    "        interp = ClassificationInterpretation.from_learner(tab_model)\n",
    "        interp.plot_confusion_matrix()\n",
    "                \n",
    "\n",
    "    tab_model.save(f'{file_name}.model')\n",
    "    print(f'loss: {results[0]}, accuracy: {results[1]*100: .2f}%')\n",
    "\n",
    "\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "    wrapped_model = SklearnWrapper(tab_model)\n",
    "\n",
    "    classes = list(tab_model.dls.vocab)\n",
    "    if len(classes) == 2:\n",
    "        wrapped_model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        wrapped_model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "    \n",
    "    wrapped_model._target_labels = target_label\n",
    "    \n",
    "    model_data: Model_data = Model_data(file_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, name)\n",
    "\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Todo: make into general function that takes sklearn model instead of just a knn\n",
    "\n",
    "def run_knn_experiment(df: pd.DataFrame, name: str, target_label: str, split=0.2, categorical : list = ['Protocol'], leave_out: list = [], model = None) -> Model_data:\n",
    "    '''\n",
    "        Run binary classification using K-Nearest Neighbors\n",
    "        returns the 10-tuple Model_data\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var: str = target_label\n",
    "    print(df.shape)\n",
    "\n",
    " \n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]) - set(untouched_features))\n",
    "\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\n",
    "    dls = to.dataloaders(bs=64)\n",
    "    temp_model = tabular_learner(dls)\n",
    "    classes : list = list(temp_model.dls.vocab)\n",
    "\n",
    "\n",
    "    # extract the name from the path\n",
    "    p = pathlib.Path(name)\n",
    "    name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "    # Now that we have the train and test datasets, we set up a gridsearch of the K-NN classifier\n",
    "    # using SciKitLearn and print the results \n",
    "    # params = {\"n_neighbors\": range(1, 50)}\n",
    "    # model = GridSearchCV(KNeighborsClassifier(), params)\n",
    "    \n",
    "    if( model == None ):\n",
    "        model = KNeighborsClassifier()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    report = classification_report(y_test, prediction)\n",
    "    print(report)\n",
    "    print(f'\\tAccuracy: {accuracy_score(y_test, prediction)}\\n')\n",
    "\n",
    "    # print(\"Best Parameters found by gridsearch:\")\n",
    "    # print(model.best_params_)\n",
    "\n",
    "\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    if len(classes) == 2:\n",
    "        model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "\n",
    "    model_data: Model_data = Model_data(name, model, classes, X_train, y_train, X_test, y_test, to, dls, f'K_Nearest_Neighbors')\n",
    "\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # for analysis and further experimentation\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_and_split_data(df: pd.DataFrame, target_label: str, split=0, name='', categorical : list = ['Protocol'], scale:bool = False, leave_out: list = []) -> Model_data:\n",
    "    '''\n",
    "        Transform and split the data into a train and test set\n",
    "        returns the 10-tuple with the following indicies:\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var             : str  = target_label\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in df.columns:\n",
    "            untouched_features.append(x)\n",
    "\n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]) - set(untouched_features))\n",
    "\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    if(scale): \n",
    "        procs.append(Normal)\n",
    "\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\n",
    "    dls = to.dataloaders(bs=64)\n",
    "    model = tabular_learner(dls)\n",
    "    classes : list = list(model.dls.vocab)\n",
    "\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    if len(classes) == 2:\n",
    "        model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        model.target_type_ = 'single'\n",
    "        \n",
    "    model_data: Model_data = Model_data(name, model, classes, X_train, y_train, X_test, y_test, to, dls, 'Transformed_and_Split_data')\n",
    "\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # for analysis and further experimentation\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validated Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_datum = namedtuple('Model_datum', ['models', 'results'])\n",
    "\n",
    "# Todo: create crossvalidated experiments for resnet, tabnet, and models with a sklearn api\n",
    "\n",
    "\n",
    "\n",
    "def run_cross_validated_deep_nn_experiment(\n",
    "    df: pd.DataFrame, \n",
    "    name: str, \n",
    "    target_label: str, \n",
    "    shape: tuple, \n",
    "    k_folds: int = 10,\n",
    "    categorical = ['Protocol'],\n",
    "    dropout_rate: float = 0.01,\n",
    "    callbacks: list = [ShowGraphCallback],\n",
    "    metrics: list or None= None,\n",
    "    procs: list = [FillMissing, Categorify, Normalize],\n",
    "    experiment_type: str or None = None,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    lr_choice: str = 'valley'\n",
    ") -> list:\n",
    "    '''\n",
    "        Function will fit a deep neural network to the given dataset using cross-validation\n",
    "    '''\n",
    "    metrics_ = metrics\n",
    "    lr_choice = {\n",
    "        'valley': 0,\n",
    "        'slide': 1,\n",
    "        'steep': 2,\n",
    "        'minimum': 3,\n",
    "    }[lr_choice]\n",
    "\n",
    "    if(experiment_type is None):\n",
    "        experiment_type = f'Deep_NN_{shape[0]}x{shape[1]}'\n",
    "\n",
    "    p = pathlib.Path(name)\n",
    "    name: str = str(p.parts[-1])\n",
    "    dep_var: str = target_label\n",
    "\n",
    "    print('Shape of input dataframe:', df.shape)\n",
    "    print(f\"Running {k_folds}-fold cross-validation\")\n",
    "\n",
    "    categorical_features: list = []\n",
    "    untouched_features  : list = []\n",
    "\n",
    "\n",
    "    for x in categorical:\n",
    "        if x in df.columns:\n",
    "            categorical_features.append(x)\n",
    "\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]) - set(untouched_features))\n",
    "\n",
    "\n",
    "\n",
    "    ss = StratifiedShuffleSplit(n_splits=k_folds, random_state=seed, test_size=1/k_folds)\n",
    "\n",
    "\n",
    "    model_data_list  : list = [0]*k_folds\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = [accuracy, BalancedAccuracy(), RocAuc(), MatthewsCorrCoef(), F1Score(average='macro'), Precision(average='macro'), Recall(average='macro')]\n",
    "        fold_results: dict = {'loss': [], 'accuracy': [], 'BalancedAccuracy': [], 'roc_auc': [], 'MCC': [], 'f1': [], 'precision': [], 'recall': [], 'all': []}\n",
    "    else:\n",
    "        fold_results: dict = {'all': []}\n",
    "\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(ss.split(df.copy().drop(target_label, axis=1), df[target_label])):\n",
    "        \n",
    "        fold_name = f'{name}_fold_{i+1}'\n",
    "\n",
    "        print(i)\n",
    "        print(f'Train Index: {train_index.shape}')\n",
    "        print(f'Test Index: {test_index.shape}')\n",
    "\n",
    "        splits = IndexSplitter(test_index)(df)\n",
    "        to = TabularPandas(\n",
    "            df            , y_names=dep_var                , \n",
    "            splits=splits , cat_names=categorical_features ,\n",
    "            procs=procs   , cont_names=continuous_features , \n",
    "        )\n",
    "\n",
    "        dls = to.dataloaders(bs=batch_size)\n",
    "\n",
    "        model = tabular_learner(\n",
    "            dls, \n",
    "            layers=list(shape), \n",
    "            metrics=metrics, \n",
    "            cbs=callbacks,\n",
    "        )\n",
    "\n",
    "        model.model.emb_drop = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        lr = model.lr_find(suggest_funcs=[valley, slide, steep, minimum])\n",
    "\n",
    "        model.fit_one_cycle(epochs, lr[lr_choice])\n",
    "        model.save(f'{name}.model')\n",
    "\n",
    "        model_results = model.validate()\n",
    "\n",
    "        if(metrics_ == None):\n",
    "            fold_results['loss'].append(model_results[0])\n",
    "            fold_results['accuracy'].append(model_results[1])\n",
    "            fold_results['BalancedAccuracy'].append(model_results[2])\n",
    "            fold_results['roc_auc'].append(model_results[3])\n",
    "            fold_results['MCC'].append(model_results[4])\n",
    "            fold_results['f1'].append(model_results[5])\n",
    "            fold_results['precision'].append(model_results[6])\n",
    "            fold_results['recall'].append(model_results[7])\n",
    "            fold_results['all'].append(model_results)\n",
    "            print(f'loss: {model_results[0]}, accuracy: {model_results[1]*100}%')\n",
    "        else:\n",
    "            fold_results['all'].append(model_results)\n",
    "            print(f'loss: {model_results[0]}')\n",
    "\n",
    "        X_train = to.train.xs.reset_index(drop=True)\n",
    "        X_test = to.valid.xs.reset_index(drop=True)\n",
    "        y_train = to.train.ys.values.ravel()\n",
    "        y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "        wrapped_model = SklearnWrapper(model)\n",
    "        wrapped_model._target_labels = dep_var\n",
    "        classes = list(model.dls.vocab)\n",
    "\n",
    "\n",
    "\n",
    "        # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "        if len(classes) == 2:\n",
    "            wrapped_model.target_type_ = 'binary'\n",
    "        elif len(classes) > 2:  \n",
    "            wrapped_model.target_type_ = 'multiclass'\n",
    "        else:\n",
    "            print('Must be more than one class to perform classification')\n",
    "            raise ValueError('Wrong number of classes')\n",
    "        \n",
    "\n",
    "\n",
    "        model_data: Model_data = Model_data(fold_name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, experiment_type)\n",
    "        model_data_list[i] = model_data\n",
    "\n",
    "    model_datum: Model_datum = Model_datum(model_data_list, fold_results)\n",
    "\n",
    "    return model_datum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Preparation Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_classes_and_create_Xy_df(model_data: Model_data, target_label: str):\n",
    "    \"\"\"\n",
    "        Function takes a Model_data namedtuple and returns a dataframe with the X and decoded y data\n",
    "    \"\"\"\n",
    "    X = model_data.X_train\n",
    "    y = model_data.y_train\n",
    "    classes = model_data.classes\n",
    "\n",
    "    Xy_df = pd.DataFrame(X)\n",
    "    y_s: list = []\n",
    "    for x in y:\n",
    "        y_s.append(classes[x])\n",
    "    Xy_df[target_label] = y_s\n",
    "\n",
    "    return Xy_df\n",
    "\n",
    "\n",
    "def normalize_bilabel_dataset_between_0_1(df: pd.DataFrame, labels = ['Traffic Type', 'Application Type']) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function takes a dataframe and merges its labels before normalizing data. \n",
    "        The labels are then split back into their original form, but the merged label is kept for verification purposes.\n",
    "\n",
    "        returns a trilabel dataframe with the new label under the 'Merged Type' column\n",
    "    '''\n",
    "    inter_df = df.copy()\n",
    "\n",
    "    d1_y = inter_df[labels[0]]\n",
    "    d2_y = inter_df[labels[1]]\n",
    "\n",
    "    merged_y = pd.concat([d1_y, d2_y], axis=1)\n",
    "\n",
    "    merged: list = []\n",
    "    for x in zip(d1_y, d2_y):\n",
    "        merged.append(f'{x[0]}_{x[1]}')\n",
    "\n",
    "    inter_df['Merged Type'] = merged\n",
    "\n",
    "    normalized_model_data = transform_and_split_data(\n",
    "        inter_df,\n",
    "        'Merged Type',\n",
    "        split=0,\n",
    "        scale=True,\n",
    "        categorical=[],\n",
    "        leave_out=labels\n",
    "    )\n",
    "\n",
    "    merged_df = decode_classes_and_create_Xy_df(normalized_model_data, 'Merged Type')\n",
    "\n",
    "    merged_y = merged_df['Merged Type']\n",
    "    new_labels: list = []\n",
    "\n",
    "    for l in labels:\n",
    "        new_labels.append([])\n",
    "\n",
    "    for x in merged_y:\n",
    "        split_label = x.split('_')\n",
    "\n",
    "        for i in range(len(split_label)):\n",
    "            new_labels[i].append(split_label[i])\n",
    "\n",
    "    for i, x in enumerate(labels):\n",
    "        merged_df[x] = new_labels[i]\n",
    "\n",
    "    total_df = merged_df.copy()\n",
    "    total_df['Merged Type'] = merged_y\n",
    "\n",
    "\n",
    "    return total_df\n",
    "\n",
    "    \n",
    "\n",
    "def create_n_cloud_from_dataset(df: pd.DataFrame, n_points: int = 100, target_label: str = 'label', allow_partial: bool = False, segmented=True):\n",
    "    '''\n",
    "        Function takes a dataframe and splits it into point clouds with n_points each. \n",
    "            The generated clouds will all have the indicated number of points, discarding the remainder if allow_partial is False.\n",
    "            The generated clouds will be partitioned by the classes contained within the target_label column.\n",
    "            Each cloud will be labeled with the class it was generated from.\n",
    "\n",
    "            If the segmented parameter is true, the clouds will be split into df.shape[0]/n_points number of clouds.\n",
    "                This is because we segment the dataframe to produce the clouds.\n",
    "            if the segmented parameter is false, the clouds will be split into df.shape[0] - n_points number of clouds.\n",
    "                This is because we will be using a sliding interval to construct the clouds.\n",
    "\n",
    "        returns a numpy array of point clouds as well as a numpy array with class labels and class mappings.\n",
    "    '''\n",
    "\n",
    "    data = transform_and_split_data(df, target_label=target_label, split=0)\n",
    "    X = data.X_train.copy()\n",
    "    X[target_label] = data.y_train.copy()\n",
    "\n",
    "    classes = data.classes\n",
    "\n",
    "    # create n clouds by class\n",
    "    clouds          : list = []\n",
    "    clouds_y        : list = []\n",
    "    clouds_y_decoded: list = []\n",
    "\n",
    "    for i, x in enumerate(classes):\n",
    "        temp_df = X[X[target_label] == i]\n",
    "        if(segmented):\n",
    "            iterations: int = int(temp_df.shape[0]/n_points)\n",
    "        else:\n",
    "            iterations: int = temp_df.shape[0] - n_points\n",
    "\n",
    "\n",
    "        for j in range(int(iterations)):\n",
    "            if(segmented):\n",
    "                cloud = temp_df[j*n_points:(j+1)*n_points].drop(target_label, axis=1).values\n",
    "            else:\n",
    "                cloud = temp_df[j:j+n_points].drop(target_label, axis=1).values\n",
    "            \n",
    "            keep: bool = allow_partial and not segmented\n",
    "            if(len(cloud) == n_points or keep):\n",
    "                clouds.append(cloud)\n",
    "                clouds_y.append(i)\n",
    "                clouds_y_decoded.append(x)\n",
    "\n",
    "    clouds = np.array(clouds)\n",
    "    clouds_y = np.array(clouds_y)\n",
    "    clouds_y_decoded = np.array(clouds_y_decoded)\n",
    "\n",
    "    return clouds, clouds_y, classes, clouds_y_decoded\n",
    "\n",
    "\n",
    "def calculate_correlations(model_data: Model_data, target_label: str):\n",
    "    '''\n",
    "        Function merges together the encoded and standardized model data and labels to calculate pearson correlation\n",
    "    '''\n",
    "\n",
    "    encoded_data = model_data.X_train.copy()\n",
    "    encoded_data[target_label] = model_data.y_train\n",
    "\n",
    "    return encoded_data.corr()\n",
    "\n",
    "\n",
    "def extract_correlations(correlations: pd.DataFrame, feature: str, leave_out: list = ['Traffic Type', 'Application Type']) -> list:\n",
    "    '''\n",
    "        Function takes a correlation dataframe and extracts a list of features correlated with the input feature. \n",
    "            Anything in leave_out is not included in the list.\n",
    "    '''\n",
    "\n",
    "    correlation_order: list = list(correlations.sort_values(by=feature, ascending=False).index)\n",
    "\n",
    "    for x in leave_out:\n",
    "        if x in correlation_order:\n",
    "            correlation_order.remove(x)\n",
    "\n",
    "    return correlation_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical functions on the interval [0,1] with unary function proceeded by binary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unary functions\n",
    "\n",
    "def clamp_to_0_1(x):\n",
    "    return max(min(x, 1), 0)\n",
    "\n",
    "def p_2_pot(x):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using a second degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    return np.power(2, 2*(x-.5))\n",
    "\n",
    "def p_4_pot(x):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using a fourth degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    return np.power(4, 2*(x-.5))\n",
    "\n",
    "def p_6_pot(x):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using a sixth degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    return np.power(6, 2*(x-.5))\n",
    "\n",
    "def gaussian_spike(x):\n",
    "    return np.exp(-np.power(x, 2))\n",
    "\n",
    "def double_gaussian_spike(x):\n",
    "    ''' \n",
    "        A double gaussian spike shape in the interval [0,1]\n",
    "            gives larger changes in gradient to spread around input \n",
    "            in the interval [0,1]\n",
    "    '''\n",
    "\n",
    "    first_spike  = (-0.5) * gaussian_spike((53*x) - 2.2)\n",
    "    second_spike = gaussian_spike((2.7*x) - 2.2)\n",
    "\n",
    "    return first_spike + second_spike\n",
    "\n",
    "def sharp_zero_spike(x):\n",
    "    return 1 - np.exp(-np.power(np.power((200*x), 2)+0.0001, -1))\n",
    "\n",
    "def curve_over_x_1(x):\n",
    "    '''\n",
    "        A curve on the interval [0,1] that maps 0 to 0 and 1 to 1 but stays above\n",
    "            the line f(x) = x\n",
    "    '''\n",
    "\n",
    "    return x*np.exp(1-x)\n",
    "\n",
    "def curve_over_x_2(x):\n",
    "    return -(x - 1)*(x - 1) + 1\n",
    "\n",
    "\n",
    "# binary functions / operations\n",
    "# _ precedes names to prevent name conflicts\n",
    "\n",
    "def _dist_l1(x, y):\n",
    "    return np.abs(x - y)\n",
    "\n",
    "def _dist_l2(x, y):\n",
    "    return np.sqrt(np.power(x, 2) + np.power(y, 2))\n",
    "\n",
    "def _avg(x, y):\n",
    "    return (x + y) / 2\n",
    "\n",
    "def _mult(x, y):\n",
    "    return x * y\n",
    "\n",
    "def _add(x, y):\n",
    "    return x + y\n",
    "\n",
    "def _rbf_l1(x, y):\n",
    "    '''radial basis function'''\n",
    "    return np.exp(-_dist_l1(x, y)**2)\n",
    "\n",
    "def _rbf_l2(x, y):\n",
    "    '''radial basis function'''\n",
    "    return np.exp(-_dist_l2(x, y)**2)\n",
    "\n",
    "\n",
    "# closures allowing customizable functions\n",
    "\n",
    "def mult_by_n(n):\n",
    "    def f(x):\n",
    "        return n * x\n",
    "    return f\n",
    "\n",
    "\n",
    "def p_n_pot(n):\n",
    "    ''' \n",
    "        A pot shape in the interval [0,1] using an nth-degree polynomial\n",
    "        0 and 1 are mapped to 1 and .5 is mapped to 0\n",
    "    '''\n",
    "    def f(x):\n",
    "        return np.power(n, 2*(x-.5))\n",
    "    return f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to automate the Identity-oriented Tabular Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_single_channel_image_matrix(\n",
    "    df_sample: pd.DataFrame, \n",
    "    row_ordering: list, \n",
    "    column_ordering: list, \n",
    "    padding: tuple, \n",
    "    identity: float = 1, \n",
    "    operation: callable = lambda x, y: x*y,\n",
    "    activation: callable = lambda x: x,\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Function will create a len(row_ordering) x len(column_ordering) matrix\n",
    "        representing the input sample.\n",
    "    \n",
    "    This uses the Identity-oriented Tabular Image Transformation algorithm to produce an image from tabular data:\n",
    "            From the sample, two vectors reordered by the specified row and column ordering are padded with identity values\n",
    "                before the operation is then applied to the generalised outer product of the vectors.\n",
    "\n",
    "            Since we have an identity (or constant value if an identity does not exist), the top right corner with a size dictated \n",
    "                by the padding parameter will have a monochromatic square the allows classifiers to understand the orientation \n",
    "                of the image.\n",
    "\n",
    "            The padding[0]xlen(row_ordering) and len(column_ordering)xpadding[1] submatrices will contain feature bands that \n",
    "                are meant to communicate the true values of the input features to the classifier\n",
    "\n",
    "            The remaining interior submatrix will contain the nested feature geometry imposed by the operation on the \n",
    "                outer product of the feature vectors ordered by the input row_ordering and column_ordering vectors.\n",
    "\n",
    "            An activation function is called on the output of the operation in order to tune the output before being clamped \n",
    "                to the interval [0,1]. \n",
    "\n",
    "            The produced image is called a Symmetric Identity-oriented Tabular Image if and only if these conditions are met\n",
    "                1) the row_ordering and column_ordering vectors are the same or are in reversed order\n",
    "                2) padding[0] = padding[1]\n",
    "                3) the operation (binary function) being performed is commutative, i.e. f(a, b) = f(b, a)\n",
    "\n",
    "            The produced image is called an Asymmetric Identity-oriented Tabular Image if it does not fulfill one of the above conditions.\n",
    "    '''\n",
    "\n",
    "    row_vector    = [identity] * padding[0] + list(df_sample.reindex(columns=row_ordering   ).values[0])\n",
    "    column_vector = [identity] * padding[1] + list(df_sample.reindex(columns=column_ordering).values[0])\n",
    "\n",
    "    product: callable = lambda x, y: clamp_to_0_1(activation(operation(x, y)))\n",
    "    composed_outer = np.frompyfunc(product, 2, 1)\n",
    "\n",
    "    pic = composed_outer.outer(row_vector, column_vector).astype(np.float64)\n",
    "\n",
    "    return pic\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using Linear Feature Reduction through Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_data = namedtuple(\n",
    "    'pca_data',\n",
    "    ['title', 'X_train', 'y_train', 'Xy', 'components', 'n_components', 'classes', 'target_label']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_1D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes the 10-tuple model_data from the transform and split function and uses Principal component analysis\n",
    "            to break up the feature dimensions into a single dimension for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=1)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\"]\n",
    "\n",
    "    _X              = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X           = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns   = _X.columns\n",
    "    new_X[\"filler\"] = 0\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=cluster[\"component_1\"],\n",
    "                y=cluster[\"filler\"],\n",
    "                mode='markers',\n",
    "                marker = dict(color = label_colors[i]),\n",
    "                name=classes[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5,zeroline= False\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= '',\n",
    "            ticklen= 5,\n",
    "            zeroline= False\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into two dimension  for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=2)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\"]\n",
    "\n",
    "\n",
    "    _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns = _X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_3D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into three dimensions for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=3)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\"]\n",
    "\n",
    "\n",
    "    _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns = _X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                z      = cluster[\"component_3\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_4D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into four dimensions for cluster analysis.\n",
    "            Displays a 2 two-dimensional graphs of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=4)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\", \"component_4\"]\n",
    "\n",
    "\n",
    "    _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(_X.sample(samples)))\n",
    "    new_X.columns = _X.columns\n",
    "\n",
    "\n",
    "    traces1: list = []\n",
    "    traces2: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces1.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "        traces2.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_3\"]    ,\n",
    "                y      = cluster[\"component_4\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout1 = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout2 = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 3',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 4',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces1, 'layout': layout1})\n",
    "    iplot({'data': traces2, 'layout': layout2})\n",
    "\n",
    "\n",
    "def pca_to_model_data(pca_data: PCA_data) -> Model_data:\n",
    "    '''\n",
    "        Takes a PCA_data namedtuple and returns a model_data namedtuple\n",
    "    '''\n",
    "\n",
    "    model_data: Model_data = Model_data(\n",
    "        pca_data.title,\n",
    "        None,\n",
    "        pca_data.classes,\n",
    "        pca_data.X_train,\n",
    "        pca_data.y_train,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        'pca'\n",
    "    )\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "def run_pca_on_dataset(model_data: tuple, target_label: str = 'label',  title='PCA data', components: int = 6) -> PCA_data:\n",
    "\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    # num_labels   = len(X.groupby(target_label).size())\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=components)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X))\n",
    "    component.columns = ['PCA_' + str(i+1) for i in range(components)]\n",
    "\n",
    "    for col in component.columns:\n",
    "        X[col] = component[col]\n",
    "\n",
    "\n",
    "    Xy = decode_classes_and_create_Xy_df(Model_data(None, None, classes, component.copy(), model_data.y_train, None, None, None, None, 'pca'), target_label)\n",
    "    component[target_label] = model_data.y_train\n",
    "\n",
    "    pca_data: PCA_data = PCA_data(title, X, model_data.y_train, Xy, component, components, classes, target_label)\n",
    "\n",
    "\n",
    "\n",
    "    return pca_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using Non-Linear Feature Reduction through T-Stochastic Neighbor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clusters_by_label_1D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into a single dimension for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X           = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns   = X.columns\n",
    "    new_X[\"filler\"] = 0\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=1, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\"]\n",
    "    new_X             = pd.concat([X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=cluster[\"component_1\"],\n",
    "                y=cluster[\"filler\"],\n",
    "                mode='markers',\n",
    "                marker = dict(color = label_colors[i]),\n",
    "                name=classes[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5,zeroline= False\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= '',\n",
    "            ticklen= 5,\n",
    "            zeroline= False\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into two dimension  for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=2, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(columns=['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\"]\n",
    "    new_X             = pd.concat([new_X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_3D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into three dimensions for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=3, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\"]\n",
    "    new_X             = pd.concat([new_X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                z      = cluster[\"component_3\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title    = 'Component 1',\n",
    "            ticklen  = 5            ,\n",
    "            zeroline = False        ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title    = 'Component 2',\n",
    "            ticklen  = 5            ,\n",
    "            zeroline = False        ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using tools built using Topological Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Topology_data = namedtuple(\n",
    "    'topology_data',\n",
    "    ['title', 'clouds', 'clouds_y', 'persistence', 'fig', 'features', 'classes', 'target_label', 'clouds_y_decoded', 'Xy']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_single_batch_dataset_topology(model_data: tuple, samples: int or None = 1000, dimensions: tuple = (0,1), title=''):\n",
    "    \"\"\"\n",
    "    Function calculates the topology of the dataset and returns it\n",
    "        requires data to be transformed by fastai first to properly encode and normalize the\n",
    "        data.\n",
    "\n",
    "    Returns a namedtuple with the following fields:\n",
    "        title: Graph title\n",
    "        persistence: Vietoris-Rips Peristence\n",
    "        fig: plotly figure\n",
    "        features: Persistence Entropy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if(model_data.name != '' and title == ''):\n",
    "        title = model_data.name\n",
    "\n",
    "\n",
    "    if(samples is None):\n",
    "        X = model_data.X_train.values\n",
    "    else:\n",
    "        X = model_data.X_train.sample(samples).values\n",
    "        \n",
    "    X = np.array([X])\n",
    "\n",
    "    # calculate the topology\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=dimensions)\n",
    "    topology = VR.fit_transform(X)\n",
    "\n",
    "    # calculate the persistence diagram\n",
    "    fig = plot_diagram(topology[0])\n",
    "\n",
    "    # calculate the entropy\n",
    "    PE = PersistenceEntropy()\n",
    "    entropy = PE.fit_transform(topology)\n",
    "\n",
    "    # package in topological_data named tuple. We do this so data can be accessed using [i] or . operators\n",
    "    topological_data: Topology_data = Topology_data(title, X, None, topology, fig, entropy, model_data.classes, '', None, None)\n",
    "\n",
    "    return topological_data\n",
    "\n",
    "\n",
    "def run_tda_on_dataset(df: pd.DataFrame, target_label: str = 'label',  title='Clusters by Label', n_points: int = 100, dimension: tuple = (0,1), allow_partial: bool = False, segmented: bool = True, jobs: int = None) -> Topology_data:\n",
    "    '''\n",
    "        Function takes a dataset and breaks it into n_point clouds, where n_point is the number of points in each cloud.\n",
    "            Then, the giotto-tda Topological Data Analysis toolkit is used to generate a \n",
    "            persistence module for each cloud. This module describes the topological structure of the cloud.\n",
    "            The topological diagrams have their entropy calculated to break up the feature dimensions into\n",
    "            the indicated dimensions corresponding to topological features like connectivity(0d), loopiness(1d), caviatation(2d), etc\n",
    "            for cluster analysis.\n",
    "            Allow_partial is a boolean that determines whether or not to allow clouds to be less \n",
    "            than n_points if there arent n_points remaining.\n",
    "\n",
    "        Returns the topological data for the dataset\n",
    "\n",
    "        Topology_data = namedtuple(\n",
    "           'topology_data',\n",
    "            ['title', 'clouds', 'clouds_y', 'persistence', 'fig', 'features', 'classes', 'target_label', 'clouds_y_decoded']\n",
    "        )\n",
    "    '''\n",
    "\n",
    "\n",
    "    clouds, clouds_y, classes, clouds_y_decoded = create_n_cloud_from_dataset(df, n_points=n_points, target_label=target_label, allow_partial=allow_partial, segmented=segmented)\n",
    "\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=dimension, n_jobs=jobs)\n",
    "    PE = PersistenceEntropy()\n",
    "\n",
    "    topologies = VR.fit_transform(clouds)\n",
    "\n",
    "    entropies = pd.DataFrame(PE.fit_transform(topologies))\n",
    "\n",
    "    col: list = []\n",
    "    for n in dimension:\n",
    "        col.append(f'{n}d entropy')\n",
    "    entropies.columns = col\n",
    "\n",
    "    Xy = entropies.copy()\n",
    "    Xy[target_label] = clouds_y_decoded\n",
    "\n",
    "\n",
    "    entropies[target_label] = clouds_y\n",
    "\n",
    "    topology_data: Topology_data = Topology_data(title, clouds, clouds_y, topologies, None, entropies, classes, target_label, clouds_y_decoded, Xy)\n",
    "\n",
    "    return topology_data\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_tda(df: pd.DataFrame, target_label: str = 'label',  title='Clusters by Label', n_points: int = 100, allow_partial: bool = False) -> Topology_data:\n",
    "    '''\n",
    "        Function takes a dataset and breaks it into n_point clouds, where n_point is the number of points in each cloud.\n",
    "            Then, the giotto-tda Topological Data Analysis toolkit is used to generate a \n",
    "            persistence module for each cloud. This module describes the topological structure of the cloud.\n",
    "            The topological diagrams have their entropy calculated to break up the feature dimensions into\n",
    "            two dimensions corresponding to connectivity and loopinessfor cluster analysis.\n",
    "            Allow_partial is a boolean that determines whether or not to allow clouds to be less \n",
    "            than n_points if there arent n_points remaining.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        Returns the topological data for the dataset\n",
    "    '''\n",
    "\n",
    "\n",
    "    clouds, clouds_y, classes, clouds_y_decoded = create_n_cloud_from_dataset(df, n_points=n_points, target_label=target_label, allow_partial=allow_partial)\n",
    "\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=(0,1))\n",
    "    PE = PersistenceEntropy()\n",
    "\n",
    "    topologies      = VR.fit_transform(clouds)\n",
    "\n",
    "    X               = pd.DataFrame(PE.fit_transform(topologies))\n",
    "    X.columns       = [\"Connectivity\", \"Loopiness\"]\n",
    "    X[target_label] = clouds_y\n",
    "    num_labels      = len(X.groupby(target_label).size())\n",
    "    label_colors    = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = X[X[target_label] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"Connectivity\"]   ,\n",
    "                y      = cluster[\"Loopiness\"]      ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Connectivity',\n",
    "            ticklen= 5           ,\n",
    "            zeroline= False      ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Loopiness',\n",
    "            ticklen= 5        ,\n",
    "            zeroline= False   ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = {'data': traces, 'layout': layout}\n",
    "\n",
    "    iplot(fig)\n",
    "\n",
    "    topology_data: Topology_data = Topology_data(title, clouds, clouds_y, topologies, fig, X, classes, target_label, clouds_y_decoded, None)\n",
    "\n",
    "    return topology_data\n",
    "\n",
    "\n",
    "def extract_topological_correlations(topology_data: Topology_data, original_dataset: pd.DataFrame, label: str):\n",
    "    '''\n",
    "        Takes a Topology_data namedtuple and returns a dictionary of lists of features ordered topological correlations \n",
    "            for each value in the topology_data.entropy dataframe with the values of the points in the point clouds\n",
    "            being used to calculate the persistence entropy\n",
    "    '''        \n",
    "\n",
    "    cols = list(original_dataset.columns)\n",
    "    if(label in cols):\n",
    "        cols.remove(label)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    new_cols = list(topology_data.features.columns)\n",
    "    for i, x in enumerate(topology_data.clouds):\n",
    "        new_df = pd.DataFrame(x)\n",
    "        for col in new_cols:\n",
    "            new_df[col] = topology_data.features[col][i]\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "    df.columns = cols + new_cols\n",
    "\n",
    "    correlation = df.corr()\n",
    "\n",
    "    correlations: dict = {}\n",
    "    for col in new_cols:\n",
    "        if col != label:\n",
    "            correlations[col] = extract_correlations(correlation, col)\n",
    "\n",
    "    for cor in correlations.keys():\n",
    "        for col in new_cols:\n",
    "            if col in correlations[cor]:\n",
    "                correlations[cor].remove(col)\n",
    "\n",
    "\n",
    "    correlations['Graph'] = plt.figure(figsize=(20,15))\n",
    "    sns.heatmap(correlation, vmax=1, square=True, cmap='YlGnBu')\n",
    "    plt.title(\"Topographic Correlation Heatmap for n_clouds Generated by Traffic Type\", fontsize=16)\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "\n",
    "def top_to_model_data(topology_data: Topology_data) -> Model_data:\n",
    "    '''\n",
    "        Takes a Topology_data namedtuple and returns a model_data namedtuple\n",
    "    '''\n",
    "\n",
    "    X_train = topology_data.features.copy()\n",
    "\n",
    "    if topology_data.target_label in X_train.columns:\n",
    "        X_train.drop(topology_data.target_label, axis=1, inplace=True)\n",
    "\n",
    "    model_data: Model_data = Model_data(\n",
    "        topology_data.title,\n",
    "        None,\n",
    "        topology_data.classes,\n",
    "        X_train,   \n",
    "        topology_data.clouds_y,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        'topology'\n",
    "    )\n",
    "\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools to visualize the results of classification experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a confusion matrix\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ConfusionMatrix(model_data[1], classes=model_data[2], title=model_data[0], ax=ax, cmap=cmap, is_fitted=True)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    \n",
    "    return visualizer\n",
    "\n",
    "\n",
    "def visualize_roc(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a \n",
    "        Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ROCAUC(model_data[1], classes=model_data[2], title=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    \n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def visualize_pr_curve(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a \n",
    "        Precision-Recall Curve\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.PrecisionRecallCurve(model_data[1], title=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def visualize_report(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a report\n",
    "        detailing the Precision, Recall, f1, and Support scores for all \n",
    "        classification outcomes\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ClassificationReport(model_data[1], classes=model_data[2], title=model_data[0], support=True, ax=ax, cmap=cmap)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def visualize_class_balance(model_data: tuple, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a histogram\n",
    "        detailing the balance between classification outcomes\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.target.ClassBalance(labels=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.fit(model_data[4], model_data[6])\n",
    "    \n",
    "    return visualizer\n",
    "\n",
    "\n",
    "\n",
    "def confusion_matrix_from_dataset(model_data: tuple, df: pd.DataFrame, ax=None, cmap='Blues') -> yb.classifier:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses the model to classify\n",
    "            the data passed in as the dataframe. Produces a confusion matrix\n",
    "            (currently only confirmed to work with fastai models)\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    dl = model_data[1].model.dls.test_dl(df, bs=64)\n",
    "    preds, v, dec_preds = model_data[1].model.get_preds(dl=dl, with_decoded=True)\n",
    "\n",
    "    visualizer = yb.classifier.ConfusionMatrix(model_data[1], classes=model_data[2], title=model_data[0], ax=ax, cmap=cmap)\n",
    "    visualizer.score(dl.xs, dl.y)\n",
    "    acc = accuracy_score(dl.y, dec_preds)\n",
    "    print(f'Accuracy: {acc}')\n",
    "\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "def show_stacked_graphs(figures: list, stack_shape: tuple = (1,1), title: str = '', individual_titles: list or str = None) -> widgets.VBox: \n",
    "    \"\"\"\n",
    "    Function takes a list of figures and stacks them in a grid.\n",
    "        stack_shape is a tuple of the number of rows and columns in the grid and\n",
    "        the product of the indices must equal the number of figures.\n",
    "        individual_titles allows one to specify titles for each figure, or a single title for all figures.\n",
    "\n",
    "    Returns a plotly figure with the input figures stacked as indicated\n",
    "    \"\"\"\n",
    "\n",
    "    if(individual_titles is not None and type(individual_titles) is not str):\n",
    "        assert len(figures) == len(individual_titles)     , \"Number of titles must match number of figures\"\n",
    "    assert len(figures) == stack_shape[0] * stack_shape[1], \"Number of figures must match stack shape\"\n",
    "\n",
    "    vertical_stack  : list = []\n",
    "    horizantal_stack: list = []\n",
    "\n",
    "    if(title != ''):\n",
    "        vertical_stack.append(widgets.Label(title))\n",
    "    \n",
    "    for i in range(stack_shape[1]):\n",
    "        for j in range(stack_shape[0]):\n",
    "                \n",
    "            temp_fig = go.FigureWidget(figures[i * stack_shape[0] + j].data)\n",
    "            if(type(individual_titles) == str):\n",
    "                temp_fig.layout.title = individual_titles\n",
    "            elif(type(individual_titles) == list):\n",
    "                temp_fig.layout.title = individual_titles[i * stack_shape[0] + j]\n",
    "                \n",
    "            horizantal_stack.append(temp_fig)\n",
    "        vertical_stack.append(widgets.HBox(horizantal_stack))\n",
    "        horizantal_stack = []\n",
    "\n",
    "    fig = widgets.VBox(vertical_stack)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D(model_data: Model_data, x_col: str, y_col: str, target_label: str = 'label', samples=25000, title='Clusters by Label', ) -> dict:\n",
    "    '''\n",
    "        Takes a model_data tuple and displays the indicated columns of the X_train data as a scatter plot.\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X[target_label]   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby(target_label).size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "\n",
    "    # _X            = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X[target_label] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[x_col]            ,\n",
    "                y      = cluster[y_col]            ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis = dict(\n",
    "            title    = x_col,\n",
    "            ticklen  = 5    ,\n",
    "            zeroline = False,\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            title    = y_col,\n",
    "            ticklen  = 5    ,\n",
    "            zeroline = False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig: dict = {'data': traces, 'layout': layout}\n",
    "    fig = go.Figure(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def faceted_clusters_by_label_2D(model_data: Model_data, x_col: str, y_col: str, target_label: str = 'label', samples=25000, title='Clusters by Label', ) -> dict:\n",
    "    '''\n",
    "        Takes a model_data tuple and returns a list of figures that each contain a highlighted value in the label with the other values\n",
    "            serving as a grey background in a scatter plot that demonstrates the distribution\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X[target_label]   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby(target_label).size()) + 1\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.7)\n",
    "\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "    figs: list = []\n",
    "    for i, x in enumerate(classes):\n",
    "\n",
    "        cluster_1 = new_X[new_X[target_label] == i]\n",
    "        cluster_2 = new_X[new_X[target_label] != i]\n",
    "\n",
    "        traces: list = []\n",
    "\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster_2[x_col]                     ,\n",
    "                y      = cluster_2[y_col]                     ,\n",
    "                marker = {'color': 'rgba(170, 170, 170, 0.5)'},\n",
    "                mode   = 'markers'                            ,\n",
    "                name   = 'Other'                              ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster_1[x_col]          ,\n",
    "                y      = cluster_1[y_col]          ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = x                         ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        layout = dict(\n",
    "            title = title,\n",
    "            xaxis = dict(\n",
    "                title    = x_col,\n",
    "                ticklen  = 5    ,\n",
    "                zeroline = False,\n",
    "            ),\n",
    "            yaxis = dict(\n",
    "                title    = y_col,\n",
    "                ticklen  = 5    ,\n",
    "                zeroline = False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig = go.Figure({'data': traces, 'layout': layout})\n",
    "        # a, b = fig.data\n",
    "        # fig.data = b, a\n",
    "        figs.append(fig)\n",
    "\n",
    "    return figs\n",
    "\n",
    "\n",
    "def visualize_side_by_side(\n",
    "    model_datum: list,\n",
    "    results: dict,\n",
    "    title: str = \"Confusion Matrices\",\n",
    "    model_descriptions: list or None = None,\n",
    "    plotting_function: callable = visualize_confusion_matrix,\n",
    "    shape: tuple = (2,5),\n",
    "    size: tuple = (20,10),\n",
    "    x_label: str = 'Predicted',\n",
    "    y_label: str = 'True',\n",
    ") -> tuple:\n",
    "    '''\n",
    "        Function will take the plotting function and execute it on each Model_data tuple passed in through the model_datum list\n",
    "            The plots will be oriented in a subplot grid with the number of rows and columns specified by the shape tuple\n",
    "            average accuracy will be calculated and displayed in the subtitle of the figure\n",
    "            \n",
    "    '''\n",
    "\n",
    "    print('Ignore yellowbrick warnings, this is a side-effect of using the sklearn wrapper on the fastai model')\n",
    "    rows = shape[0]\n",
    "    cols = shape[1]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=size)\n",
    "    fig.suptitle(title)\n",
    "    fig.text(0.5, .001, f'    Average Accuracy: {np.average(results[\"accuracy\"]) * 100: .4f}', ha='center')\n",
    "\n",
    "    viz: list = [0] * len(model_datum)\n",
    "    for i in range(rows*cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        if i < len(model_datum):\n",
    "            if(rows == 1):\n",
    "                current_ax = ax[col]\n",
    "            else:\n",
    "                current_ax = ax[row][col]\n",
    "            viz[i] = plotting_function(model_datum[i], ax=current_ax)\n",
    "            viz[i].finalize()\n",
    "            if model_descriptions is not None:\n",
    "                current_ax.set_title(model_descriptions[i])\n",
    "\n",
    "        if(row == rows-1):\n",
    "            current_ax.set_xlabel(x_label)\n",
    "        else:\n",
    "            current_ax.set_xlabel('')\n",
    "            current_ax.xaxis.set_ticklabels([])\n",
    "\n",
    "        if(col == 0):\n",
    "            current_ax.set_ylabel(y_label)\n",
    "        else:\n",
    "            current_ax.set_ylabel('')\n",
    "            current_ax.yaxis.set_ticklabels([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def show_feature_importance_TA(model: Learner, df: pd.DataFrame, ax = None, figsize = (15,5), title='Feature Importances') -> tuple:\n",
    "    '''\n",
    "        Function shows the feature importance calculated by the attention layer of the TabNet model.\n",
    "\n",
    "    '''\n",
    "\n",
    "    dl = model.dls.test_dl(df, bs=64)\n",
    "    feature_importances = tabnet_feature_importances(model.model, dl)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.bar(dl.x_names, feature_importances, color='g')\n",
    "    ax.set_xticks(range(len(feature_importances)))\n",
    "    ax.set_xticklabels(dl.x_names, rotation=90)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def plot_point_clouds(clouds: list, shape: tuple = (1, 4), titles: list or None = None, homology_dimensions: list or None = None, color='magma'):\n",
    "\n",
    "\n",
    "    specs = np.zeros(shape).astype(object)\n",
    "    cols = shape[1]\n",
    "\n",
    "    for i in range(specs.shape[0]):\n",
    "        for j in range(specs.shape[1]):\n",
    "            specs[i, j] = {'type': 'scene'}\n",
    "\n",
    "\n",
    "    fig = make_subplots(rows=shape[0], cols=shape[1], specs=specs.tolist(), column_titles=titles)\n",
    "\n",
    "    for i, cloud in enumerate(clouds):\n",
    "        row = (i // cols) + 1\n",
    "        col = (i % cols) + 1\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            # x=cloud[0, :],\n",
    "            # y=cloud[1, :],\n",
    "            # z=cloud[2, :],\n",
    "            x=cloud[:, 0],\n",
    "            y=cloud[:, 1],\n",
    "            z=cloud[:, 2],\n",
    "            showlegend=False,\n",
    "            mode=\"markers\",\n",
    "            marker={\"size\": 4,\n",
    "                    \"color\": list(range(cloud.shape[0])),\n",
    "                    \"colorscale\": color,\n",
    "                    \"opacity\": 0.8}\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_persistence_diagrams(diagrams: list, shape: tuple = (1, 4), titles: list or None = None, homology_dimensions: list or None = None, colors: list or None = None):\n",
    "    '''\n",
    "        Function plots a set of persistence diagrams\n",
    "    '''\n",
    "\n",
    "\n",
    "    cols = shape[1]\n",
    "    fig = make_subplots(rows=shape[0], cols=cols, column_titles=titles)\n",
    "\n",
    "    if homology_dimensions is None:\n",
    "        homology_dimensions = np.unique(diagrams[0][:, 2])\n",
    "\n",
    "    if colors is None:\n",
    "        colors = get_n_color_list(len(homology_dimensions), opacity=.8)\n",
    "\n",
    "    for i, diagram in enumerate(diagrams):\n",
    "        row = (i // cols) + 1\n",
    "        col = (i % cols) + 1\n",
    "\n",
    "\n",
    "        diagram = diagram[diagram[:, 0] != diagram[:, 1]]\n",
    "        diagram_no_dims = diagram[:, :2]\n",
    "        posinfinite_mask = np.isposinf(diagram_no_dims)\n",
    "        neginfinite_mask = np.isneginf(diagram_no_dims)\n",
    "        max_val = np.max(np.where(posinfinite_mask, -np.inf, diagram_no_dims))\n",
    "        min_val = np.min(np.where(neginfinite_mask, np.inf, diagram_no_dims))\n",
    "        parameter_range = max_val - min_val\n",
    "        extra_space_factor = 0.02\n",
    "        has_posinfinite_death = np.any(posinfinite_mask[:, 1])\n",
    "\n",
    "        if has_posinfinite_death:\n",
    "            posinfinity_val = max_val + 0.1 * parameter_range\n",
    "            extra_space_factor += 0.1\n",
    "\n",
    "        extra_space = extra_space_factor * parameter_range\n",
    "        min_val_display = min_val - extra_space\n",
    "        max_val_display = max_val + extra_space\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = [min_val_display, max_val_display],\n",
    "                y = [min_val_display, max_val_display],\n",
    "                mode       = \"lines\",\n",
    "                line       = {\"dash\": \"dash\", \"width\": 1, \"color\": \"black\"},\n",
    "                showlegend = False,\n",
    "                hoverinfo  = \"none\"\n",
    "            ), \n",
    "            row = row, \n",
    "            col = col\n",
    "        )\n",
    "\n",
    "        for k, dim in enumerate(homology_dimensions):\n",
    "            name = f\"{int(dim)}d Persistence\" if dim != np.inf else \"Persistence\"\n",
    "            subdiagram = diagram[diagram[:, 2] == dim]\n",
    "            unique, inverse, counts = np.unique(subdiagram, axis=0, return_inverse=True, return_counts=True)\n",
    "            \n",
    "            hovertext = [\n",
    "                f\"{tuple(unique[unique_row_index][:2])}\" + (\n",
    "                    f\", multiplicity: {counts[unique_row_index]}\" if counts[unique_row_index] > 1 else \"\"\n",
    "                ) for unique_row_index in inverse\n",
    "            ]\n",
    "\n",
    "            y = subdiagram[:, 1]\n",
    "            if has_posinfinite_death:\n",
    "                y[np.isposinf(y)] = posinfinity_val\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = subdiagram[:, 0], \n",
    "                    y = y, \n",
    "                    hoverinfo  = \"text\", hovertext=hovertext, name=name,\n",
    "                    marker     = {'color': colors[k]},\n",
    "                    mode       = \"markers\", \n",
    "                    showlegend = (i==0),\n",
    "                ), \n",
    "                row=row, \n",
    "                col=col\n",
    "            )\n",
    "\n",
    "\n",
    "        if(row == (len(diagrams)//cols)):\n",
    "            fig.update_xaxes(title_text=\"Feature Birth\", row=row, col=col)\n",
    "        \n",
    "        if(col == 1):\n",
    "            fig.update_yaxes(title_text=\"Feature Death\", row=row, col=col)\n",
    "\n",
    "\n",
    "        if has_posinfinite_death:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = [min_val_display, max_val_display],\n",
    "                    y = [posinfinity_val, posinfinity_val],\n",
    "                    line = {\"dash\": \"dash\", \"width\": 0.5, \"color\": \"black\"},\n",
    "                    hoverinfo = \"none\",\n",
    "                    showlegend = True,\n",
    "                    name = u\"\\u221E\",\n",
    "                    mode = \"lines\",\n",
    "                ), \n",
    "                row=row, \n",
    "                col=col\n",
    "            )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_point_clouds_and_persistence_diagram(point_clouds: list, persistence_diagrams: list, rows: int, columns: int, homology_dimensions: list or None = None, color='magma', column_titles: list or None = None):\n",
    "\n",
    "\n",
    "\n",
    "    cols = columns\n",
    "\n",
    "    ri1 = {'type': 'scene'}\n",
    "    ri2 = {'type': 'xy'}\n",
    "    specs = np.zeros((2,4)).astype(object)\n",
    "\n",
    "    for i in range(specs.shape[0]):\n",
    "        for j in range(specs.shape[1]):\n",
    "            if(i == 0):\n",
    "                specs[i, j] = ri1\n",
    "            else:\n",
    "                specs[i, j] = ri2\n",
    "\n",
    "\n",
    "    if homology_dimensions is None:\n",
    "        homology_dimensions = np.unique(persistence_diagrams[0][:, 2])\n",
    "\n",
    "    label_colors = get_n_color_list(len(homology_dimensions), opacity=.8)\n",
    "\n",
    "\n",
    "    fig = make_subplots(rows=rows, cols=cols, specs=specs.tolist(), column_titles=column_titles)\n",
    "\n",
    "\n",
    "\n",
    "    for i, diagram in enumerate(persistence_diagrams):\n",
    "        row = (i // cols) + 1\n",
    "        col = (i % cols) + 1\n",
    "\n",
    "        if(i >= rows*cols):\n",
    "            break\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=point_clouds[i][0, :],\n",
    "            y=point_clouds[i][1, :],\n",
    "            z=point_clouds[i][2, :],\n",
    "            # x=point_clouds[i][:, 0],\n",
    "            # y=point_clouds[i][:, 1],\n",
    "            # z=point_clouds[i][:, 2],\n",
    "            showlegend=False,\n",
    "            mode=\"markers\",\n",
    "            marker={\"size\": 4,\n",
    "                    \"color\": list(range(point_clouds[j].shape[0])),\n",
    "                    \"colorscale\": color,\n",
    "                    \"opacity\": 0.8}\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "\n",
    "        row += 1\n",
    "\n",
    "        diagram = diagram[diagram[:, 0] != diagram[:, 1]]\n",
    "        diagram_num_dims = diagram[:, :2]\n",
    "        posinfinite_mask = np.isposinf(diagram_num_dims)\n",
    "        neginfinite_mask = np.isneginf(diagram_num_dims)\n",
    "        max_val = np.max(np.where(posinfinite_mask, -np.inf, diagram_num_dims))\n",
    "        min_val = np.min(np.where(neginfinite_mask, np.inf, diagram_num_dims))\n",
    "        parameter_range = max_val - min_val\n",
    "        extra_space_factor = 0.02\n",
    "        has_posinfinite_death = np.any(posinfinite_mask[:, 1])\n",
    "\n",
    "        if has_posinfinite_death:\n",
    "            posinfinity_val     = max_val + 0.1 * parameter_range\n",
    "            extra_space_factor += 0.1\n",
    "\n",
    "\n",
    "        extra_space     = extra_space_factor * parameter_range\n",
    "        min_val_display = min_val - extra_space\n",
    "        max_val_display = max_val + extra_space\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x = [min_val_display, max_val_display],\n",
    "                y = [min_val_display, max_val_display],\n",
    "                mode = \"lines\",\n",
    "                line = {\"dash\": \"dash\", \"width\": 1, \"color\": \"black\"},\n",
    "                showlegend = False,\n",
    "                hoverinfo = \"none\"\n",
    "            ), \n",
    "            row = row, \n",
    "            col = col\n",
    "        )\n",
    "\n",
    "        for k, dim in enumerate(homology_dimensions):\n",
    "\n",
    "            name = f\"{int(dim)}d Persistence\" if dim != np.inf else \"Persistence\"\n",
    "            subdiagram = diagram[diagram[:, 2] == dim]\n",
    "            unique, inverse, counts = np.unique(subdiagram, axis=0, return_inverse=True, return_counts=True)\n",
    "            \n",
    "            hovertext = [\n",
    "                f\"{tuple(unique[unique_row_index][:2])}\" + (\n",
    "                    f\", multiplicity: {counts[unique_row_index]}\"\n",
    "                    if counts[unique_row_index] > 1 else \"\"\n",
    "                ) for unique_row_index in inverse\n",
    "            ]\n",
    "\n",
    "            y = subdiagram[:, 1]\n",
    "            if has_posinfinite_death:\n",
    "                y[np.isposinf(y)] = posinfinity_val\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x = subdiagram[:, 0], \n",
    "                    y = y, \n",
    "                    name       = name     , \n",
    "                    mode       = \"markers\", \n",
    "                    hoverinfo  = \"text\"   , \n",
    "                    hovertext  = hovertext, \n",
    "                    showlegend = (i==0)   ,\n",
    "                    marker     = {'color': label_colors[k]},\n",
    "                ), \n",
    "                row = row, \n",
    "                col = col\n",
    "            )\n",
    "\n",
    "\n",
    "        if(row == 2):\n",
    "            fig.update_xaxes(title_text=\"Feature Birth\", row=row, col=col)\n",
    "        \n",
    "        if(col == 1):\n",
    "            fig.update_yaxes(title_text=\"Feature Death\", row=row, col=col)\n",
    "\n",
    "        # Add a horizontal dashed line for points with infinite death\n",
    "        if has_posinfinite_death:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[min_val_display, max_val_display],\n",
    "                y=[posinfinity_val, posinfinity_val],\n",
    "                mode=\"lines\",\n",
    "                line={\"dash\": \"dash\", \"width\": 0.5, \"color\": \"black\"},\n",
    "                showlegend=True,\n",
    "                name=u\"\\u221E\",\n",
    "                hoverinfo=\"none\"\n",
    "            ), row=row, col=col)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be looking at 9 files:\n",
      "[   './phase2/Darknet_experiments_base.csv',\n",
      "    './phase2/copulagan_application_dataset_50k.csv',\n",
      "    './phase2/copulagan_traffic_dataset_100k.csv',\n",
      "    './phase2/ctgan_application_dataset_50k.csv',\n",
      "    './phase2/ctgan_traffic_dataset_100k.csv',\n",
      "    './phase2/vae_application_dataset.csv',\n",
      "    './phase2/vae_traffic_dataset.csv',\n",
      "    './phase2/smote_fake_application.csv',\n",
      "    './phase2/smote_fake_traffic.csv']\n"
     ]
    }
   ],
   "source": [
    "print(f'We will be looking at {len(file_set)} files:')\n",
    "pretty(file_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "We load the data and separate the dataset by label, giving us a traffic dataset and an application dataset. We will show correlations in both datasets, extract the correlations to labels or features, and use these to construct images from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/9: We now look at ./phase2/Darknet_experiments_base.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/Darknet_experiments_base.csv\n",
      "\tTo Dataset Cache: ./cache/Darknet_experiments_base.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/Darknet_experiments_base.csv  \n",
      "        Job Number:\t\t\t1\n",
      "        Shape:\t\t\t\t(117620, 64)\n",
      "        Samples:\t\t\t117620 \n",
      "        Features:\t\t\t64\n",
      "    \n",
      "Dataset 2/9: We now look at ./phase2/copulagan_application_dataset_50k.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/copulagan_application_dataset_50k.csv\n",
      "\tTo Dataset Cache: ./cache/copulagan_application_dataset_50k.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/copulagan_application_dataset_50k.csv  \n",
      "        Job Number:\t\t\t2\n",
      "        Shape:\t\t\t\t(400000, 65)\n",
      "        Samples:\t\t\t400000 \n",
      "        Features:\t\t\t65\n",
      "    \n",
      "Dataset 3/9: We now look at ./phase2/copulagan_traffic_dataset_100k.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/copulagan_traffic_dataset_100k.csv\n",
      "\tTo Dataset Cache: ./cache/copulagan_traffic_dataset_100k.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/copulagan_traffic_dataset_100k.csv  \n",
      "        Job Number:\t\t\t3\n",
      "        Shape:\t\t\t\t(300000, 65)\n",
      "        Samples:\t\t\t300000 \n",
      "        Features:\t\t\t65\n",
      "    \n",
      "Dataset 4/9: We now look at ./phase2/ctgan_application_dataset_50k.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/ctgan_application_dataset_50k.csv\n",
      "\tTo Dataset Cache: ./cache/ctgan_application_dataset_50k.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/ctgan_application_dataset_50k.csv  \n",
      "        Job Number:\t\t\t4\n",
      "        Shape:\t\t\t\t(400000, 65)\n",
      "        Samples:\t\t\t400000 \n",
      "        Features:\t\t\t65\n",
      "    \n",
      "Dataset 5/9: We now look at ./phase2/ctgan_traffic_dataset_100k.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/ctgan_traffic_dataset_100k.csv\n",
      "\tTo Dataset Cache: ./cache/ctgan_traffic_dataset_100k.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/ctgan_traffic_dataset_100k.csv  \n",
      "        Job Number:\t\t\t5\n",
      "        Shape:\t\t\t\t(300000, 65)\n",
      "        Samples:\t\t\t300000 \n",
      "        Features:\t\t\t65\n",
      "    \n",
      "Dataset 6/9: We now look at ./phase2/vae_application_dataset.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/vae_application_dataset.csv\n",
      "\tTo Dataset Cache: ./cache/vae_application_dataset.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/vae_application_dataset.csv  \n",
      "        Job Number:\t\t\t6\n",
      "        Shape:\t\t\t\t(400000, 65)\n",
      "        Samples:\t\t\t400000 \n",
      "        Features:\t\t\t65\n",
      "    \n",
      "Dataset 7/9: We now look at ./phase2/vae_traffic_dataset.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/vae_traffic_dataset.csv\n",
      "\tTo Dataset Cache: ./cache/vae_traffic_dataset.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/vae_traffic_dataset.csv  \n",
      "        Job Number:\t\t\t7\n",
      "        Shape:\t\t\t\t(300000, 65)\n",
      "        Samples:\t\t\t300000 \n",
      "        Features:\t\t\t65\n",
      "    \n",
      "Dataset 8/9: We now look at ./phase2/smote_fake_application.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/smote_fake_application.csv\n",
      "\tTo Dataset Cache: ./cache/smote_fake_application.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/smote_fake_application.csv  \n",
      "        Job Number:\t\t\t8\n",
      "        Shape:\t\t\t\t(270540, 64)\n",
      "        Samples:\t\t\t270540 \n",
      "        Features:\t\t\t64\n",
      "    \n",
      "Dataset 9/9: We now look at ./phase2/smote_fake_traffic.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase2/smote_fake_traffic.csv\n",
      "\tTo Dataset Cache: ./cache/smote_fake_traffic.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase2/smote_fake_traffic.csv  \n",
      "        Job Number:\t\t\t9\n",
      "        Shape:\t\t\t\t(162307, 64)\n",
      "        Samples:\t\t\t162307 \n",
      "        Features:\t\t\t64\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "base_dataset                 : dict = examine_dataset(1)\n",
    "copulagan_application_dataset: dict = examine_dataset(2)\n",
    "copulagan_traffic_dataset    : dict = examine_dataset(3)\n",
    "ctgan_application_dataset    : dict = examine_dataset(4)\n",
    "ctgan_traffic_dataset        : dict = examine_dataset(5)\n",
    "vae_application_dataset      : dict = examine_dataset(6)\n",
    "vae_traffic_dataset          : dict = examine_dataset(7)\n",
    "smote_application_dataset    : dict = examine_dataset(8)\n",
    "smote_traffic_dataset        : dict = examine_dataset(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700000, 64)\n",
      "(700000, 64)\n",
      "(700000, 64)\n",
      "(432847, 64)\n"
     ]
    }
   ],
   "source": [
    "final_df = base_dataset['Dataset'].copy()\n",
    "final_df['Data Source'] = 'Real'\n",
    "\n",
    "synthetic_datasets_1 = {\n",
    "    'CopulaGAN': pd.concat([copulagan_application_dataset['Dataset'], copulagan_traffic_dataset['Dataset']]),\n",
    "    'CTGAN': pd.concat([ctgan_application_dataset['Dataset'], ctgan_traffic_dataset['Dataset']]),\n",
    "    'VAE': pd.concat([vae_application_dataset['Dataset'], vae_traffic_dataset['Dataset']]),\n",
    "    'SMOTE': pd.concat([smote_application_dataset['Dataset'], smote_traffic_dataset['Dataset']]),\n",
    "}\n",
    "\n",
    "for key in synthetic_datasets_1.keys():\n",
    "    if('Unnamed: 0' in synthetic_datasets_1[key].columns):\n",
    "        synthetic_datasets_1[key] = synthetic_datasets_1[key].drop('Unnamed: 0', axis=1)\n",
    "\n",
    "for key in synthetic_datasets_1.keys():\n",
    "    print(synthetic_datasets_1[key].shape)\n",
    "\n",
    "synthetic_datasets_2 = {}\n",
    "\n",
    "for key in synthetic_datasets_1.keys():\n",
    "    synthetic_datasets_2[key] = synthetic_datasets_1[key].copy()\n",
    "    synthetic_datasets_2[key]['Data Source'] = key\n",
    "\n",
    "for key in synthetic_datasets_2.keys():\n",
    "    final_df = pd.concat([final_df, synthetic_datasets_2[key]])\n",
    "\n",
    "final_df.to_csv('./phase2/CMU_SynTraffic_2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packet</th>\n",
       "      <th>Total Bwd packets</th>\n",
       "      <th>Total Length of Fwd Packet</th>\n",
       "      <th>Total Length of Bwd Packet</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <th>Bwd Packet Length Min</th>\n",
       "      <th>Bwd Packet Length Mean</th>\n",
       "      <th>Bwd Packet Length Std</th>\n",
       "      <th>Flow Bytes/s</th>\n",
       "      <th>Flow Packets/s</th>\n",
       "      <th>Flow IAT Mean</th>\n",
       "      <th>Flow IAT Std</th>\n",
       "      <th>Flow IAT Max</th>\n",
       "      <th>Flow IAT Min</th>\n",
       "      <th>Fwd IAT Total</th>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <th>Fwd IAT Std</th>\n",
       "      <th>Fwd IAT Max</th>\n",
       "      <th>Fwd IAT Min</th>\n",
       "      <th>Bwd IAT Total</th>\n",
       "      <th>Bwd IAT Mean</th>\n",
       "      <th>Bwd IAT Std</th>\n",
       "      <th>Bwd IAT Max</th>\n",
       "      <th>Bwd IAT Min</th>\n",
       "      <th>Fwd PSH Flags</th>\n",
       "      <th>Fwd Header Length</th>\n",
       "      <th>Bwd Header Length</th>\n",
       "      <th>Fwd Packets/s</th>\n",
       "      <th>Bwd Packets/s</th>\n",
       "      <th>Packet Length Min</th>\n",
       "      <th>Packet Length Max</th>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <th>Packet Length Std</th>\n",
       "      <th>Packet Length Variance</th>\n",
       "      <th>FIN Flag Count</th>\n",
       "      <th>SYN Flag Count</th>\n",
       "      <th>RST Flag Count</th>\n",
       "      <th>PSH Flag Count</th>\n",
       "      <th>ACK Flag Count</th>\n",
       "      <th>Down/Up Ratio</th>\n",
       "      <th>Average Packet Size</th>\n",
       "      <th>Fwd Segment Size Avg</th>\n",
       "      <th>Bwd Segment Size Avg</th>\n",
       "      <th>Bwd Packet/Bulk Avg</th>\n",
       "      <th>Bwd Bulk Rate Avg</th>\n",
       "      <th>Subflow Fwd Packets</th>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <th>Subflow Bwd Bytes</th>\n",
       "      <th>FWD Init Win Bytes</th>\n",
       "      <th>Bwd Init Win Bytes</th>\n",
       "      <th>Fwd Act Data Pkts</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Traffic Type</th>\n",
       "      <th>Application Type</th>\n",
       "      <th>Data Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>119384497</td>\n",
       "      <td>767</td>\n",
       "      <td>2027</td>\n",
       "      <td>90681</td>\n",
       "      <td>2448831</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>118.228162</td>\n",
       "      <td>224.244409</td>\n",
       "      <td>1420</td>\n",
       "      <td>0</td>\n",
       "      <td>1208.106068</td>\n",
       "      <td>401.596755</td>\n",
       "      <td>2.127171e+04</td>\n",
       "      <td>23.403374</td>\n",
       "      <td>4.274418e+04</td>\n",
       "      <td>5.833164e+05</td>\n",
       "      <td>10430835</td>\n",
       "      <td>0</td>\n",
       "      <td>119384314</td>\n",
       "      <td>1.558542e+05</td>\n",
       "      <td>1.106494e+06</td>\n",
       "      <td>10431320</td>\n",
       "      <td>39</td>\n",
       "      <td>119384354</td>\n",
       "      <td>58926.13722</td>\n",
       "      <td>685064.8872</td>\n",
       "      <td>10431069</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15340</td>\n",
       "      <td>40540</td>\n",
       "      <td>6.424620</td>\n",
       "      <td>16.978754</td>\n",
       "      <td>0</td>\n",
       "      <td>1420</td>\n",
       "      <td>908.785331</td>\n",
       "      <td>606.092512</td>\n",
       "      <td>367348.133600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1032</td>\n",
       "      <td>2794</td>\n",
       "      <td>2</td>\n",
       "      <td>909.110594</td>\n",
       "      <td>118.228162</td>\n",
       "      <td>1208.106068</td>\n",
       "      <td>336</td>\n",
       "      <td>150679</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>876</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>166</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.683488e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>119754701</td>\n",
       "      <td>1021</td>\n",
       "      <td>2537</td>\n",
       "      <td>171290</td>\n",
       "      <td>2922270</td>\n",
       "      <td>1460</td>\n",
       "      <td>0</td>\n",
       "      <td>167.766895</td>\n",
       "      <td>280.092409</td>\n",
       "      <td>1420</td>\n",
       "      <td>0</td>\n",
       "      <td>1151.860465</td>\n",
       "      <td>464.838481</td>\n",
       "      <td>2.583247e+04</td>\n",
       "      <td>29.710733</td>\n",
       "      <td>3.366733e+04</td>\n",
       "      <td>4.437175e+05</td>\n",
       "      <td>10951164</td>\n",
       "      <td>0</td>\n",
       "      <td>119754701</td>\n",
       "      <td>1.174066e+05</td>\n",
       "      <td>8.231362e+05</td>\n",
       "      <td>10951327</td>\n",
       "      <td>22</td>\n",
       "      <td>119754258</td>\n",
       "      <td>47221.71057</td>\n",
       "      <td>525618.1358</td>\n",
       "      <td>10951341</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20420</td>\n",
       "      <td>50740</td>\n",
       "      <td>8.525761</td>\n",
       "      <td>21.184972</td>\n",
       "      <td>0</td>\n",
       "      <td>1460</td>\n",
       "      <td>869.374262</td>\n",
       "      <td>612.115804</td>\n",
       "      <td>374685.758100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1271</td>\n",
       "      <td>3558</td>\n",
       "      <td>2</td>\n",
       "      <td>869.618606</td>\n",
       "      <td>167.766895</td>\n",
       "      <td>1151.860465</td>\n",
       "      <td>421</td>\n",
       "      <td>94293</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>821</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>292</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.558961e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>118908117</td>\n",
       "      <td>683</td>\n",
       "      <td>1662</td>\n",
       "      <td>77649</td>\n",
       "      <td>2001627</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>113.688141</td>\n",
       "      <td>221.086482</td>\n",
       "      <td>1420</td>\n",
       "      <td>0</td>\n",
       "      <td>1204.348375</td>\n",
       "      <td>411.993028</td>\n",
       "      <td>1.748641e+04</td>\n",
       "      <td>19.721110</td>\n",
       "      <td>5.072872e+04</td>\n",
       "      <td>6.356834e+05</td>\n",
       "      <td>11346294</td>\n",
       "      <td>0</td>\n",
       "      <td>118908117</td>\n",
       "      <td>1.743521e+05</td>\n",
       "      <td>1.170126e+06</td>\n",
       "      <td>11346697</td>\n",
       "      <td>20</td>\n",
       "      <td>118907832</td>\n",
       "      <td>71588.09874</td>\n",
       "      <td>755418.0467</td>\n",
       "      <td>11346612</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13660</td>\n",
       "      <td>33240</td>\n",
       "      <td>5.743931</td>\n",
       "      <td>13.977179</td>\n",
       "      <td>0</td>\n",
       "      <td>1420</td>\n",
       "      <td>886.538363</td>\n",
       "      <td>616.479582</td>\n",
       "      <td>380047.075100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>2345</td>\n",
       "      <td>2</td>\n",
       "      <td>886.916418</td>\n",
       "      <td>113.688141</td>\n",
       "      <td>1204.348375</td>\n",
       "      <td>275</td>\n",
       "      <td>168275</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>853</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>142</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>3.301057e+07</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>739728</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>383.958982</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>383.958982</td>\n",
       "      <td>1.468107e+03</td>\n",
       "      <td>5.407393</td>\n",
       "      <td>2.465760e+05</td>\n",
       "      <td>4.269072e+05</td>\n",
       "      <td>739526</td>\n",
       "      <td>55</td>\n",
       "      <td>739728</td>\n",
       "      <td>7.397280e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>739728</td>\n",
       "      <td>739728</td>\n",
       "      <td>739526</td>\n",
       "      <td>739526.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>739526</td>\n",
       "      <td>739526</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>2.703696</td>\n",
       "      <td>2.703696</td>\n",
       "      <td>0</td>\n",
       "      <td>543</td>\n",
       "      <td>325.800000</td>\n",
       "      <td>297.413349</td>\n",
       "      <td>88454.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>407.250000</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>271.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>41180</td>\n",
       "      <td>65535</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>5.227810e+05</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>149270</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>565</td>\n",
       "      <td>565</td>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>399.515331</td>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>399.515331</td>\n",
       "      <td>7.570175e+03</td>\n",
       "      <td>26.797079</td>\n",
       "      <td>4.975667e+04</td>\n",
       "      <td>8.598711e+04</td>\n",
       "      <td>149046</td>\n",
       "      <td>43</td>\n",
       "      <td>149270</td>\n",
       "      <td>1.492700e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>149270</td>\n",
       "      <td>149270</td>\n",
       "      <td>149046</td>\n",
       "      <td>149046.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>149046</td>\n",
       "      <td>149046</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>13.398540</td>\n",
       "      <td>13.398540</td>\n",
       "      <td>0</td>\n",
       "      <td>565</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>309.463245</td>\n",
       "      <td>95767.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>423.750000</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "      <td>41180</td>\n",
       "      <td>65535</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.052330e+05</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>1.456330e+15</td>\n",
       "      <td>Tor</td>\n",
       "      <td>audio-streaming</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162302</th>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65.747847</td>\n",
       "      <td>0.027957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.537465e+06</td>\n",
       "      <td>47316.981442</td>\n",
       "      <td>4.610032e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>6</td>\n",
       "      <td>6.721351e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>23891.063437</td>\n",
       "      <td>23425.918005</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>45.584721</td>\n",
       "      <td>34.957799</td>\n",
       "      <td>1270.705250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68.377081</td>\n",
       "      <td>65.747847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>VPN</td>\n",
       "      <td>file-transfer</td>\n",
       "      <td>SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162303</th>\n",
       "      <td>17</td>\n",
       "      <td>410987</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.070592e+02</td>\n",
       "      <td>4.866327</td>\n",
       "      <td>4.109875e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>410987</td>\n",
       "      <td>410987</td>\n",
       "      <td>410987</td>\n",
       "      <td>4.109875e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>410987</td>\n",
       "      <td>410987</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4.866327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>VPN</td>\n",
       "      <td>voip</td>\n",
       "      <td>SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162304</th>\n",
       "      <td>17</td>\n",
       "      <td>698</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>185</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.123209e+05</td>\n",
       "      <td>2865.329513</td>\n",
       "      <td>6.980000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>698</td>\n",
       "      <td>698</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1432.664756</td>\n",
       "      <td>1432.664756</td>\n",
       "      <td>33</td>\n",
       "      <td>185</td>\n",
       "      <td>83.666667</td>\n",
       "      <td>87.757241</td>\n",
       "      <td>7701.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>VPN</td>\n",
       "      <td>audio-streaming</td>\n",
       "      <td>SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162305</th>\n",
       "      <td>17</td>\n",
       "      <td>664</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>213</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>213</td>\n",
       "      <td>213</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.765060e+05</td>\n",
       "      <td>3012.048193</td>\n",
       "      <td>6.640000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>664</td>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1506.024096</td>\n",
       "      <td>1506.024096</td>\n",
       "      <td>37</td>\n",
       "      <td>213</td>\n",
       "      <td>95.666667</td>\n",
       "      <td>101.613647</td>\n",
       "      <td>10325.333330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>VPN</td>\n",
       "      <td>audio-streaming</td>\n",
       "      <td>SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162306</th>\n",
       "      <td>6</td>\n",
       "      <td>6339239</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.650160e+01</td>\n",
       "      <td>0.946486</td>\n",
       "      <td>1.267848e+06</td>\n",
       "      <td>1.220148e+06</td>\n",
       "      <td>3213124</td>\n",
       "      <td>205880</td>\n",
       "      <td>6339239</td>\n",
       "      <td>1.267848e+06</td>\n",
       "      <td>1.220148e+06</td>\n",
       "      <td>3213124</td>\n",
       "      <td>205880</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.666667</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1.433510e+15</td>\n",
       "      <td>1.882656e+06</td>\n",
       "      <td>1.433510e+15</td>\n",
       "      <td>1.433510e+15</td>\n",
       "      <td>VPN</td>\n",
       "      <td>file-transfer</td>\n",
       "      <td>SMOTE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2650467 rows  65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Protocol  Flow Duration  Total Fwd Packet  Total Bwd packets  \\\n",
       "0              6      119384497               767               2027   \n",
       "1              6      119754701              1021               2537   \n",
       "2              6      118908117               683               1662   \n",
       "3              6         739728                 2                  2   \n",
       "4              6         149270                 2                  2   \n",
       "...          ...            ...               ...                ...   \n",
       "162302         6             46                 1                  0   \n",
       "162303        17         410987                 2                  0   \n",
       "162304        17            698                 1                  1   \n",
       "162305        17            664                 1                  1   \n",
       "162306         6        6339239                 6                  0   \n",
       "\n",
       "        Total Length of Fwd Packet  Total Length of Bwd Packet  \\\n",
       "0                            90681                     2448831   \n",
       "1                           171290                     2922270   \n",
       "2                            77649                     2001627   \n",
       "3                              543                         543   \n",
       "4                              565                         565   \n",
       "...                            ...                         ...   \n",
       "162302                          70                           0   \n",
       "162303                          44                           0   \n",
       "162304                          33                         185   \n",
       "162305                          37                         213   \n",
       "162306                         168                           0   \n",
       "\n",
       "        Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
       "0                         543                      0              118.228162   \n",
       "1                        1460                      0              167.766895   \n",
       "2                         543                      0              113.688141   \n",
       "3                         543                      0              271.500000   \n",
       "4                         565                      0              282.500000   \n",
       "...                       ...                    ...                     ...   \n",
       "162302                     65                     65               65.747847   \n",
       "162303                     22                     22               22.000000   \n",
       "162304                     33                     33               33.000000   \n",
       "162305                     37                     37               37.000000   \n",
       "162306                     28                     28               28.000000   \n",
       "\n",
       "        Fwd Packet Length Std  Bwd Packet Length Max  Bwd Packet Length Min  \\\n",
       "0                  224.244409                   1420                      0   \n",
       "1                  280.092409                   1420                      0   \n",
       "2                  221.086482                   1420                      0   \n",
       "3                  383.958982                    543                      0   \n",
       "4                  399.515331                    565                      0   \n",
       "...                       ...                    ...                    ...   \n",
       "162302               0.027957                      0                      0   \n",
       "162303               0.000000                      0                      0   \n",
       "162304               0.000000                    185                    185   \n",
       "162305               0.000000                    213                    213   \n",
       "162306               0.000000                      0                      0   \n",
       "\n",
       "        Bwd Packet Length Mean  Bwd Packet Length Std  Flow Bytes/s  \\\n",
       "0                  1208.106068             401.596755  2.127171e+04   \n",
       "1                  1151.860465             464.838481  2.583247e+04   \n",
       "2                  1204.348375             411.993028  1.748641e+04   \n",
       "3                   271.500000             383.958982  1.468107e+03   \n",
       "4                   282.500000             399.515331  7.570175e+03   \n",
       "...                        ...                    ...           ...   \n",
       "162302                0.000000               0.000000  1.537465e+06   \n",
       "162303                0.000000               0.000000  1.070592e+02   \n",
       "162304              185.000000               0.000000  3.123209e+05   \n",
       "162305              213.000000               0.000000  3.765060e+05   \n",
       "162306                0.000000               0.000000  2.650160e+01   \n",
       "\n",
       "        Flow Packets/s  Flow IAT Mean  Flow IAT Std  Flow IAT Max  \\\n",
       "0            23.403374   4.274418e+04  5.833164e+05      10430835   \n",
       "1            29.710733   3.366733e+04  4.437175e+05      10951164   \n",
       "2            19.721110   5.072872e+04  6.356834e+05      11346294   \n",
       "3             5.407393   2.465760e+05  4.269072e+05        739526   \n",
       "4            26.797079   4.975667e+04  8.598711e+04        149046   \n",
       "...                ...            ...           ...           ...   \n",
       "162302    47316.981442   4.610032e+01  0.000000e+00            46   \n",
       "162303        4.866327   4.109875e+05  0.000000e+00        410987   \n",
       "162304     2865.329513   6.980000e+02  0.000000e+00           698   \n",
       "162305     3012.048193   6.640000e+02  0.000000e+00           664   \n",
       "162306        0.946486   1.267848e+06  1.220148e+06       3213124   \n",
       "\n",
       "        Flow IAT Min  Fwd IAT Total  Fwd IAT Mean   Fwd IAT Std  Fwd IAT Max  \\\n",
       "0                  0      119384314  1.558542e+05  1.106494e+06     10431320   \n",
       "1                  0      119754701  1.174066e+05  8.231362e+05     10951327   \n",
       "2                  0      118908117  1.743521e+05  1.170126e+06     11346697   \n",
       "3                 55         739728  7.397280e+05  0.000000e+00       739728   \n",
       "4                 43         149270  1.492700e+05  0.000000e+00       149270   \n",
       "...              ...            ...           ...           ...          ...   \n",
       "162302            46              6  6.721351e+00  0.000000e+00            6   \n",
       "162303        410987         410987  4.109875e+05  0.000000e+00       410987   \n",
       "162304           698              0  0.000000e+00  0.000000e+00            0   \n",
       "162305           664              0  0.000000e+00  0.000000e+00            0   \n",
       "162306        205880        6339239  1.267848e+06  1.220148e+06      3213124   \n",
       "\n",
       "        Fwd IAT Min  Bwd IAT Total  Bwd IAT Mean  Bwd IAT Std  Bwd IAT Max  \\\n",
       "0                39      119384354   58926.13722  685064.8872     10431069   \n",
       "1                22      119754258   47221.71057  525618.1358     10951341   \n",
       "2                20      118907832   71588.09874  755418.0467     11346612   \n",
       "3            739728         739526  739526.00000       0.0000       739526   \n",
       "4            149270         149046  149046.00000       0.0000       149046   \n",
       "...             ...            ...           ...          ...          ...   \n",
       "162302            6              0       0.00000       0.0000            0   \n",
       "162303       410987              0       0.00000       0.0000            0   \n",
       "162304            0              0       0.00000       0.0000            0   \n",
       "162305            0              0       0.00000       0.0000            0   \n",
       "162306       205880              0       0.00000       0.0000            0   \n",
       "\n",
       "        Bwd IAT Min  Fwd PSH Flags  Fwd Header Length  Bwd Header Length  \\\n",
       "0                 0              1              15340              40540   \n",
       "1                 3              1              20420              50740   \n",
       "2                 0              1              13660              33240   \n",
       "3            739526              1                 40                 40   \n",
       "4            149046              1                 40                 40   \n",
       "...             ...            ...                ...                ...   \n",
       "162302            0              0                 31                 30   \n",
       "162303            0              0                 16                  0   \n",
       "162304            0              0                  8                  8   \n",
       "162305            0              0                  8                  8   \n",
       "162306            0              0                 48                  0   \n",
       "\n",
       "        Fwd Packets/s  Bwd Packets/s  Packet Length Min  Packet Length Max  \\\n",
       "0            6.424620      16.978754                  0               1420   \n",
       "1            8.525761      21.184972                  0               1460   \n",
       "2            5.743931      13.977179                  0               1420   \n",
       "3            2.703696       2.703696                  0                543   \n",
       "4           13.398540      13.398540                  0                565   \n",
       "...               ...            ...                ...                ...   \n",
       "162302   23891.063437   23425.918005                  5                 65   \n",
       "162303       4.866327       0.000000                 22                 22   \n",
       "162304    1432.664756    1432.664756                 33                185   \n",
       "162305    1506.024096    1506.024096                 37                213   \n",
       "162306       0.946486       0.000000                 28                 28   \n",
       "\n",
       "        Packet Length Mean  Packet Length Std  Packet Length Variance  \\\n",
       "0               908.785331         606.092512           367348.133600   \n",
       "1               869.374262         612.115804           374685.758100   \n",
       "2               886.538363         616.479582           380047.075100   \n",
       "3               325.800000         297.413349            88454.700000   \n",
       "4               339.000000         309.463245            95767.500000   \n",
       "...                    ...                ...                     ...   \n",
       "162302           45.584721          34.957799             1270.705250   \n",
       "162303           22.000000           0.000000                0.000000   \n",
       "162304           83.666667          87.757241             7701.333333   \n",
       "162305           95.666667         101.613647            10325.333330   \n",
       "162306           28.000000           0.000000                0.000000   \n",
       "\n",
       "        FIN Flag Count  SYN Flag Count  RST Flag Count  PSH Flag Count  \\\n",
       "0                    0               0               0            1032   \n",
       "1                    0               0               0            1271   \n",
       "2                    0               0               0             800   \n",
       "3                    0               0               0               2   \n",
       "4                    0               0               0               2   \n",
       "...                ...             ...             ...             ...   \n",
       "162302               0               0               0               0   \n",
       "162303               0               0               0               0   \n",
       "162304               0               0               0               0   \n",
       "162305               0               0               0               0   \n",
       "162306               0               0               0               0   \n",
       "\n",
       "        ACK Flag Count  Down/Up Ratio  Average Packet Size  \\\n",
       "0                 2794              2           909.110594   \n",
       "1                 3558              2           869.618606   \n",
       "2                 2345              2           886.916418   \n",
       "3                    4              1           407.250000   \n",
       "4                    4              1           423.750000   \n",
       "...                ...            ...                  ...   \n",
       "162302               1              0            68.377081   \n",
       "162303               0              0            33.000000   \n",
       "162304               0              1           125.500000   \n",
       "162305               0              1           143.500000   \n",
       "162306               0              0            32.666667   \n",
       "\n",
       "        Fwd Segment Size Avg  Bwd Segment Size Avg  Bwd Packet/Bulk Avg  \\\n",
       "0                 118.228162           1208.106068                  336   \n",
       "1                 167.766895           1151.860465                  421   \n",
       "2                 113.688141           1204.348375                  275   \n",
       "3                 271.500000            271.500000                    0   \n",
       "4                 282.500000            282.500000                    0   \n",
       "...                      ...                   ...                  ...   \n",
       "162302             65.747847              0.000000                    0   \n",
       "162303             22.000000              0.000000                    0   \n",
       "162304             33.000000            185.000000                    0   \n",
       "162305             37.000000            213.000000                    0   \n",
       "162306             28.000000              0.000000                    4   \n",
       "\n",
       "        Bwd Bulk Rate Avg  Subflow Fwd Packets  Subflow Fwd Bytes  \\\n",
       "0                  150679                    0                 32   \n",
       "1                   94293                    0                 48   \n",
       "2                  168275                    0                 33   \n",
       "3                       0                    0                135   \n",
       "4                       0                    0                141   \n",
       "...                   ...                  ...                ...   \n",
       "162302                  0                    0                 34   \n",
       "162303                  0                    1                 22   \n",
       "162304                  0                    0                 16   \n",
       "162305                  0                    0                 18   \n",
       "162306                 76                    1                 28   \n",
       "\n",
       "        Subflow Bwd Bytes  FWD Init Win Bytes  Bwd Init Win Bytes  \\\n",
       "0                     876               65535               65535   \n",
       "1                     821               65535               65535   \n",
       "2                     853               65535               65535   \n",
       "3                     135               41180               65535   \n",
       "4                     141               41180               65535   \n",
       "...                   ...                 ...                 ...   \n",
       "162302                  0                 343                 343   \n",
       "162303                  0                   0                   0   \n",
       "162304                 92                   0                   0   \n",
       "162305                106                   0                   0   \n",
       "162306                  0                   0                   0   \n",
       "\n",
       "        Fwd Act Data Pkts  Fwd Seg Size Min     Idle Mean      Idle Std  \\\n",
       "0                     166                20  1.456330e+15  3.683488e+07   \n",
       "1                     292                20  1.456330e+15  3.558961e+07   \n",
       "2                     142                20  1.456330e+15  3.301057e+07   \n",
       "3                       0                20  1.456330e+15  5.227810e+05   \n",
       "4                       0                20  1.456330e+15  1.052330e+05   \n",
       "...                   ...               ...           ...           ...   \n",
       "162302                  0                31  0.000000e+00  0.000000e+00   \n",
       "162303                  1                 8  0.000000e+00  0.000000e+00   \n",
       "162304                  0                 8  0.000000e+00  0.000000e+00   \n",
       "162305                  0                 8  0.000000e+00  0.000000e+00   \n",
       "162306                  5                 8  1.433510e+15  1.882656e+06   \n",
       "\n",
       "            Idle Max      Idle Min Traffic Type Application Type Data Source  \n",
       "0       1.456330e+15  1.456330e+15          Tor  audio-streaming        Real  \n",
       "1       1.456330e+15  1.456330e+15          Tor  audio-streaming        Real  \n",
       "2       1.456330e+15  1.456330e+15          Tor  audio-streaming        Real  \n",
       "3       1.456330e+15  1.456330e+15          Tor  audio-streaming        Real  \n",
       "4       1.456330e+15  1.456330e+15          Tor  audio-streaming        Real  \n",
       "...              ...           ...          ...              ...         ...  \n",
       "162302  0.000000e+00  0.000000e+00          VPN    file-transfer       SMOTE  \n",
       "162303  0.000000e+00  0.000000e+00          VPN             voip       SMOTE  \n",
       "162304  0.000000e+00  0.000000e+00          VPN  audio-streaming       SMOTE  \n",
       "162305  0.000000e+00  0.000000e+00          VPN  audio-streaming       SMOTE  \n",
       "162306  1.433510e+15  1.433510e+15          VPN    file-transfer       SMOTE  \n",
       "\n",
       "[2650467 rows x 65 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1728x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 6), nrows=1, ncols=3)\n",
    "final_df.groupby('Traffic Type').size().plot(kind='pie', ax=ax[0], cmap='Blues')\n",
    "final_df.groupby('Application Type').size().plot(kind='pie', ax=ax[1], cmap='Blues')\n",
    "final_df.groupby('Data Source').size().plot(kind='pie', ax=ax[2], cmap='Blues')\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data Source\n",
       "CTGAN        700000\n",
       "CopulaGAN    700000\n",
       "Real         117620\n",
       "SMOTE        432847\n",
       "VAE          700000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('Data Source').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(final_df.groupby('Application Type').size().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 65)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dddf = pd.DataFrame()\n",
    "for c in classes:\n",
    "    dddf = dddf.append(final_df[final_df['Application Type'] == c].sample(1000))\n",
    "\n",
    "dddf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traffic Type\n",
       "Regular    1146331\n",
       "Tor         454789\n",
       "VPN        1049347\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('Traffic Type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2650467, 65)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = 14\n",
    "sum([i for i in range(output_shape + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metrics.tabular import MulticlassDecisionTreeClassifier, CSTest, KSTest, ContinuousKLDivergence, BNLikelihood, GMLogLikelihood, LogisticDetection, MulticlassMLPClassifier, BNLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pomegranate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the Baseline dataset\n"
     ]
    }
   ],
   "source": [
    "evals = {}\n",
    "\n",
    "print(f'Evaluating the Baseline dataset')\n",
    "evals['Baseline'] = {}\n",
    "evals['Baseline']['BNLikelihood'] = BNLikelihood.compute(base_dataset['Dataset'], base_dataset['Dataset'])\n",
    "evals['Baseline']['BNLogLikelihood'] = BNLogLikelihood.compute(base_dataset['Dataset'], base_dataset['Dataset'])\n",
    "evals['Baseline']['GMLogLikelihood'] = GMLogLikelihood.compute(base_dataset['Dataset'], base_dataset['Dataset'])\n",
    "evals['Baseline']['LogisticDetection'] = LogisticDetection.compute(base_dataset['Dataset'], base_dataset['Dataset'])\n",
    "evals['Baseline']['CSTest'] = CSTest.compute(base_dataset['Dataset'], base_dataset['Dataset'])\n",
    "evals['Baseline']['KSTest'] = KSTest.compute(base_dataset['Dataset'], base_dataset['Dataset'])\n",
    "evals['Baseline']['MLP'] = MulticlassMLPClassifier.compute(base_dataset['Dataset'], base_dataset['Dataset'], target = 'Application Type')\n",
    "evals['Baseline']['DT'] = MulticlassDecisionTreeClassifier.compute(base_dataset['Dataset'], base_dataset['Dataset'], target = 'Application Type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = list(synthetic_datasets_1.keys())\n",
    "\n",
    "for i, name in enumerate(dataset_names):\n",
    "    print(f'Evaluating the {name} dataset')\n",
    "    evals[name] = {}\n",
    "    evals[name]['BNLikelihood'] = BNLikelihood.compute(base_dataset['Dataset'], synthetic_datasets_1[name])\n",
    "    evals[name]['BNLogLikelihood'] = BNLogLikelihood.compute(base_dataset['Dataset'], synthetic_datasets_1[name])\n",
    "    evals[name]['GMLogLikelihood'] = GMLogLikelihood.compute(base_dataset['Dataset'], synthetic_datasets_1[name])\n",
    "    evals[name]['LogisticDetection'] = LogisticDetection.compute(base_dataset['Dataset'], synthetic_datasets_1[name])\n",
    "    evals[name]['CSTest'] = CSTest.compute(base_dataset['Dataset'], synthetic_datasets_1[name])\n",
    "    evals[name]['KSTest'] = KSTest.compute(base_dataset['Dataset'], synthetic_datasets_1[name])\n",
    "    evals[name]['MLP'] = MulticlassMLPClassifier.compute(base_dataset['Dataset'], synthetic_datasets_1[name], target = 'Application Type')\n",
    "    evals[name]['DT'] = MulticlassDecisionTreeClassifier.compute(base_dataset['Dataset'], synthetic_datasets_1[name], target = 'Application Type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Baseline'] + dataset_names\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for i, name in enumerate(dataset_names):\n",
    "    row = [name]\n",
    "    for key in evals[name].keys():\n",
    "        row.append(evals[name][key])\n",
    "    results = results.append([row], ignore_index=True)\n",
    "\n",
    "cols = ['Dataset'] + list(evals[dataset_names[0]].keys())\n",
    "results.columns = cols\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 6), nrows=1, ncols=3)\n",
    "results.plot(kind='bar',  x='Dataset', y=['BNLikelihood', 'LogisticDetection', 'CSTest', 'KSTest', 'MLP', 'DT'], ax=ax[0], cmap='Blues')\n",
    "results.plot(x='Dataset', y='BNLogLikelihood', kind='bar',  ax=ax[1], cmap='Blues')\n",
    "results.plot(x='Dataset', y='GMLogLikelihood', kind='bar',  ax=ax[2], cmap='Blues')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot(kind='bar',  x='Dataset', y=['BNLikelihood', 'LogisticDetection', 'CSTest', 'KSTest', 'MLP', 'DT'], cmap='Blues', figsize=(20, 10))\n",
    "# results.plot(kind='bar',  x='Dataset', y=['BNLikelihood', 'LogisticDetection', 'CSTest', 'KSTest', 'MLP', 'DT'], ax=ax[0], cmap='Blues')\n",
    "fig, ax = plt.subplots(figsize=(20, 10), nrows=1, ncols=2)\n",
    "results.plot(x='Dataset', y='BNLogLikelihood', kind='bar',  ax=ax[0], cmap='Blues')\n",
    "results.plot(x='Dataset', y='GMLogLikelihood', kind='bar',  ax=ax[1], cmap='Blues')\n",
    "\n",
    "results.to_csv('./phase2/dataset_evaluation.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Last Execution: {datetime.datetime.now()}')\n",
    "assert False, 'Nothing after this point is included in the study'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
