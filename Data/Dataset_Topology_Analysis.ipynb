{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIC-Darknet2020 Dataset Topology Analysis\n",
    "\n",
    "Here we load data from the [CIC-Darknet2020](https://www.unb.ca/cic/datasets/darknet2020.html) dataset to explore using methods from the field of Topological Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Last Execution: 2022-03-06 21:59:39.609160\n",
      "    python:\t3.9.7\n",
      "\n",
      "    \tfastai:\t\t2.5.3\n",
      "    \tgiotto-tda:\t0.5.1\n",
      "    \tmatplotlib:\t3.5.1\n",
      "    \tnumpy:\t\t1.21.5\n",
      "    \tpandas:\t\t1.1.4\n",
      "    \tplotly:\t\t5.1.0\n",
      "    \tseaborn:\t\t0.11.2\n",
      "    \tsdv:\t\t\t0.13.1\n",
      "    \tsklearn:\t\t1.0.2\n",
      "    \tyellowbrick:\t1.3.post1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import colorsys, datetime, os, pathlib, platform, pprint, sys\n",
    "import fastai\n",
    "import gtda\n",
    "import ipywidgets as widgets\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import sdv\n",
    "import sklearn\n",
    "import yellowbrick as yb\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from fastai.tabular.data import TabularDataLoaders, TabularPandas, MultiCategoryBlock\n",
    "from fastai.tabular.all import FillMissing, Categorify, Normalize, tabular_learner, accuracy, ClassificationInterpretation, ShowGraphCallback, RandomSplitter, range_of\n",
    "\n",
    "from gtda.diagrams import PersistenceEntropy\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sdv.tabular import TVAE\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from yellowbrick.model_selection import CVScores, LearningCurve, ValidationCurve\n",
    "\n",
    "seed: int = 14\n",
    "\n",
    "# allow plots to be shown in the notebook\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "Model_data = namedtuple(\n",
    "    'model_data', \n",
    "    ['name', 'model', 'classes', 'X_train', 'y_train', 'X_test', 'y_test', 'to', 'dls', 'model_type']\n",
    ")\n",
    "\n",
    "Topology_data = namedtuple(\n",
    "    'topology_data',\n",
    "    ['title', 'persistence', 'fig', 'entropy']\n",
    ")\n",
    "\n",
    "# set up pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# set up pretty printer for easier data evaluation\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\n",
    "\n",
    "\n",
    "# print library and python versions for reproducibility\n",
    "print(\n",
    "    f'''\n",
    "    Last Execution: {datetime.datetime.now()}\n",
    "    python:\\t{platform.python_version()}\n",
    "\n",
    "    \\tfastai:\\t\\t{fastai.__version__}\n",
    "    \\tgiotto-tda:\\t{gtda.__version__}\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\n",
    "    \\tnumpy:\\t\\t{np.__version__}\n",
    "    \\tpandas:\\t\\t{pd.__version__}\n",
    "    \\tplotly:\\t\\t{py.__version__}\n",
    "    \\tseaborn:\\t\\t{sns.__version__}\n",
    "    \\tsdv:\\t\\t\\t{sdv.__version__}\n",
    "    \\tsklearn:\\t\\t{sklearn.__version__}\n",
    "    \\tyellowbrick:\\t{yb.__version__}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create some helper functions to load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(directory: str):\n",
    "    '''\n",
    "        Closure that will return a function. \n",
    "        Function will return the filepath to the directory given to the closure\n",
    "    '''\n",
    "\n",
    "    def func(file: str) -> str:\n",
    "        return os.path.join(directory, file)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filePath):\n",
    "    '''\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\n",
    "        Function will only work when filepath is a .csv file\n",
    "    '''\n",
    "\n",
    "    p = pathlib.Path(filePath)\n",
    "    filePathClean: str = str(p.parts[-1])\n",
    "    pickleDump: str = f'./cache/{filePathClean}.pickle'\n",
    "\n",
    "    print(f'Loading Dataset: {filePath}')\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\n",
    "\n",
    "\n",
    "    # check if data already exists within cache\n",
    "    if os.path.exists(pickleDump):\n",
    "        df = pd.read_pickle(pickleDump)\n",
    " \n",
    "    # if not, load data and clean it before caching it\n",
    "    else:\n",
    "        df = pd.read_csv(filePath, low_memory=True)\n",
    "        df.to_pickle(pickleDump)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def features_with_bad_values(df: pd.DataFrame, datasetName: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will scan the dataframe for features with Inf, NaN, or Zero values.\n",
    "        Returns a new dataframe describing the distribution of these values in the original dataframe\n",
    "    '''\n",
    "\n",
    "    # Inf and NaN values can take different forms so we screen for every one of them\n",
    "    invalid_values: list = [ np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan', 0 ]\n",
    "    infs          : list = [ np.inf, 'Infinity', 'inf' ]\n",
    "    NaNs          : list = [ np.nan, 'NaN', 'nan' ]\n",
    "\n",
    "    # We will collect stats on the dataset, specifically how many instances of Infs, NaNs, and 0s are present.\n",
    "    # using a dictionary that will be converted into a (3, 2+88) dataframe\n",
    "    stats: dict = {\n",
    "        'Dataset':[ datasetName, datasetName, datasetName ],\n",
    "        'Value'  :['Inf', 'NaN', 'Zero']\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "    for col in df.columns:\n",
    "        \n",
    "        i += 1\n",
    "        feature = np.zeros(3)\n",
    "        \n",
    "        for value in invalid_values:\n",
    "            if value in infs:\n",
    "                j = 0\n",
    "            elif value in NaNs:\n",
    "                j = 1\n",
    "            else:\n",
    "                j = 2\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                feature[j] += len(indexNames)\n",
    "                \n",
    "        stats[col] = feature\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will take a dataframe and remove the columns that match a value in prune \n",
    "        Inf and Nan values will also be removed once appropriate rows and columns \n",
    "        have been removed, \n",
    "        we will return the dataframe with the appropriate values\n",
    "    '''\n",
    "\n",
    "    # remove the features in the prune list    \n",
    "    for col in prune:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    \n",
    "    # drop missing values/NaN etc.\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Search through dataframe for any Infinite or NaN values in various forms that were not picked up previously\n",
    "    invalid_values: list = [\n",
    "        np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan'\n",
    "    ]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for value in invalid_values:\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                print(f'deleting {len(indexNames)} rows with Infinity in column {col}')\n",
    "                df.drop(indexNames, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "class SklearnWrapper(BaseEstimator):\n",
    "    '''\n",
    "        A wrapper for fastai learners for creating visualizations using yellowbrick\n",
    "        code sourced from: \n",
    "        forums.fast.ai/t/fastai-with-yellowbrics-how-to-get-roc-curves-more/79408\n",
    "    '''\n",
    "    _estimator_type = \"classifier\"\n",
    "        \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.classes_ = list(self.model.dls.y.unique())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "    \n",
    "    def get_new_preds(self, X):\n",
    "        new_to = self.model.dls.valid_ds.new(X)\n",
    "        new_to.conts = new_to.conts.astype(np.float32)\n",
    "        new_dl = self.model.dls.valid.new(new_to)\n",
    "        with self.model.no_bar():\n",
    "            preds,_,dec_preds = self.model.get_preds(dl=new_dl, with_decoded=True)\n",
    "        return (preds, dec_preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.get_new_preds(X)[0].numpy()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.get_new_preds(X)[1].numpy()\n",
    "\n",
    "\n",
    "\n",
    "def get_n_color_list(n: int, opacity=.8, luminosity=.5, saturation=.5) -> list:\n",
    "    '''\n",
    "        Function creates a list of n distinct colors, formatted for use in a plotly graph\n",
    "    '''\n",
    "\n",
    "    colors = []\n",
    "\n",
    "    for i in range(n):\n",
    "        color = colorsys.hls_to_rgb(\n",
    "                i / n,\n",
    "                luminosity,\n",
    "                saturation,\n",
    "        )\n",
    "        \n",
    "        colors.append(f'rgba({color[0] * 255}, {color[1] * 255}, {color[2] * 255}, {opacity})')\n",
    "\n",
    "    return colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(job_id: int) -> dict:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe of the job_id passed in as well as that dataframe's\n",
    "        feature stats, data composition, and file name.\n",
    "\n",
    "        This dictionary is expected as the input for all of the other helper functions\n",
    "    '''\n",
    "\n",
    "    job_id = job_id - 1  # adjusts for indexing while enumerating jobs from 1\n",
    "    print(f'Dataset {job_id+1}/{len(data_set)}: We now look at {file_set[job_id]}\\n\\n')\n",
    "\n",
    "    # Load the dataset\n",
    "    df: pd.DataFrame = load_data(file_set[job_id])\n",
    " \n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        File:\\t\\t\\t\\t{file_set[job_id]}  \n",
    "        Job Number:\\t\\t\\t{job_id+1}\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             file_set[job_id],\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, file_set[job_id]), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection(df: pd.DataFrame) -> dict:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "    Dataset statistics:\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             '',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, ''), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def package_data_for_inspection_with_label(df: pd.DataFrame, label: str) -> dict['File': str, 'Dataset': pd.DataFrame, 'Feature_stats': pd.DataFrame]:\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             f'{label}',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, f'{label}'),\n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def check_infs(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of Inf.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    return inf_df[inf_df[0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of NaN.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    return nan_df[nan_df[1] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "\n",
    "    return zero_df[zero_df[2] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold(data_summary: dict, threshold: int) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold_percentage(data_summary: dict, threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with all features with\n",
    "        a frequency of 0 values greater than the threshold\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    size: int = data_summary['Dataset'].shape[0]\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold*size]\n",
    "\n",
    "\n",
    "\n",
    "def remove_infs_and_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all inf and nan values removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, [])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def rename_columns(data_summary: dict, columns: list, new_names: list) -> dict:\n",
    "    '''\n",
    "        Function will return the data_summary dict with the names of the columns in the dataframe changed\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    for x, i in enumerate(columns):\n",
    "        df.rename(columns={i: new_names[x]}, inplace=True)\n",
    "\n",
    "    data_summary['Dataset'] = df\n",
    "\n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def rename_values_in_column(data_summary: dict, replace: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe with the names of the columns changed\n",
    "\n",
    "        replace: [('column', {'old_name': 'new_name', ...}), ...]\n",
    "    '''\n",
    "    length: int = len(replace)\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    for i in range(length):\n",
    "        df[replace[i][0]].replace(replace[i][1], inplace=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prune_dataset(data_summary: dict, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all the columns in the prune list removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset'].copy()\n",
    "    df = clean_data(df, prune)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prune_feature_by_values(df: pd.DataFrame, column: str, value: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function takes a dataframe, a column name, and a list of values and returns a dataframe\n",
    "        with all rows that do not have the values in the column removed\n",
    "    '''\n",
    "    new_df = pd.DataFrame()\n",
    "    for v in value:\n",
    "        new_df = new_df.append(df[df[column] == v].copy())\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def test_infs(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no inf values.\n",
    "    '''\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    assert inf_df[inf_df[0] != 0].shape[0] == 2, 'Dataset has inf values'\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_nans(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no NaN values\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    assert nan_df[nan_df[1] != 0].shape[0] == 2, 'Dataset has NaN values'\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned(data_summary: dict, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    pruned: bool = True\n",
    "\n",
    "    for col in prune:\n",
    "        if col in data_summary['Dataset'].columns:\n",
    "            pruned = False\n",
    "\n",
    "    assert pruned, 'Dataset has columns present in prune list'\n",
    "\n",
    "    return pruned\n",
    "\n",
    "\n",
    "\n",
    "def test_pruned_size(data_summary_original: dict, data_summary_pruned: dict, prune: list) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has none of the columns present in the prune list \n",
    "    '''\n",
    "\n",
    "    original_size: int = data_summary_original['Dataset'].shape[1]\n",
    "    pruned_size: int = data_summary_pruned['Dataset'].shape[1]\n",
    "    prune_list_size: int = len(prune)\n",
    "\n",
    "    assert original_size - prune_list_size == pruned_size, 'Dataset has columns present in prune list'\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify what datasets we want to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to scale to processing numerous datasets, even though we currently are only looking at one now\n",
    "data_path_1: str = './phase1/'   \n",
    "data_set_1: list = [\n",
    "    'Darknet_reduced_features.csv',\n",
    "]\n",
    "\n",
    "data_set: list   = data_set_1\n",
    "file_path_1      = get_file_path(data_path_1)\n",
    "file_set: list   = list(map(file_path_1, data_set_1))\n",
    "current_job: int = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create some functions to help run the experiments and interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_nn_experiment(df: pd.DataFrame, name: str, target_label: str, shape: tuple) -> tuple:\n",
    "    '''\n",
    "        Run binary classification on a given dataframe, saving the model as {name}.model\n",
    "        returns the 8-tuple with the following indicies:\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var: str = target_label\n",
    " \n",
    "\n",
    "\n",
    "    assert len(shape) == 2, 'Shape must be a tuple of length 2'\n",
    "\n",
    "    categorical_features: list = []\n",
    "    # categorical_features: list = ['Application Type']\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "    if 'Protocol' in df.columns:\n",
    "        categorical_features.append(\"Protocol\")\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]))\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    splits = RandomSplitter(valid_pct=0.2, seed=seed)(range_of(df))\n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "    # The dataframe is then converted into a fastai dataset\n",
    "    dls = to.dataloaders(bs=64)\n",
    "\n",
    "    # extract the name from the path\n",
    "    p = pathlib.Path(name)\n",
    "    name: str = str(p.parts[-1])\n",
    "\n",
    "    # Next, we set up, train, and save the deep neural network\n",
    "    model = tabular_learner(\n",
    "        dls, \n",
    "        layers=list(shape), \n",
    "        metrics=accuracy, \n",
    "        cbs=ShowGraphCallback,\n",
    "    )\n",
    "\n",
    "    model.fit_one_cycle(10)\n",
    "    model.save(f'{name}.model')\n",
    "\n",
    "    # We print the results of the training    \n",
    "    loss, acc = model.validate()\n",
    "    print('loss {}: accuracy: {:.2f}%'.format(loss, acc*100))\n",
    "\n",
    "    # A confusion matrix is created to help evaluate the results\n",
    "    interp = ClassificationInterpretation.from_learner(model)\n",
    "    interp.plot_confusion_matrix()\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "    # We wrap our model to make it look like a scikitlearn model\n",
    "    # for visualization using yellowbrick\n",
    "    wrapped_model = SklearnWrapper(model)\n",
    "\n",
    "    # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    classes = list(model.dls.vocab)\n",
    "    if len(classes) == 2:\n",
    "        wrapped_model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        wrapped_model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "    \n",
    "    wrapped_model._target_labels = dep_var\n",
    "    \n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # so that yellowbrick can use them to create various visualizations\n",
    "    model_data: Model_data = Model_data(name, wrapped_model, classes, X_train, y_train, X_test, y_test, to, dls, f'Deep_NN_{shape[0]}x{shape[1]}')\n",
    "\n",
    "\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "def run_knn_experiment(df: pd.DataFrame, name: str, target_label: str) -> tuple:\n",
    "    '''\n",
    "        Run binary classification using K-Nearest Neighbors\n",
    "        returns the 8-tuple with the following indicies:\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test, model_type)\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var: str = target_label\n",
    "    categorical_features: list = []\n",
    "\n",
    "\n",
    "    if 'Protocol' in df.columns:\n",
    "        categorical_features.append(\"Protocol\")\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]))\n",
    "\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    splits = RandomSplitter(valid_pct=0.2, seed=seed)(range_of(df))\n",
    "    \n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\n",
    "    dls = to.dataloaders(bs=64)\n",
    "    model = tabular_learner(dls)\n",
    "    classes : list = list(model.dls.vocab)\n",
    "\n",
    "\n",
    "    # extract the name from the path\n",
    "    p = pathlib.Path(name)\n",
    "    name: str = str(p.parts[-1])\n",
    "\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "    # Now that we have the train and test datasets, we set up a gridsearch of the K-NN classifier\n",
    "    # using SciKitLearn and print the results \n",
    "    # params = {\"n_neighbors\": range(1, 50)}\n",
    "    # model = GridSearchCV(KNeighborsClassifier(), params)\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    report = classification_report(y_test, prediction)\n",
    "    print(report)\n",
    "    print(f'\\tAccuracy: {accuracy_score(y_test, prediction)}\\n')\n",
    "    # print(\"Best Parameters found by gridsearch:\")\n",
    "    # print(model.best_params_)\n",
    "\n",
    "\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    if len(classes) == 2:\n",
    "        model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        print('Must be more than one class to perform classification')\n",
    "        raise ValueError('Wrong number of classes')\n",
    "\n",
    "    model_data: Model_data = Model_data(name, model, classes, X_train, y_train, X_test, y_test, to, dls, f'K_Nearest_Neighbors')\n",
    "\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # for analysis and further experimentation\n",
    "    return model_data\n",
    "\n",
    "\n",
    "\n",
    "def transform_and_split_data(df: pd.DataFrame, target_label: str, split=0.2, name='') -> tuple:\n",
    "    '''\n",
    "        Transform and split the data into a train and test set\n",
    "        returns the 10-tuple with the following indicies:\n",
    "        results: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    # First we split the features into the dependent variable and \n",
    "    # continous and categorical features\n",
    "    dep_var: str = target_label\n",
    "    categorical_features: list = []\n",
    "\n",
    "\n",
    "    if 'Protocol' in df.columns:\n",
    "        categorical_features.append(\"Protocol\")\n",
    "        \n",
    "    continuous_features = list(set(df) - set(categorical_features) - set([dep_var]))\n",
    "\n",
    "\n",
    "    # Next, we set up the feature engineering pipeline, namely filling missing values\n",
    "    # encoding categorical features, and normalizing the continuous features\n",
    "    # all within a pipeline to prevent the normalization from leaking details\n",
    "    # about the test sets through the normalized mapping of the training sets\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    splits = RandomSplitter(valid_pct=split, seed=seed)(range_of(df))\n",
    "    \n",
    "    \n",
    "    # The dataframe is loaded into a fastai datastructure now that \n",
    "    # the feature engineering pipeline has been set up\n",
    "    to = TabularPandas(\n",
    "        df            , y_names=dep_var                , \n",
    "        splits=splits , cat_names=categorical_features ,\n",
    "        procs=procs   , cont_names=continuous_features , \n",
    "    )\n",
    "\n",
    "\n",
    "    # We use fastai to quickly extract the names of the classes as they are mapped to the encodings\n",
    "    dls = to.dataloaders(bs=64)\n",
    "    model = tabular_learner(dls)\n",
    "    classes : list = list(model.dls.vocab)\n",
    "\n",
    "\n",
    "    # We extract the training and test datasets from the dataframe\n",
    "    X_train = to.train.xs.reset_index(drop=True)\n",
    "    X_test = to.valid.xs.reset_index(drop=True)\n",
    "    y_train = to.train.ys.values.ravel()\n",
    "    y_test = to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "   # we add a target_type_ attribute to our model so yellowbrick knows how to make the visualizations\n",
    "    if len(classes) == 2:\n",
    "        model.target_type_ = 'binary'\n",
    "    elif len(classes) > 2:  \n",
    "        model.target_type_ = 'multiclass'\n",
    "    else:\n",
    "        model.target_type_ = 'single'\n",
    "        \n",
    "    model_data: Model_data = Model_data(name, model, classes, X_train, y_train, X_test, y_test, to, dls, 'Transformed_and_Split_data')\n",
    "\n",
    "    # Now that the classifier has been created and trained, we pass out our training values\n",
    "    # for analysis and further experimentation\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clusters_by_label_1D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes the 10-tuple model_data from the transform and split function and uses Principal component analysis\n",
    "            to break up the feature dimensions into a single dimension for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=1)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\"]\n",
    "\n",
    "    new_X           = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X           = pd.DataFrame(np.array(new_X.sample(samples)))\n",
    "    new_X.columns   = X.columns\n",
    "    new_X[\"filler\"] = 0\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=cluster[\"component_1\"],\n",
    "                y=cluster[\"filler\"],\n",
    "                mode='markers',\n",
    "                marker = dict(color = label_colors[i]),\n",
    "                name=classes[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5,zeroline= False\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= '',\n",
    "            ticklen= 5,\n",
    "            zeroline= False\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into two dimension  for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=2)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\"]\n",
    "\n",
    "\n",
    "    new_X         = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(new_X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_3D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into three dimensions for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=3)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\"]\n",
    "\n",
    "\n",
    "    new_X         = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(new_X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                z      = cluster[\"component_3\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_4D_pca(model_data: tuple, samples=25000, title='Clusters by Label') -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses Principal component analysis\n",
    "            to break up the feature dimensions into four dimensions for cluster analysis.\n",
    "            Displays a 2 two-dimensional graphs of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    pca               = PCA(n_components=4)\n",
    "    component         = pd.DataFrame(pca.fit_transform(X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\", \"component_4\"]\n",
    "\n",
    "\n",
    "    new_X         = pd.concat([X, component], axis=1, join='inner')\n",
    "    new_X         = pd.DataFrame(np.array(new_X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    traces1: list = []\n",
    "    traces2: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces1.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "        traces2.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_3\"]    ,\n",
    "                y      = cluster[\"component_4\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout1 = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layout2 = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 3',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 4',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces1, 'layout': layout1})\n",
    "    iplot({'data': traces2, 'layout': layout2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clusters_by_label_1D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into a single dimension for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X           = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns   = X.columns\n",
    "    new_X[\"filler\"] = 0\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=1, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\"]\n",
    "    new_X             = pd.concat([X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=cluster[\"component_1\"],\n",
    "                y=cluster[\"filler\"],\n",
    "                mode='markers',\n",
    "                marker = dict(color = label_colors[i]),\n",
    "                name=classes[i],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5,zeroline= False\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= '',\n",
    "            ticklen= 5,\n",
    "            zeroline= False\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_2D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into two dimension  for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=2, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(columns=['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\"]\n",
    "    new_X             = pd.concat([new_X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title= 'Component 1',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title= 'Component 2',\n",
    "            ticklen= 5          ,\n",
    "            zeroline= False     ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})\n",
    "\n",
    "\n",
    "\n",
    "def show_clusters_by_label_3D_tsne(model_data: tuple, samples=25000, title='Clusters by Label', perplexity: int = 50) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses T-Distributed Stochastic Neighbor Embedding\n",
    "            to break up the feature dimensions into three dimensions for cluster analysis.\n",
    "            Displays a graph of the clusters colored by label\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    X            = model_data.X_train.copy()\n",
    "    X['label']   = model_data.y_train.copy()\n",
    "    classes      = model_data.classes\n",
    "    num_labels   = len(X.groupby('label').size())\n",
    "    label_colors = get_n_color_list(num_labels, opacity=.4)\n",
    "\n",
    "\n",
    "    new_X         = pd.DataFrame(np.array(X.sample(samples)))\n",
    "    new_X.columns = X.columns\n",
    "\n",
    "\n",
    "    tsne              = TSNE(n_components=3, perplexity=perplexity)\n",
    "    component         = pd.DataFrame(tsne.fit_transform(new_X.drop(['label'], axis=1)))\n",
    "    component.columns = [\"component_1\", \"component_2\", \"component_3\"]\n",
    "    new_X             = pd.concat([new_X, component], axis=1, join='inner')\n",
    "\n",
    "\n",
    "    traces: list = []\n",
    "    for i in range(num_labels):\n",
    "        cluster = new_X[new_X['label'] == i]\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x      = cluster[\"component_1\"]    ,\n",
    "                y      = cluster[\"component_2\"]    ,\n",
    "                z      = cluster[\"component_3\"]    ,\n",
    "                marker = {'color': label_colors[i]},\n",
    "                mode   = 'markers'                 ,\n",
    "                name   = classes[i]                ,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    layout = dict(\n",
    "        title = title,\n",
    "        xaxis= dict(\n",
    "            title    = 'Component 1',\n",
    "            ticklen  = 5            ,\n",
    "            zeroline = False        ,\n",
    "        ),\n",
    "        yaxis= dict(\n",
    "            title    = 'Component 2',\n",
    "            ticklen  = 5            ,\n",
    "            zeroline = False        ,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    iplot({'data': traces, 'layout': layout})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(model_data: tuple) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a confusion matrix\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ConfusionMatrix(model_data[1], classes=model_data[2], title=model_data[0])\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    visualizer.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_roc(model_data: tuple) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a \n",
    "        Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ROCAUC(model_data[1], classes=model_data[2], title=model_data[0])\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    visualizer.poof()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_pr_curve(model_data: tuple) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a \n",
    "        Precision-Recall Curve\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.PrecisionRecallCurve(model_data[1], title=model_data[0])\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    visualizer.poof()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_report(model_data: tuple) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a report\n",
    "        detailing the Precision, Recall, f1, and Support scores for all \n",
    "        classification outcomes\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.classifier.ClassificationReport(model_data[1], classes=model_data[2], title=model_data[0], support=True)\n",
    "    visualizer.score(model_data[5], model_data[6])\n",
    "    visualizer.poof()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_class_balance(model_data: tuple) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and creates a histogram\n",
    "        detailing the balance between classification outcomes\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    visualizer = yb.target.ClassBalance(labels=model_data[0])\n",
    "    visualizer.fit(model_data[4], model_data[6])\n",
    "    visualizer.show()\n",
    "\n",
    "\n",
    "\n",
    "def confusion_matrix_from_dataset(model_data: tuple, df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "        Takes a 10-tuple from the run_experiments function and uses the model to classify\n",
    "            the data passed in as the dataframe. Produces a confusion matrix\n",
    "            (currently only confirmed to work with fastai models)\n",
    "\n",
    "        model_data: tuple = (name, model, classes, X_train, y_train, X_test, y_test, to, dls, model_type)\n",
    "    '''\n",
    "\n",
    "    dl = model_data[1].model.dls.test_dl(df, bs=64)\n",
    "    preds, v, dec_preds = model_data[1].model.get_preds(dl=dl, with_decoded=True)\n",
    "\n",
    "    visualizer = yb.classifier.ConfusionMatrix(model_data[1], classes=model_data[2], title=model_data[0])\n",
    "    visualizer.score(dl.xs, dl.y)\n",
    "    visualizer.show()\n",
    "    acc = accuracy_score(dl.y, dec_preds)\n",
    "    print(f'Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be looking at 1 files:\n",
      "[   './phase1/Darknet_reduced_features.csv']\n"
     ]
    }
   ],
   "source": [
    "print(f'We will be looking at {len(file_set)} files:')\n",
    "pretty(file_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Analysis\n",
    "\n",
    "We load the data and separate the dataset by label, giving us a traffic dataset and an application dataset. We also want to investigate how merging the Non-Tor and NonVNP labels together affects the clustering, so rename the samples under these labels as regular and produce a second traffic dataset with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/1: We now look at ./phase1/Darknet_reduced_features.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./phase1/Darknet_reduced_features.csv\n",
      "\tTo Dataset Cache: ./cache/Darknet_reduced_features.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./phase1/Darknet_reduced_features.csv  \n",
      "        Job Number:\t\t\t1\n",
      "        Shape:\t\t\t\t(141481, 64)\n",
      "        Samples:\t\t\t141481 \n",
      "        Features:\t\t\t64\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(141481, 63)\n",
      "        Samples:\t\t\t141481 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(5000, 63)\n",
      "        Samples:\t\t\t5000 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(5000, 63)\n",
      "        Samples:\t\t\t5000 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(141481, 63)\n",
      "        Samples:\t\t\t141481 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(22919, 63)\n",
      "        Samples:\t\t\t22919 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(23861, 63)\n",
      "        Samples:\t\t\t23861 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(1392, 63)\n",
      "        Samples:\t\t\t1392 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(93309, 63)\n",
      "        Samples:\t\t\t93309 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(1300, 63)\n",
      "        Samples:\t\t\t1300 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(1300, 63)\n",
      "        Samples:\t\t\t1300 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(1300, 63)\n",
      "        Samples:\t\t\t1300 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(1300, 63)\n",
      "        Samples:\t\t\t1300 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(5000, 63)\n",
      "        Samples:\t\t\t5000 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(5000, 63)\n",
      "        Samples:\t\t\t5000 \n",
      "        Features:\t\t\t63\n",
      "    \n",
      "\n",
      "        Shape:\t\t\t\t(5000, 63)\n",
      "        Samples:\t\t\t5000 \n",
      "        Features:\t\t\t63\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dataset_1            : dict = examine_dataset(1)\n",
    "traffic_dataset_1    : dict = package_data_for_inspection_with_label(prune_dataset(dataset_1, ['Application Type'])  , 'Traffic_Dataset_1_Tor_VPN_Non_Tor_NonVPN_full')\n",
    "traffic_dataset_2    : dict = package_data_for_inspection_with_label(traffic_dataset_1['Dataset'].sample(5000).copy(), 'Traffic_Dataset_2_Tor_VPN_Non_Tor_NonVPN_5k')\n",
    "traffic_dataset_3    : dict = package_data_for_inspection_with_label(traffic_dataset_1['Dataset'].sample(5000).copy(), 'Traffic_Dataset_3_Tor_VPN_Non_Tor_NonVPN_5k')\n",
    "application_dataset_1: dict = package_data_for_inspection_with_label(prune_dataset(dataset_1, ['Traffic Type'])      , 'Application_Dataset_1')\n",
    "\n",
    "\n",
    "vpn_dataset_1        : dict = package_data_for_inspection_with_label(prune_feature_by_values(traffic_dataset_1['Dataset'].copy(), 'Traffic Type', ['VPN'])    , 'VPN_Dataset_1_full')\n",
    "non_vpn_dataset_1    : dict = package_data_for_inspection_with_label(prune_feature_by_values(traffic_dataset_1['Dataset'].copy(), 'Traffic Type', ['NonVPN']) , 'NonVPN_Dataset_1_full')\n",
    "tor_dataset_1        : dict = package_data_for_inspection_with_label(prune_feature_by_values(traffic_dataset_1['Dataset'].copy(), 'Traffic Type', ['Tor'])    , 'Tor_Dataset_1_full')\n",
    "non_tor_dataset_1    : dict = package_data_for_inspection_with_label(prune_feature_by_values(traffic_dataset_1['Dataset'].copy(), 'Traffic Type', ['Non-Tor']), 'Non_Tor_Dataset_1_full')\n",
    "\n",
    "\n",
    "vpn_dataset_2        : dict = package_data_for_inspection_with_label(vpn_dataset_1['Dataset'].sample(1300).copy()     , 'VPN_Dataset_2_1.3k')\n",
    "non_vpn_dataset_2    : dict = package_data_for_inspection_with_label(non_vpn_dataset_1['Dataset'].sample(1300).copy() , 'NonVPN_Dataset_2_1.3k')\n",
    "tor_dataset_2        : dict = package_data_for_inspection_with_label(tor_dataset_1['Dataset'].sample(1300).copy()     , 'Tor_Dataset_2_1.3k')\n",
    "non_tor_dataset_2    : dict = package_data_for_inspection_with_label(non_tor_dataset_1['Dataset'].sample(1300).copy() , 'Non_Tor_Dataset_2_1.3k')\n",
    "\n",
    "vpn_dataset_3        : dict = package_data_for_inspection_with_label(vpn_dataset_1['Dataset'].sample(5000).copy()    , 'VPN_Dataset_3_5k')\n",
    "non_vpn_dataset_3    : dict = package_data_for_inspection_with_label(non_vpn_dataset_1['Dataset'].sample(5000).copy(), 'NonVPN_Dataset_3_5k')\n",
    "non_tor_dataset_3    : dict = package_data_for_inspection_with_label(non_tor_dataset_1['Dataset'].sample(5000).copy(), 'Non_Tor_Dataset_3_5k')\n",
    "\n",
    "\n",
    "traffic_dataset_1['Data']     = transform_and_split_data(traffic_dataset_1['Dataset']    , 'Traffic Type')\n",
    "traffic_dataset_2['Data']     = transform_and_split_data(traffic_dataset_2['Dataset']    , 'Traffic Type', split=.5)\n",
    "traffic_dataset_3['Data']     = transform_and_split_data(traffic_dataset_3['Dataset']    , 'Traffic Type', split=.5)\n",
    "application_dataset_1['Data'] = transform_and_split_data(application_dataset_1['Dataset'], 'Application Type')\n",
    "\n",
    "\n",
    "vpn_dataset_2['Data']     = transform_and_split_data(vpn_dataset_2['Dataset']    , 'Traffic Type', split=0, name='VPN')\n",
    "non_vpn_dataset_2['Data'] = transform_and_split_data(non_vpn_dataset_2['Dataset'], 'Traffic Type', split=0, name='NonVPN')\n",
    "tor_dataset_2['Data']     = transform_and_split_data(tor_dataset_2['Dataset']    , 'Traffic Type', split=0, name='Tor')\n",
    "non_tor_dataset_2['Data'] = transform_and_split_data(non_tor_dataset_2['Dataset'], 'Traffic Type', split=0, name='NonTor')\n",
    "\n",
    "\n",
    "vpn_dataset_3['Data']     = transform_and_split_data(vpn_dataset_3['Dataset']    , 'Traffic Type', split=0, name='VPN')\n",
    "non_vpn_dataset_3['Data'] = transform_and_split_data(non_vpn_dataset_3['Dataset'], 'Traffic Type', split=0, name='NonVPN')\n",
    "non_tor_dataset_3['Data'] = transform_and_split_data(non_tor_dataset_3['Dataset'], 'Traffic Type', split=0, name='NonTor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run some models to get baseline results and to transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_point_clouds(n_samples_per_shape: int, n_points: int, noise: float):\n",
    "    \"\"\"Make point clouds for circles, spheres, and tori with random noise.\n",
    "    \"\"\"\n",
    "    circle_point_clouds = [\n",
    "        np.asarray(\n",
    "            [\n",
    "                [np.sin(t) + noise * (np.random.rand(1)[0] - 0.5), np.cos(t) + noise * (np.random.rand(1)[0] - 0.5), 0]\n",
    "                for t in range((n_points ** 2))\n",
    "            ]\n",
    "        )\n",
    "        for kk in range(n_samples_per_shape)\n",
    "    ]\n",
    "    # label circles with 0\n",
    "    circle_labels = np.zeros(n_samples_per_shape)\n",
    "\n",
    "    sphere_point_clouds = [\n",
    "        np.asarray(\n",
    "            [\n",
    "                [\n",
    "                    np.cos(s) * np.cos(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    np.cos(s) * np.sin(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    np.sin(s) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                ]\n",
    "                for t in range(n_points)\n",
    "                for s in range(n_points)\n",
    "            ]\n",
    "        )\n",
    "        for kk in range(n_samples_per_shape)\n",
    "    ]\n",
    "    # label spheres with 1\n",
    "    sphere_labels = np.ones(n_samples_per_shape)\n",
    "\n",
    "    torus_point_clouds = [\n",
    "        np.asarray(\n",
    "            [\n",
    "                [\n",
    "                    (2 + np.cos(s)) * np.cos(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    (2 + np.cos(s)) * np.sin(t) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                    np.sin(s) + noise * (np.random.rand(1)[0] - 0.5),\n",
    "                ]\n",
    "                for t in range(n_points)\n",
    "                for s in range(n_points)\n",
    "            ]\n",
    "        )\n",
    "        for kk in range(n_samples_per_shape)\n",
    "    ]\n",
    "    # label tori with 2\n",
    "    torus_labels = 2 * np.ones(n_samples_per_shape)\n",
    "\n",
    "    point_clouds = np.concatenate((circle_point_clouds, sphere_point_clouds, torus_point_clouds))\n",
    "    labels = np.concatenate((circle_labels, sphere_labels, torus_labels))\n",
    "\n",
    "    return point_clouds, labels\n",
    "\n",
    "\n",
    "def make_gravitational_waves(\n",
    "    path_to_data: pathlib.Path,\n",
    "    n_signals: int = 30,\n",
    "    downsample_factor: int = 2,\n",
    "    r_min: float = 0.075,\n",
    "    r_max: float = 0.65,\n",
    "    n_snr_values: int = 10,\n",
    "        ):\n",
    "    def padrand(V, n, kr):\n",
    "        cut = np.random.randint(n)\n",
    "        rand1 = np.random.randn(cut)\n",
    "        rand2 = np.random.randn(n - cut)\n",
    "        out = np.concatenate((rand1 * kr, V, rand2 * kr))\n",
    "        return out\n",
    "\n",
    "    Rcoef = np.linspace(r_min, r_max, n_snr_values)\n",
    "    Npad = 500  # number of padding points on either side of the vector\n",
    "    gw = np.load(path_to_data / \"gravitational_wave_signals.npy\")\n",
    "    Norig = len(gw[\"data\"][0])\n",
    "    Ndat = len(gw[\"signal_present\"])\n",
    "    N = int(Norig / downsample_factor)\n",
    "\n",
    "    ncoeff = []\n",
    "    Rcoeflist = []\n",
    "\n",
    "    for j in range(n_signals):\n",
    "        ncoeff.append(10 ** (-19) * (1 / Rcoef[j % n_snr_values]))\n",
    "        Rcoeflist.append(Rcoef[j % n_snr_values])\n",
    "\n",
    "    noisy_signals = []\n",
    "    gw_signals = []\n",
    "    k = 0\n",
    "    labels = np.zeros(n_signals)\n",
    "\n",
    "    for j in range(n_signals):\n",
    "        signal = gw[\"data\"][j % Ndat][range(0, Norig, downsample_factor)]\n",
    "        sigp = int((np.random.randn() < 0))\n",
    "        noise = ncoeff[j] * np.random.randn(N)\n",
    "        labels[j] = sigp\n",
    "        if sigp == 1:\n",
    "            rawsig = padrand(signal + noise, Npad, ncoeff[j])\n",
    "            if k == 0:\n",
    "                k = 1\n",
    "        else:\n",
    "            rawsig = padrand(noise, Npad, ncoeff[j])\n",
    "        noisy_signals.append(rawsig.copy())\n",
    "        gw_signals.append(signal)\n",
    "\n",
    "    return noisy_signals, gw_signals, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 point clouds in 3 dimensions, each with 100 points.\n"
     ]
    }
   ],
   "source": [
    "n_samples_per_class = 10\n",
    "point_clouds, labels = make_point_clouds(n_samples_per_class, 10, 0.1)\n",
    "point_clouds.shape\n",
    "print(f\"There are {point_clouds.shape[0]} point clouds in {point_clouds.shape[2]} dimensions, \"\n",
    "      f\"each with {point_clouds.shape[1]} points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud3 = np.concatenate(\n",
    "    (\n",
    "        np.array([vpn_dataset_2['Data'].X_train.values])    ,\n",
    "        np.array([vpn_dataset_2['Data'].X_test.values] )    ,\n",
    "        np.array([non_vpn_dataset_2['Data'].X_train.values]),\n",
    "        np.array([non_vpn_dataset_2['Data'].X_test.values]) ,\n",
    "        np.array([tor_dataset_2['Data'].X_train.values])    ,\n",
    "        np.array([tor_dataset_2['Data'].X_test.values])     ,\n",
    "        np.array([non_tor_dataset_2['Data'].X_train.values]),\n",
    "        np.array([non_tor_dataset_2['Data'].X_test.values]) ,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 650, 62)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_cloud3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "VR3 = VietorisRipsPersistence(homology_dimensions=[0,1,2])\n",
    "diagram3 = VR3.fit_transform(point_cloud3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 824, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagram3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model_data = namedtuple(\n",
    "#     'model_data', \n",
    "#     ['name', 'model', 'classes', 'X_train', 'y_train', 'X_test', 'y_test', 'to', 'dls', 'model_type']\n",
    "# )\n",
    "\n",
    "# Topology_data = namedtuple(\n",
    "#     'topology_data',\n",
    "#     ['title', 'persistence', 'fig', 'entropy']\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "def calculate_single_batch_dataset_topology(model_data: tuple, samples: int or None = 1000, dimensions: tuple = (0,1), title=''):\n",
    "    \"\"\"\n",
    "    Function calculates the topology of the dataset and returns it\n",
    "        requires data to be transformed by fastai first to properly encode and normalize the\n",
    "        data.\n",
    "\n",
    "    Returns a namedtuple with the following fields:\n",
    "        title: Graph title\n",
    "        persistence: Vietoris-Rips Peristence\n",
    "        fig: plotly figure\n",
    "        entropy: Persistence Entropy\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if(model_data.name != '' and title == ''):\n",
    "        title = model_data.name\n",
    "\n",
    "\n",
    "    if(samples is None):\n",
    "        X = model_data.X_train.values\n",
    "    else:\n",
    "        X = model_data.X_train.sample(samples).values\n",
    "        \n",
    "    X = np.array([X])\n",
    "\n",
    "    # calculate the topology\n",
    "    VR = VietorisRipsPersistence(homology_dimensions=dimensions)\n",
    "    topology = VR.fit_transform(X)\n",
    "\n",
    "    # calculate the persistence diagram\n",
    "    fig = plot_diagram(topology[0])\n",
    "\n",
    "    # calculate the entropy\n",
    "    PE = PersistenceEntropy()\n",
    "    entropy = PE.fit_transform(topology)\n",
    "\n",
    "    # package in topological_data named tuple. We do this so data can be accessed using [i] or . operators\n",
    "    topological_data: Topology_data = Topology_data(title, topology, fig, entropy)\n",
    "\n",
    "    return topological_data\n",
    "\n",
    "\n",
    "def show_stacked_graphs(figures: list, stack_shape: tuple = (1,1), title: str = '', individual_titles: list or bool = False):\n",
    "    \"\"\"\n",
    "    Function takes a list of figures and stacks them in a grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures: list of plotly figures\n",
    "        list of plotly figures to be stacked\n",
    "    stack_shape: tuple of ints\n",
    "        shape of the stack. Default is (1,1)\n",
    "    title: str\n",
    "        title of the graph\n",
    "    show: bool\n",
    "        whether to show the graph or not. Default is True\n",
    "\n",
    "    Returns a plotly figure with the inpute figures stacked as indicated\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(figures) == stack_shape[0] * stack_shape[1], \"Number of figures must match stack shape\"\n",
    "\n",
    "    vertical_stack  : list = []\n",
    "    horizantal_stack: list = []\n",
    "\n",
    "    if(title != ''):\n",
    "        vertical_stack.append(widgets.Label(title))\n",
    "    \n",
    "    for i in range(stack_shape[1]):\n",
    "        for j in range(stack_shape[0]):\n",
    "            horizantal_stack.append(go.FigureWidget(figures[i * stack_shape[0] + j].data))\n",
    "        vertical_stack.append(widgets.HBox(horizantal_stack))\n",
    "        horizantal_stack = []\n",
    "    fig = widgets.VBox(vertical_stack)\n",
    "\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_dataset_1['Topology'] = calculate_dataset_topology(traffic_dataset_1['Data'], samples=2000, dimensions=(0,1), title='Traffic Dataset 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topology_data(title='Traffic Dataset 1', persistence=array([[[0.00000000e+00, 8.42936956e-08, 0.00000000e+00],\n",
       "        [0.00000000e+00, 8.42936956e-08, 0.00000000e+00],\n",
       "        [0.00000000e+00, 1.19209290e-07, 0.00000000e+00],\n",
       "        ...,\n",
       "        [5.89706842e-03, 7.00550061e-03, 1.00000000e+00],\n",
       "        [5.62417274e-03, 6.91427849e-03, 1.00000000e+00],\n",
       "        [4.99571022e-03, 5.33307577e-03, 1.00000000e+00]]]), fig=Figure({\n",
       "    'data': [{'hoverinfo': 'none',\n",
       "              'line': {'color': 'black', 'dash': 'dash', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'showlegend': False,\n",
       "              'type': 'scatter',\n",
       "              'x': [-1.1201055908203126, 57.12538513183594],\n",
       "              'y': [-1.1201055908203126, 57.12538513183594]},\n",
       "             {'hoverinfo': 'text',\n",
       "              'hovertext': [(0.0, 8.429369557916289e-08), multiplicity: 2, (0.0,\n",
       "                            8.429369557916289e-08), multiplicity: 2, (0.0,\n",
       "                            1.1920928955078125e-07), multiplicity: 4, ..., (0.0,\n",
       "                            37.847503662109375), (0.0, 41.764686584472656), (0.0,\n",
       "                            56.005279541015625)],\n",
       "              'mode': 'markers',\n",
       "              'name': 'H0',\n",
       "              'type': 'scatter',\n",
       "              'x': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "              'y': array([8.42936956e-08, 8.42936956e-08, 1.19209290e-07, ..., 3.78475037e+01,\n",
       "                          4.17646866e+01, 5.60052795e+01])},\n",
       "             {'hoverinfo': 'text',\n",
       "              'hovertext': [(14.380936622619629, 14.418966293334961),\n",
       "                            (13.159102439880371, 13.343947410583496),\n",
       "                            (10.802297592163086, 11.060097694396973), ...,\n",
       "                            (0.005897068418562412, 0.007005500607192516),\n",
       "                            (0.005624172743409872, 0.006914278492331505),\n",
       "                            (0.004995710216462612, 0.005333075765520334)],\n",
       "              'mode': 'markers',\n",
       "              'name': 'H1',\n",
       "              'type': 'scatter',\n",
       "              'x': array([1.43809366e+01, 1.31591024e+01, 1.08022976e+01, ..., 5.89706842e-03,\n",
       "                          5.62417274e-03, 4.99571022e-03]),\n",
       "              'y': array([1.44189663e+01, 1.33439474e+01, 1.10600977e+01, ..., 7.00550061e-03,\n",
       "                          6.91427849e-03, 5.33307577e-03])}],\n",
       "    'layout': {'height': 500,\n",
       "               'plot_bgcolor': 'white',\n",
       "               'template': '...',\n",
       "               'width': 500,\n",
       "               'xaxis': {'autorange': False,\n",
       "                         'exponentformat': 'e',\n",
       "                         'linecolor': 'black',\n",
       "                         'linewidth': 1,\n",
       "                         'mirror': False,\n",
       "                         'range': [-1.1201055908203126, 57.12538513183594],\n",
       "                         'showexponent': 'all',\n",
       "                         'showline': True,\n",
       "                         'side': 'bottom',\n",
       "                         'ticks': 'outside',\n",
       "                         'title': {'text': 'Birth'},\n",
       "                         'type': 'linear',\n",
       "                         'zeroline': True},\n",
       "               'yaxis': {'autorange': False,\n",
       "                         'exponentformat': 'e',\n",
       "                         'linecolor': 'black',\n",
       "                         'linewidth': 1,\n",
       "                         'mirror': False,\n",
       "                         'range': [-1.1201055908203126, 57.12538513183594],\n",
       "                         'scaleanchor': 'x',\n",
       "                         'scaleratio': 1,\n",
       "                         'showexponent': 'all',\n",
       "                         'showline': True,\n",
       "                         'side': 'left',\n",
       "                         'ticks': 'outside',\n",
       "                         'title': {'text': 'Death'},\n",
       "                         'type': 'linear',\n",
       "                         'zeroline': True}}\n",
       "}), entropy=array([[8.95909669, 7.42222979]]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_dataset_1['Topology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot_diagram(diagram3[0])\n",
    "p2 = plot_diagram(diagram3[1])\n",
    "p3 = plot_diagram(diagram3[2])\n",
    "p4 = plot_diagram(diagram3[3])\n",
    "p5 = plot_diagram(diagram3[4])\n",
    "p6 = plot_diagram(diagram3[5])\n",
    "p7 = plot_diagram(diagram3[6])\n",
    "p8 = plot_diagram(diagram3[7])\n",
    "ps = [p1, p2, p3, p4, p5, p6, p7, p8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa48e14935048a29ae20696a0c090b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Traffic Dataset 1'), HBox(children=(FigureWidget({\n",
       "    'data': [{'hoverinfo': 'non"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_stacked_graphs(ps, stack_shape=(2,4), title='Traffic Dataset 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.32406839, 5.1243836 ],\n",
       "       [6.94413666, 4.99184496],\n",
       "       [6.95312667, 5.24449007],\n",
       "       [6.92634246, 5.37887215],\n",
       "       [8.05151298, 6.31608737],\n",
       "       [8.09695682, 6.26762694],\n",
       "       [7.66136713, 5.6431511 ],\n",
       "       [7.38402225, 5.80863017]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
